[
  {
    "path": "posts/2021-01-17-random-sampling-a-table-animation/",
    "title": "Random Sampling: A table animation",
    "description": "Plus a convenient way of rendering LaTeX expressions as images",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2021-01-17",
    "categories": [
      "data visualization",
      "data wrangling"
    ],
    "contents": "\r\n\r\nContents\r\nStatic\r\nAn aside on LaTeX equations\r\n\r\nAnimated\r\nFinal Product\r\n\r\n\r\n\r\n\r\nd-article table th {\r\n  font-size: 12px;\r\n}\r\nd-article table td {\r\n  font-size: 12px;\r\n}\r\n\r\nIn my last blogpost, I demonstrated a couple use cases for the higher-order functionals reduce() and accumulate() from the {purrr} package. In one example, I made an animated {kableExtra} table by accumulate()-ing over multiple calls to column_spec() that set a background color for a column.\r\nAnimated tables are virtually non-existent in the wild, and probably for a good reason. but I wanted to extend upon my previous table animation and create something that’s maybe a bit more on the “informative” side.\r\nTo that end, here’s an animate table that simulates sampling from a bivariate normal distribution.\r\nStatic\r\nLet’s first start by generating 100,000 data points:\r\n\r\n\r\nset.seed(2021)\r\n\r\nlibrary(dplyr)\r\n\r\nsamples_data <- MASS::mvrnorm(1e5, c(0, 0), matrix(c(1, .7, .7, 1), ncol = 2)) %>% \r\n  as_tibble(.name_repair = ~c(\"x\", \"y\")) %>% \r\n  mutate(across(everything(), ~ as.character(.x - .x %% 0.2)))\r\n\r\nsamples_data\r\n\r\n\r\n  # A tibble: 100,000 x 2\r\n     x     y    \r\n     <chr> <chr>\r\n   1 0     -0.4 \r\n   2 0.2   0.6  \r\n   3 0.4   0.2  \r\n   4 0.6   -0.2 \r\n   5 0.6   0.8  \r\n   6 -1.8  -2   \r\n   7 0.8   -0.6 \r\n   8 1.2   0.4  \r\n   9 0.4   -0.4 \r\n  10 1.4   1.6  \r\n  # ... with 99,990 more rows\r\n\r\nLet’s see how this looks when we turn this into a “matrix”1. To place continuous values into discrete cells in the table, I’m also binning both variables by 0.2:\r\n\r\n\r\nsamples_data_spread <- samples_data %>% \r\n  count(x, y) %>% \r\n  right_join(\r\n    tidyr::crossing(\r\n      x = as.character(seq(-3, 3, 0.2)),\r\n      y = as.character(seq(-3, 3, 0.2))\r\n    ),\r\n    by = c(\"x\", \"y\")\r\n  ) %>% \r\n  tidyr::pivot_wider(names_from = y, values_from = n) %>% \r\n  arrange(-as.numeric(x)) %>% \r\n  select(c(\"x\", as.character(seq(-3, 3, 0.2)))) %>% \r\n  rename(\" \" = x)\r\n\r\nsamples_data_spread\r\n\r\n\r\n  # A tibble: 31 x 32\r\n     ` `    `-3` `-2.8` `-2.6` `-2.4` `-2.2`  `-2` `-1.8` `-1.6` `-1.4` `-1.2`\r\n     <chr> <int>  <int>  <int>  <int>  <int> <int>  <int>  <int>  <int>  <int>\r\n   1 3        NA     NA     NA     NA     NA    NA     NA     NA     NA     NA\r\n   2 2.8      NA     NA     NA     NA     NA    NA     NA     NA     NA     NA\r\n   3 2.6      NA     NA     NA     NA     NA    NA     NA     NA     NA     NA\r\n   4 2.4      NA     NA     NA     NA     NA    NA     NA     NA     NA     NA\r\n   5 2.2      NA     NA     NA     NA     NA    NA     NA     NA     NA     NA\r\n   6 2        NA     NA     NA     NA     NA    NA     NA     NA     NA     NA\r\n   7 1.8      NA     NA     NA     NA     NA    NA     NA     NA     NA     NA\r\n   8 1.6      NA     NA     NA     NA     NA    NA     NA      1      1      4\r\n   9 1.4      NA     NA     NA     NA     NA    NA      1     NA      1      4\r\n  10 1.2      NA     NA     NA     NA     NA    NA     NA      3      2      7\r\n  # ... with 21 more rows, and 21 more variables: `-1` <int>, `-0.8` <int>,\r\n  #   `-0.6` <int>, `-0.4` <int>, `-0.2` <int>, `0` <int>, `0.2` <int>,\r\n  #   `0.4` <int>, `0.6` <int>, `0.8` <int>, `1` <int>, `1.2` <int>, `1.4` <int>,\r\n  #   `1.6` <int>, `1.8` <int>, `2` <int>, `2.2` <int>, `2.4` <int>, `2.6` <int>,\r\n  #   `2.8` <int>, `3` <int>\r\n\r\nNow we can turn this into a table and fill the cells according to the counts using reduce():\r\n\r\n\r\nlibrary(kableExtra)\r\n\r\nsamples_data_table <- samples_data_spread %>% \r\n  kable() %>% \r\n  kable_classic() %>% \r\n  purrr::reduce(2L:length(samples_data_spread), ~ {\r\n    column_spec(\r\n      kable_input = .x,\r\n      column = .y,\r\n      background = spec_color(\r\n        samples_data_spread[[.y]],\r\n        scale_from = c(1, max(as.numeric(as.matrix(samples_data_spread)), na.rm = TRUE)),\r\n        na_color = \"white\",\r\n        option = \"plasma\"\r\n      ),\r\n      color = \"white\"\r\n    )},\r\n    .init = .\r\n  )\r\n\r\nsamples_data_table\r\n\r\n\r\n\r\n\r\n\r\n-3\r\n\r\n\r\n-2.8\r\n\r\n\r\n-2.6\r\n\r\n\r\n-2.4\r\n\r\n\r\n-2.2\r\n\r\n\r\n-2\r\n\r\n\r\n-1.8\r\n\r\n\r\n-1.6\r\n\r\n\r\n-1.4\r\n\r\n\r\n-1.2\r\n\r\n\r\n-1\r\n\r\n\r\n-0.8\r\n\r\n\r\n-0.6\r\n\r\n\r\n-0.4\r\n\r\n\r\n-0.2\r\n\r\n\r\n0\r\n\r\n\r\n0.2\r\n\r\n\r\n0.4\r\n\r\n\r\n0.6\r\n\r\n\r\n0.8\r\n\r\n\r\n1\r\n\r\n\r\n1.2\r\n\r\n\r\n1.4\r\n\r\n\r\n1.6\r\n\r\n\r\n1.8\r\n\r\n\r\n2\r\n\r\n\r\n2.2\r\n\r\n\r\n2.4\r\n\r\n\r\n2.6\r\n\r\n\r\n2.8\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n8\r\n\r\n\r\n2\r\n\r\n\r\n7\r\n\r\n\r\n7\r\n\r\n\r\n8\r\n\r\n\r\n4\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2.8\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\n3\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n20\r\n\r\n\r\n14\r\n\r\n\r\n20\r\n\r\n\r\n15\r\n\r\n\r\n8\r\n\r\n\r\n18\r\n\r\n\r\n6\r\n\r\n\r\n10\r\n\r\n\r\n7\r\n\r\n\r\n3\r\n\r\n\r\n2.6\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\n7\r\n\r\n\r\n11\r\n\r\n\r\n21\r\n\r\n\r\n26\r\n\r\n\r\n17\r\n\r\n\r\n26\r\n\r\n\r\n29\r\n\r\n\r\n28\r\n\r\n\r\n21\r\n\r\n\r\n17\r\n\r\n\r\n10\r\n\r\n\r\n3\r\n\r\n\r\n8\r\n\r\n\r\n2.4\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n7\r\n\r\n\r\n11\r\n\r\n\r\n17\r\n\r\n\r\n20\r\n\r\n\r\n32\r\n\r\n\r\n33\r\n\r\n\r\n43\r\n\r\n\r\n52\r\n\r\n\r\n43\r\n\r\n\r\n37\r\n\r\n\r\n23\r\n\r\n\r\n23\r\n\r\n\r\n17\r\n\r\n\r\n9\r\n\r\n\r\n7\r\n\r\n\r\n2.2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\n7\r\n\r\n\r\n6\r\n\r\n\r\n12\r\n\r\n\r\n20\r\n\r\n\r\n18\r\n\r\n\r\n46\r\n\r\n\r\n51\r\n\r\n\r\n59\r\n\r\n\r\n66\r\n\r\n\r\n58\r\n\r\n\r\n73\r\n\r\n\r\n53\r\n\r\n\r\n41\r\n\r\n\r\n21\r\n\r\n\r\n20\r\n\r\n\r\n16\r\n\r\n\r\n8\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\n12\r\n\r\n\r\n17\r\n\r\n\r\n20\r\n\r\n\r\n35\r\n\r\n\r\n53\r\n\r\n\r\n83\r\n\r\n\r\n103\r\n\r\n\r\n93\r\n\r\n\r\n117\r\n\r\n\r\n106\r\n\r\n\r\n111\r\n\r\n\r\n74\r\n\r\n\r\n52\r\n\r\n\r\n42\r\n\r\n\r\n27\r\n\r\n\r\n17\r\n\r\n\r\n5\r\n\r\n\r\n1.8\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n8\r\n\r\n\r\n10\r\n\r\n\r\n14\r\n\r\n\r\n40\r\n\r\n\r\n50\r\n\r\n\r\n81\r\n\r\n\r\n108\r\n\r\n\r\n128\r\n\r\n\r\n132\r\n\r\n\r\n149\r\n\r\n\r\n143\r\n\r\n\r\n146\r\n\r\n\r\n103\r\n\r\n\r\n89\r\n\r\n\r\n57\r\n\r\n\r\n39\r\n\r\n\r\n19\r\n\r\n\r\n23\r\n\r\n\r\n7\r\n\r\n\r\n1.6\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n6\r\n\r\n\r\n6\r\n\r\n\r\n14\r\n\r\n\r\n19\r\n\r\n\r\n39\r\n\r\n\r\n67\r\n\r\n\r\n99\r\n\r\n\r\n136\r\n\r\n\r\n148\r\n\r\n\r\n183\r\n\r\n\r\n197\r\n\r\n\r\n214\r\n\r\n\r\n185\r\n\r\n\r\n170\r\n\r\n\r\n109\r\n\r\n\r\n81\r\n\r\n\r\n60\r\n\r\n\r\n40\r\n\r\n\r\n24\r\n\r\n\r\n11\r\n\r\n\r\n11\r\n\r\n\r\n1.4\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n8\r\n\r\n\r\n17\r\n\r\n\r\n37\r\n\r\n\r\n50\r\n\r\n\r\n74\r\n\r\n\r\n115\r\n\r\n\r\n170\r\n\r\n\r\n225\r\n\r\n\r\n277\r\n\r\n\r\n307\r\n\r\n\r\n323\r\n\r\n\r\n292\r\n\r\n\r\n243\r\n\r\n\r\n186\r\n\r\n\r\n123\r\n\r\n\r\n110\r\n\r\n\r\n61\r\n\r\n\r\n47\r\n\r\n\r\n19\r\n\r\n\r\n7\r\n\r\n\r\n3\r\n\r\n\r\n1.2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\n7\r\n\r\n\r\n22\r\n\r\n\r\n29\r\n\r\n\r\n60\r\n\r\n\r\n88\r\n\r\n\r\n144\r\n\r\n\r\n204\r\n\r\n\r\n273\r\n\r\n\r\n317\r\n\r\n\r\n376\r\n\r\n\r\n381\r\n\r\n\r\n337\r\n\r\n\r\n323\r\n\r\n\r\n262\r\n\r\n\r\n208\r\n\r\n\r\n135\r\n\r\n\r\n92\r\n\r\n\r\n58\r\n\r\n\r\n41\r\n\r\n\r\n17\r\n\r\n\r\n8\r\n\r\n\r\n7\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\n18\r\n\r\n\r\n26\r\n\r\n\r\n64\r\n\r\n\r\n83\r\n\r\n\r\n160\r\n\r\n\r\n239\r\n\r\n\r\n329\r\n\r\n\r\n375\r\n\r\n\r\n504\r\n\r\n\r\n501\r\n\r\n\r\n474\r\n\r\n\r\n455\r\n\r\n\r\n336\r\n\r\n\r\n315\r\n\r\n\r\n223\r\n\r\n\r\n154\r\n\r\n\r\n94\r\n\r\n\r\n52\r\n\r\n\r\n23\r\n\r\n\r\n11\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\n0.8\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n4\r\n\r\n\r\n8\r\n\r\n\r\n10\r\n\r\n\r\n29\r\n\r\n\r\n68\r\n\r\n\r\n104\r\n\r\n\r\n163\r\n\r\n\r\n269\r\n\r\n\r\n336\r\n\r\n\r\n375\r\n\r\n\r\n517\r\n\r\n\r\n566\r\n\r\n\r\n612\r\n\r\n\r\n572\r\n\r\n\r\n480\r\n\r\n\r\n355\r\n\r\n\r\n256\r\n\r\n\r\n190\r\n\r\n\r\n129\r\n\r\n\r\n58\r\n\r\n\r\n37\r\n\r\n\r\n14\r\n\r\n\r\n14\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n0.6\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n9\r\n\r\n\r\n20\r\n\r\n\r\n39\r\n\r\n\r\n62\r\n\r\n\r\n104\r\n\r\n\r\n161\r\n\r\n\r\n272\r\n\r\n\r\n373\r\n\r\n\r\n459\r\n\r\n\r\n591\r\n\r\n\r\n674\r\n\r\n\r\n684\r\n\r\n\r\n641\r\n\r\n\r\n587\r\n\r\n\r\n487\r\n\r\n\r\n365\r\n\r\n\r\n251\r\n\r\n\r\n167\r\n\r\n\r\n97\r\n\r\n\r\n56\r\n\r\n\r\n25\r\n\r\n\r\n9\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n0.4\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\n4\r\n\r\n\r\n27\r\n\r\n\r\n32\r\n\r\n\r\n60\r\n\r\n\r\n101\r\n\r\n\r\n159\r\n\r\n\r\n260\r\n\r\n\r\n413\r\n\r\n\r\n535\r\n\r\n\r\n680\r\n\r\n\r\n794\r\n\r\n\r\n796\r\n\r\n\r\n780\r\n\r\n\r\n704\r\n\r\n\r\n537\r\n\r\n\r\n452\r\n\r\n\r\n345\r\n\r\n\r\n218\r\n\r\n\r\n119\r\n\r\n\r\n69\r\n\r\n\r\n38\r\n\r\n\r\n16\r\n\r\n\r\n8\r\n\r\n\r\n10\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\n0.2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n9\r\n\r\n\r\n33\r\n\r\n\r\n46\r\n\r\n\r\n91\r\n\r\n\r\n152\r\n\r\n\r\n229\r\n\r\n\r\n388\r\n\r\n\r\n519\r\n\r\n\r\n654\r\n\r\n\r\n777\r\n\r\n\r\n851\r\n\r\n\r\n881\r\n\r\n\r\n712\r\n\r\n\r\n674\r\n\r\n\r\n535\r\n\r\n\r\n389\r\n\r\n\r\n285\r\n\r\n\r\n176\r\n\r\n\r\n102\r\n\r\n\r\n45\r\n\r\n\r\n29\r\n\r\n\r\n14\r\n\r\n\r\n7\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\n0\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n11\r\n\r\n\r\n18\r\n\r\n\r\n51\r\n\r\n\r\n77\r\n\r\n\r\n154\r\n\r\n\r\n210\r\n\r\n\r\n351\r\n\r\n\r\n521\r\n\r\n\r\n645\r\n\r\n\r\n778\r\n\r\n\r\n876\r\n\r\n\r\n866\r\n\r\n\r\n812\r\n\r\n\r\n685\r\n\r\n\r\n593\r\n\r\n\r\n459\r\n\r\n\r\n296\r\n\r\n\r\n190\r\n\r\n\r\n117\r\n\r\n\r\n50\r\n\r\n\r\n39\r\n\r\n\r\n24\r\n\r\n\r\n8\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\n-0.2\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n15\r\n\r\n\r\n36\r\n\r\n\r\n59\r\n\r\n\r\n112\r\n\r\n\r\n196\r\n\r\n\r\n286\r\n\r\n\r\n410\r\n\r\n\r\n620\r\n\r\n\r\n747\r\n\r\n\r\n856\r\n\r\n\r\n854\r\n\r\n\r\n836\r\n\r\n\r\n721\r\n\r\n\r\n683\r\n\r\n\r\n493\r\n\r\n\r\n344\r\n\r\n\r\n215\r\n\r\n\r\n162\r\n\r\n\r\n70\r\n\r\n\r\n50\r\n\r\n\r\n21\r\n\r\n\r\n6\r\n\r\n\r\n5\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-0.4\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n12\r\n\r\n\r\n24\r\n\r\n\r\n60\r\n\r\n\r\n85\r\n\r\n\r\n168\r\n\r\n\r\n256\r\n\r\n\r\n373\r\n\r\n\r\n551\r\n\r\n\r\n689\r\n\r\n\r\n785\r\n\r\n\r\n842\r\n\r\n\r\n776\r\n\r\n\r\n773\r\n\r\n\r\n683\r\n\r\n\r\n504\r\n\r\n\r\n395\r\n\r\n\r\n233\r\n\r\n\r\n154\r\n\r\n\r\n94\r\n\r\n\r\n43\r\n\r\n\r\n35\r\n\r\n\r\n7\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-0.6\r\n\r\n\r\nNA\r\n\r\n\r\n4\r\n\r\n\r\n13\r\n\r\n\r\n16\r\n\r\n\r\n30\r\n\r\n\r\n62\r\n\r\n\r\n119\r\n\r\n\r\n219\r\n\r\n\r\n331\r\n\r\n\r\n447\r\n\r\n\r\n573\r\n\r\n\r\n714\r\n\r\n\r\n736\r\n\r\n\r\n787\r\n\r\n\r\n725\r\n\r\n\r\n658\r\n\r\n\r\n524\r\n\r\n\r\n389\r\n\r\n\r\n255\r\n\r\n\r\n219\r\n\r\n\r\n108\r\n\r\n\r\n66\r\n\r\n\r\n37\r\n\r\n\r\n5\r\n\r\n\r\n8\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-0.8\r\n\r\n\r\n3\r\n\r\n\r\n8\r\n\r\n\r\n13\r\n\r\n\r\n40\r\n\r\n\r\n59\r\n\r\n\r\n81\r\n\r\n\r\n181\r\n\r\n\r\n263\r\n\r\n\r\n330\r\n\r\n\r\n469\r\n\r\n\r\n600\r\n\r\n\r\n661\r\n\r\n\r\n681\r\n\r\n\r\n652\r\n\r\n\r\n639\r\n\r\n\r\n484\r\n\r\n\r\n368\r\n\r\n\r\n274\r\n\r\n\r\n160\r\n\r\n\r\n123\r\n\r\n\r\n48\r\n\r\n\r\n26\r\n\r\n\r\n13\r\n\r\n\r\n10\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-1\r\n\r\n\r\n6\r\n\r\n\r\n8\r\n\r\n\r\n21\r\n\r\n\r\n34\r\n\r\n\r\n80\r\n\r\n\r\n133\r\n\r\n\r\n195\r\n\r\n\r\n293\r\n\r\n\r\n386\r\n\r\n\r\n457\r\n\r\n\r\n556\r\n\r\n\r\n626\r\n\r\n\r\n574\r\n\r\n\r\n526\r\n\r\n\r\n461\r\n\r\n\r\n363\r\n\r\n\r\n246\r\n\r\n\r\n190\r\n\r\n\r\n105\r\n\r\n\r\n56\r\n\r\n\r\n26\r\n\r\n\r\n16\r\n\r\n\r\n6\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-1.2\r\n\r\n\r\n10\r\n\r\n\r\n8\r\n\r\n\r\n21\r\n\r\n\r\n45\r\n\r\n\r\n77\r\n\r\n\r\n146\r\n\r\n\r\n198\r\n\r\n\r\n266\r\n\r\n\r\n360\r\n\r\n\r\n436\r\n\r\n\r\n469\r\n\r\n\r\n480\r\n\r\n\r\n457\r\n\r\n\r\n393\r\n\r\n\r\n344\r\n\r\n\r\n242\r\n\r\n\r\n169\r\n\r\n\r\n104\r\n\r\n\r\n79\r\n\r\n\r\n33\r\n\r\n\r\n23\r\n\r\n\r\n9\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-1.4\r\n\r\n\r\n6\r\n\r\n\r\n13\r\n\r\n\r\n31\r\n\r\n\r\n62\r\n\r\n\r\n96\r\n\r\n\r\n163\r\n\r\n\r\n200\r\n\r\n\r\n299\r\n\r\n\r\n337\r\n\r\n\r\n360\r\n\r\n\r\n364\r\n\r\n\r\n364\r\n\r\n\r\n319\r\n\r\n\r\n239\r\n\r\n\r\n190\r\n\r\n\r\n129\r\n\r\n\r\n84\r\n\r\n\r\n50\r\n\r\n\r\n33\r\n\r\n\r\n17\r\n\r\n\r\n11\r\n\r\n\r\n7\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-1.6\r\n\r\n\r\n18\r\n\r\n\r\n25\r\n\r\n\r\n39\r\n\r\n\r\n61\r\n\r\n\r\n110\r\n\r\n\r\n138\r\n\r\n\r\n184\r\n\r\n\r\n235\r\n\r\n\r\n281\r\n\r\n\r\n278\r\n\r\n\r\n294\r\n\r\n\r\n246\r\n\r\n\r\n211\r\n\r\n\r\n176\r\n\r\n\r\n148\r\n\r\n\r\n92\r\n\r\n\r\n40\r\n\r\n\r\n23\r\n\r\n\r\n17\r\n\r\n\r\n8\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-1.8\r\n\r\n\r\n11\r\n\r\n\r\n19\r\n\r\n\r\n33\r\n\r\n\r\n52\r\n\r\n\r\n90\r\n\r\n\r\n139\r\n\r\n\r\n165\r\n\r\n\r\n185\r\n\r\n\r\n225\r\n\r\n\r\n192\r\n\r\n\r\n206\r\n\r\n\r\n157\r\n\r\n\r\n125\r\n\r\n\r\n100\r\n\r\n\r\n62\r\n\r\n\r\n40\r\n\r\n\r\n28\r\n\r\n\r\n11\r\n\r\n\r\n10\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-2\r\n\r\n\r\n11\r\n\r\n\r\n29\r\n\r\n\r\n34\r\n\r\n\r\n50\r\n\r\n\r\n78\r\n\r\n\r\n102\r\n\r\n\r\n139\r\n\r\n\r\n141\r\n\r\n\r\n144\r\n\r\n\r\n149\r\n\r\n\r\n110\r\n\r\n\r\n104\r\n\r\n\r\n76\r\n\r\n\r\n49\r\n\r\n\r\n41\r\n\r\n\r\n19\r\n\r\n\r\n19\r\n\r\n\r\n5\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-2.2\r\n\r\n\r\n11\r\n\r\n\r\n23\r\n\r\n\r\n38\r\n\r\n\r\n48\r\n\r\n\r\n76\r\n\r\n\r\n76\r\n\r\n\r\n99\r\n\r\n\r\n105\r\n\r\n\r\n88\r\n\r\n\r\n81\r\n\r\n\r\n81\r\n\r\n\r\n72\r\n\r\n\r\n36\r\n\r\n\r\n20\r\n\r\n\r\n25\r\n\r\n\r\n9\r\n\r\n\r\n6\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-2.4\r\n\r\n\r\n12\r\n\r\n\r\n21\r\n\r\n\r\n24\r\n\r\n\r\n44\r\n\r\n\r\n56\r\n\r\n\r\n53\r\n\r\n\r\n51\r\n\r\n\r\n69\r\n\r\n\r\n66\r\n\r\n\r\n54\r\n\r\n\r\n46\r\n\r\n\r\n24\r\n\r\n\r\n21\r\n\r\n\r\n9\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-2.6\r\n\r\n\r\n12\r\n\r\n\r\n15\r\n\r\n\r\n20\r\n\r\n\r\n34\r\n\r\n\r\n32\r\n\r\n\r\n30\r\n\r\n\r\n40\r\n\r\n\r\n34\r\n\r\n\r\n36\r\n\r\n\r\n28\r\n\r\n\r\n21\r\n\r\n\r\n15\r\n\r\n\r\n8\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-2.8\r\n\r\n\r\n5\r\n\r\n\r\n17\r\n\r\n\r\n28\r\n\r\n\r\n27\r\n\r\n\r\n19\r\n\r\n\r\n14\r\n\r\n\r\n20\r\n\r\n\r\n26\r\n\r\n\r\n15\r\n\r\n\r\n10\r\n\r\n\r\n10\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n-3\r\n\r\n\r\n6\r\n\r\n\r\n10\r\n\r\n\r\n3\r\n\r\n\r\n11\r\n\r\n\r\n21\r\n\r\n\r\n11\r\n\r\n\r\n13\r\n\r\n\r\n6\r\n\r\n\r\n10\r\n\r\n\r\n8\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nAn aside on LaTeX equations\r\nAs an aside, let’s say we also want to annotate this table with the true distribution where this sample came from. As specified in our call to MASS::mvrnorm() used to make samples_data, the distribution is one where both variables have a mean of 0 and a standard deviation of 1, plus a correlation of 0.7:\r\n\\[\\begin{bmatrix} X \\\\ Y \\end{bmatrix}\\ \\sim\\ N(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix}1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix})\\]\r\nWhere the LaTeX code for the above formula is:\r\n\r\n  \\begin{bmatrix} X \\\\ Y \\end{bmatrix}\\ \\sim\\\r\n  N(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\r\n  \\begin{bmatrix}1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix})\r\n\r\nMany different solutions already exist to LaTeX math annotations. The most common is probably Non-Standard Evaluation (NSE) methods using parse(), expression(), bquote() etc. There are bulkier solutions like the {latex2exp} package that plots plotmath expressions, though it hasn’t been updated since 2015 and I personally had difficulty getting it to work.\r\nOne solution I’ve never heard of/considered before is querying a web LaTeX editor that has an API. The Online LaTeX Equation Editor by CodeCogs is the perfect example of this. A simple link that contains the LaTeX code in a URL-compatible encoding renders the resulting expression as an image!\r\nI wrote a function latex_query (not thoroughly tested) in my personal package that takes LaTeX code and generates a CodeCogs URL containing the rendered expression2\r\n\r\n\r\n# NOTE the string literal syntax using r\"(...)\" is only available in R 4.0.0 and up\r\nlatex_url <- junebug::latex_query(\r\n  formula = r\"(\\begin{bmatrix} X \\\\ Y \\end{bmatrix}\\ \\sim\\\r\n              N(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\r\n              \\begin{bmatrix}1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix}))\",\r\n  dpi = 150\r\n)\r\n\r\nknitr::include_graphics(latex_url)\r\n\r\n\r\n\r\n\r\nThe variable latex_url is this really long URL which, as we see above, points to a rendered image of the LaTeX expression we fed it!\r\nAnnotating our table, then, is pretty straightforward. We save it as an image, read in the LaTeX equation as an image, then combine!\r\n\r\n\r\nsave_kable(samples_data_table, \"img/samples_data_table.png\")\r\n\r\nlibrary(magick)\r\n\r\nimage_composite(\r\n  image_read(\"img/samples_data_table.png\"),\r\n  image_read(latex_url),\r\n  offset = \"+50+50\"\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\nAnimated\r\nFor an animated version, we add a step where we split the data at every 10,000 additional samples before binning the observations into cells. We then draw a table at each point of the accumulation using {kableExtra} with the help of map() and reduce() (plus some more kable styling).\r\n\r\n\r\nsamples_tables <- purrr::map(1L:10L, ~{\r\n  samples_slice <- samples_data %>% \r\n    slice(1L:(.x * 1e4)) %>% \r\n    count(x, y) %>% \r\n    right_join(\r\n      tidyr::crossing(\r\n        x = as.character(seq(-3, 3, 0.2)),\r\n        y = as.character(seq(-3, 3, 0.2))\r\n      ),\r\n      by = c(\"x\", \"y\")\r\n    ) %>% \r\n    tidyr::pivot_wider(names_from = y, values_from = n) %>% \r\n    arrange(-as.numeric(x)) %>% \r\n    select(c(\"x\", as.character(seq(-3, 3, 0.2)))) %>% \r\n    rename(\" \" = x)\r\n\r\n  \r\n  samples_slice %>%\r\n    kable() %>% \r\n    kable_classic() %>% \r\n    purrr::reduce(\r\n      2L:length(samples_slice),\r\n      ~ {\r\n        .x %>% \r\n          column_spec(\r\n            column = .y,\r\n            width_min = \"35px\",\r\n            background = spec_color(\r\n              samples_slice[[.y]],\r\n              scale_from = c(1, max(as.numeric(as.matrix(samples_slice)), na.rm = TRUE)),\r\n              na_color = \"white\",\r\n              option = \"plasma\"\r\n            ),\r\n            color = \"white\"\r\n          ) %>% \r\n          row_spec(\r\n            row = .y - 1L,\r\n            hline_after = FALSE,\r\n            extra_css = \"border-top:none; padding-top:15px;\"\r\n          )\r\n      },\r\n      .init = .\r\n    ) %>% \r\n    row_spec(0L, bold = TRUE) %>% \r\n    column_spec(1L, bold = TRUE, border_right = TRUE) %>% \r\n    kable_styling(\r\n      full_width = F,\r\n      font_size = 10,\r\n      html_font = \"IBM Plex Mono\",\r\n    )\r\n})\r\n\r\n\r\n\r\nThe result, samples_tables is a list of tables. We can walk() over that list with save_kable() to write them as images and then read them back in with {magick}:\r\n\r\n\r\npurrr::iwalk(samples_tables, ~ save_kable(.x, file = glue::glue(\"tbl_imgs/tbl{.y}.png\")))\r\n\r\ntable_imgs <- image_read(paste0(\"tbl_imgs/tbl\", 1:10, \".png\"))\r\n\r\n\r\n\r\nNow we can add our LaTeX expression from the previous section as an annotation to these table images using image_composite():\r\n\r\n\r\ntable_imgs_annotated <- table_imgs %>% \r\n  image_composite(\r\n    image_read(latex_url),\r\n    offset = \"+100+80\"\r\n  )\r\n\r\n\r\n\r\nFinally, we just patch the table images together into an animation using image_animate() and we have our animated table!\r\n\r\n\r\ntable_imgs_animated <- table_imgs_annotated %>% \r\n  image_animate(optimize = TRUE)\r\n\r\n\r\n\r\nFinal Product\r\n\r\n\r\n\r\nYou can also see the difference in the degree of “interpolation” by directly comparing the table at 10 thousand vs 100 thousand samples (the first and last frames):\r\n\r\n\r\n\r\nNeat!\r\n\r\nVisually speaking. It’s still a dataframe object for compatibility with {kableExtra}↩︎\r\nDetails about the API - https://www.codecogs.com/latex/editor-api.php↩︎\r\n",
    "preview": "posts/2021-01-17-random-sampling-a-table-animation/table_preview.png",
    "last_modified": "2021-01-18T08:31:53+09:00",
    "input_file": {},
    "preview_width": 1185,
    "preview_height": 1180
  },
  {
    "path": "posts/2020-12-13-collapse-repetitive-piping-with-reduce/",
    "title": "Collapse repetitive piping with reduce()",
    "description": "Featuring accumulate()",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-12-13",
    "categories": [
      "data wrangling",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nHappy pipes\r\nSad (repetitive) pipes\r\nIntroducing purrr::reduce()\r\n\r\nExample 1: {ggplot2}\r\nA reduce() solution\r\nfeat. accumulate()\r\n\r\nExample 2: {kableExtra}\r\nA reduce2() solution\r\nfeat. accumulate2()\r\n\r\nExample 3: {dplyr}\r\nA reduce() solution\r\nfeat. {data.table}\r\n\r\nMisc.\r\n\r\n\r\n\r\n\r\n\r\n\r\ntable.lightable-classic {\r\n  margin-bottom: 30px;\r\n}\r\n\r\nIntroduction\r\nHappy pipes\r\nModern day programming with R is all about pipes.1 You start out with some object that undergoes incremental changes as it is passed (piped) into a chain of functions and finally returned as the desired output, like in this simple example. 2\r\n\r\n\r\nset.seed(2021) # Can 2020 be over already?\r\n\r\nsquare <- function(x) x^2\r\ndeviation <- function(x) x - mean(x)\r\n\r\nnums <- runif(100)\r\n\r\nnums %>%\r\n  deviation() %>%\r\n  square() %>%\r\n  mean() %>%\r\n  sqrt()\r\n\r\n\r\n  [1] 0.3039881\r\n\r\nWhen we pipe (or pass anything through any function, for that matter), we often do one distinct thing at a time, like in the above example.\r\nSo, we rarely have a chain of functions that look like this:\r\n\r\n\r\nlibrary(dplyr)\r\n\r\nmtcars %>% \r\n  mutate(kmpg = mpg/1.6) %>% \r\n  mutate(disp = round(disp)) %>% \r\n  select(-vs) %>% \r\n  select(-am) %>% \r\n  select(-gear) %>% \r\n  select(-carb) %>% \r\n  filter(mpg > 15) %>% \r\n  filter(cyl == 6) %>% \r\n  filter(wt < 3)\r\n\r\n\r\n\r\n… because many functions are vectorized, or designed to handle multiple values by other means, like this:\r\n\r\n\r\npenguins %>% \r\n  mutate(kmpg = mpg/1.6, disp = round(disp)) %>% \r\n  select(-(vs:carb)) %>% \r\n  filter(mpg > 15, cyl == 6, wt < 3)\r\n\r\n\r\n\r\nSad (repetitive) pipes\r\nBut some functions do not handle multiple inputs the way we want it to, or just not at all. Here are some examples of what I’m talking about.\r\nIn {ggplot2}, this doesn’t plot 3 overlapping points with sizes 8, 4, and 2:\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\nggplot(mtcars, aes(hp, mpg)) + \r\n  geom_point(size = c(8, 4, 2), alpha = .5)\r\n\r\n\r\n  Error: Aesthetics must be either length 1 or the same as the data (32): size\r\n\r\nSo you have to do this:\r\n\r\n\r\nggplot(mtcars, aes(hp, mpg)) + \r\n  geom_point(size = 8, alpha = .5) +\r\n  geom_point(size = 4, alpha = .5) +\r\n  geom_point(size = 2, alpha = .5)\r\n\r\n\r\n\r\n\r\nIn {kableExtra}, this doesn’t color the third column “skyblue”, the fourth column “forestgreen”, and the fifth column “chocolate”:3\r\n\r\n\r\nlibrary(kableExtra)\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  kbl() %>% \r\n  kable_classic(html_font = \"Roboto\") %>% \r\n  column_spec(3:5, background = c(\"skyblue\", \"forestgreen\", \"chocolate\"))\r\n\r\n\r\n  Warning in ensure_len_html(background, nrows, \"background\"): The number of\r\n  provided values in background does not equal to the number of rows.\r\n\r\n\r\n\r\nmpg\r\n\r\n\r\ncyl\r\n\r\n\r\ndisp\r\n\r\n\r\nhp\r\n\r\n\r\ndrat\r\n\r\n\r\nwt\r\n\r\n\r\nqsec\r\n\r\n\r\nvs\r\n\r\n\r\nam\r\n\r\n\r\ngear\r\n\r\n\r\ncarb\r\n\r\n\r\nMazda RX4\r\n\r\n\r\n21.0\r\n\r\n\r\n6\r\n\r\n\r\n160\r\n\r\n\r\n110\r\n\r\n\r\n3.90\r\n\r\n\r\n2.620\r\n\r\n\r\n16.46\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nMazda RX4 Wag\r\n\r\n\r\n21.0\r\n\r\n\r\n6\r\n\r\n\r\n160\r\n\r\n\r\n110\r\n\r\n\r\n3.90\r\n\r\n\r\n2.875\r\n\r\n\r\n17.02\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nDatsun 710\r\n\r\n\r\n22.8\r\n\r\n\r\n4\r\n\r\n\r\n108\r\n\r\n\r\n93\r\n\r\n\r\n3.85\r\n\r\n\r\n2.320\r\n\r\n\r\n18.61\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\nHornet 4 Drive\r\n\r\n\r\n21.4\r\n\r\n\r\n6\r\n\r\n\r\n258\r\n\r\n\r\n110\r\n\r\n\r\n3.08\r\n\r\n\r\n3.215\r\n\r\n\r\n19.44\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nHornet Sportabout\r\n\r\n\r\n18.7\r\n\r\n\r\n8\r\n\r\n\r\n360\r\n\r\n\r\n175\r\n\r\n\r\n3.15\r\n\r\n\r\n3.440\r\n\r\n\r\n17.02\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\nValiant\r\n\r\n\r\n18.1\r\n\r\n\r\n6\r\n\r\n\r\n225\r\n\r\n\r\n105\r\n\r\n\r\n2.76\r\n\r\n\r\n3.460\r\n\r\n\r\n20.22\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nSo you have to do this:\r\n\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  kbl() %>% \r\n  kable_classic(html_font = \"Roboto\") %>% \r\n  column_spec(3, background = \"skyblue\") %>% \r\n  column_spec(4, background = \"forestgreen\") %>% \r\n  column_spec(5, background = \"chocolate\")\r\n\r\n\r\n\r\n\r\n\r\nmpg\r\n\r\n\r\ncyl\r\n\r\n\r\ndisp\r\n\r\n\r\nhp\r\n\r\n\r\ndrat\r\n\r\n\r\nwt\r\n\r\n\r\nqsec\r\n\r\n\r\nvs\r\n\r\n\r\nam\r\n\r\n\r\ngear\r\n\r\n\r\ncarb\r\n\r\n\r\nMazda RX4\r\n\r\n\r\n21.0\r\n\r\n\r\n6\r\n\r\n\r\n160\r\n\r\n\r\n110\r\n\r\n\r\n3.90\r\n\r\n\r\n2.620\r\n\r\n\r\n16.46\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nMazda RX4 Wag\r\n\r\n\r\n21.0\r\n\r\n\r\n6\r\n\r\n\r\n160\r\n\r\n\r\n110\r\n\r\n\r\n3.90\r\n\r\n\r\n2.875\r\n\r\n\r\n17.02\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nDatsun 710\r\n\r\n\r\n22.8\r\n\r\n\r\n4\r\n\r\n\r\n108\r\n\r\n\r\n93\r\n\r\n\r\n3.85\r\n\r\n\r\n2.320\r\n\r\n\r\n18.61\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\nHornet 4 Drive\r\n\r\n\r\n21.4\r\n\r\n\r\n6\r\n\r\n\r\n258\r\n\r\n\r\n110\r\n\r\n\r\n3.08\r\n\r\n\r\n3.215\r\n\r\n\r\n19.44\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nHornet Sportabout\r\n\r\n\r\n18.7\r\n\r\n\r\n8\r\n\r\n\r\n360\r\n\r\n\r\n175\r\n\r\n\r\n3.15\r\n\r\n\r\n3.440\r\n\r\n\r\n17.02\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\nValiant\r\n\r\n\r\n18.1\r\n\r\n\r\n6\r\n\r\n\r\n225\r\n\r\n\r\n105\r\n\r\n\r\n2.76\r\n\r\n\r\n3.460\r\n\r\n\r\n20.22\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nIn {dplyr}, this doesn’t make 3 new columns named “a”, “b”, and “c”, all filled with NA:4\r\n\r\n\r\nnew_cols <- c(\"a\", \"b\", \"c\")\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  mutate(!!new_cols := NA)\r\n\r\n\r\n  Error: The LHS of `:=` must be a string or a symbol\r\n\r\nSo you have to do either one of these:5\r\n\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  mutate(\r\n    !!new_cols[1] := NA,\r\n    !!new_cols[2] := NA,\r\n    !!new_cols[3] := NA\r\n  )\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  mutate(!!new_cols[1] := NA) %>% \r\n  mutate(!!new_cols[2] := NA) %>% \r\n  mutate(!!new_cols[3] := NA)\r\n\r\n\r\n\r\n\r\n     mpg  a  b  c\r\n  1 21.0 NA NA NA\r\n  2 21.0 NA NA NA\r\n  3 22.8 NA NA NA\r\n  4 21.4 NA NA NA\r\n  5 18.7 NA NA NA\r\n  6 18.1 NA NA NA\r\n\r\nSo we’ve got functions being repeated, but in all these cases it looks like we can’t just throw in a vector and expect the function to loop/map over them internally in the specific way that we want it to. And the “correct ways” I provided here are not very satisfying: that’s a lot of copying and pasting!\r\nPersonally, I think it’d be nice to collapse these repetitive calls - but how?\r\nIntroducing purrr::reduce()\r\nThe reduce() function from the {purrr} package is a powerful functional that allows you to abstract away from a sequence of functions that are applied in a fixed direction. You should go give Advanced R Ch. 9.5 a read if you want an in-depth explanation, but here I’m just gonna give a quick crash course for our application of it to our current problem.6\r\nAll you need to know here is that reduce() takes a vector as its first argument, a function as its second argument, and an optional .init argument.7\r\nHere’s a schematic:\r\n\r\n\r\n\r\nFigure 1: From Advanced R by Hadley Wickham\r\n\r\n\r\n\r\nLet me really quickly demonstrate reduce() in action.\r\nSay you wanted to add up the numbers 1 through 5, but only using the plus operator +. You could do something like this:8\r\n\r\n\r\n1 + 2 + 3 + 4 + 5\r\n\r\n\r\n  [1] 15\r\n\r\nWhich is the same as this:\r\n\r\n\r\nlibrary(purrr)\r\nreduce(1:5, `+`)\r\n\r\n\r\n  [1] 15\r\n\r\nAnd if you want the start value to be something that’s not the first argument of the vector, pass that to the .init argument:\r\n\r\n\r\nidentical(\r\n  0.5 + 1 + 2 + 3 + 4 + 5,\r\n  reduce(1:5, `+`, .init = 0.5)\r\n)\r\n\r\n\r\n  [1] TRUE\r\n\r\nIf you want to be specific, you can use an {rlang}-style anonymous function where .x is the accumulated value being passed into the first argument fo the function and .y is the second argument of the function.9\r\n\r\n\r\nidentical(\r\n  reduce(1:5, `+`, .init = 0.5),\r\n  reduce(1:5, ~ .x + .y, .init = 0.5)\r\n)\r\n\r\n\r\n  [1] TRUE\r\n\r\nAnd two more examples just to demonstrate that directionality matters:\r\n\r\n\r\nidentical(\r\n  reduce(1:5, `^`, .init = 0.5),\r\n  reduce(1:5, ~ .x ^ .y, .init = 0.5) # .x on left, .y on right\r\n)\r\n\r\n\r\n  [1] TRUE\r\n\r\nidentical(\r\n  reduce(1:5, `^`, .init = 0.5),\r\n  reduce(1:5, ~ .y ^ .x, .init = 0.5) # .y on left, .x on right\r\n)\r\n\r\n\r\n  [1] FALSE\r\n\r\nThat’s pretty much all you need to know - let’s jump right in!\r\nExample 1: {ggplot2}\r\nA reduce() solution\r\nRecall that we had this sad code:\r\n\r\n\r\nggplot(mtcars, aes(hp, mpg)) + \r\n  geom_point(size = 8, alpha = .5) +\r\n  geom_point(size = 4, alpha = .5) +\r\n  geom_point(size = 2, alpha = .5)\r\n\r\n\r\n\r\nFor illustrative purposes, I’m going to move the + “pipes” to the beginning of each line:\r\n\r\n\r\nggplot(mtcars, aes(hp, mpg))\r\n  + geom_point(size = 8, alpha = .5)\r\n  + geom_point(size = 4, alpha = .5)\r\n  + geom_point(size = 2, alpha = .5)\r\n\r\n\r\n\r\nAt this point, we see a clear pattern emerge line-by-line. We start with ggplot(mtcars, aes(hp, mpg)), which is kind of its own thing. Then we have three repetitions of + geom_point(size = X, alpha = .5) where the X varies between 8, 4, and 2. We also notice that the sequence of calls goes from left to right, as is the normal order of piping.\r\nNow let’s translate these observations into reduce(). I’m bad with words so here’s a visual:\r\n\r\n\r\n\r\nLet’s go over what we did in our call to reduce() above:\r\nIn the first argument, we have the vector of values that are iterated over.\r\nIn the second argument, we have an anonymous function composed of…\r\nThe .x variable, which represents the accumulated value. In this context, we keep the .x on the left because that is the left-hand side that we are carrying over to the next call via the +.\r\nThe .y variable, which takes on values from the first argument passed into reduce(). In this context, .y will be each value of the numeric vector c(8, 4, 2) since .init is given.\r\nThe repeating function call geom_point(size = .y, alpha = .5) that is called with each value of the vector passed in as the first argument.\r\n\r\nIn the third argument .init, we have ggplot(mtcars, aes(hp, mpg)) which is the non-repeating piece of code that we start with.\r\nIf you want to see the actual code run, here it is:\r\n\r\n\r\nreduce(\r\n  c(8, 4, 2),\r\n  ~ .x + geom_point(size = .y, alpha = .5),\r\n  .init = ggplot(mtcars, aes(hp, mpg))\r\n)\r\n\r\n\r\n\r\n\r\nLet’s dig in a bit more, this time with an example that looks prettier.\r\nSuppose you want to collapse the repeated calls to geom_point() in this code:\r\n\r\n\r\nviridis_colors <- viridis::viridis(10)\r\n\r\nmtcars %>% \r\n  ggplot(aes(hp, mpg)) +\r\n  geom_point(size = 20, color = viridis_colors[10]) +\r\n  geom_point(size = 18, color = viridis_colors[9]) +\r\n  geom_point(size = 16, color = viridis_colors[8]) +\r\n  geom_point(size = 14, color = viridis_colors[7]) +\r\n  geom_point(size = 12, color = viridis_colors[6]) +\r\n  geom_point(size = 10, color = viridis_colors[5]) +\r\n  geom_point(size = 8, color = viridis_colors[4]) +\r\n  geom_point(size = 6, color = viridis_colors[3]) +\r\n  geom_point(size = 4, color = viridis_colors[2]) +\r\n  geom_point(size = 2, color = viridis_colors[1]) +\r\n  scale_x_discrete(expand = expansion(.2)) +\r\n  scale_y_continuous(expand = expansion(.2)) +\r\n  theme_void() +\r\n  theme(panel.background = element_rect(fill = \"grey20\"))\r\n\r\n\r\n\r\n\r\nYou can do this with reduce() in a couple ways:10\r\n\r\n\r\nMethod 1\r\nMethod 1: Move all the “constant” parts to .init, since the order of these layers don’t matter.\r\n\r\n\r\nreduce(\r\n    10L:1L,\r\n    ~ .x + geom_point(size = .y * 2, color = viridis_colors[.y]),\r\n    \r\n    .init = mtcars %>% \r\n      ggplot(aes(hp, mpg)) +\r\n      scale_x_discrete(expand = expansion(.2)) +\r\n      scale_y_continuous(expand = expansion(.2)) +\r\n      theme_void() +\r\n      theme(panel.background = element_rect(fill = \"grey20\"))\r\n    \r\n)\r\n\r\n\r\n\r\n\r\n\r\nMethod 2\r\nMethod 2: Use reduce() in place, with the help of the {magrittr} dot .\r\n\r\n\r\nmtcars %>% \r\n  ggplot(aes(hp, mpg)) %>% \r\n  \r\n  reduce(\r\n    10L:1L,\r\n    ~ .x + geom_point(size = .y * 2, color = viridis_colors[.y]),\r\n    .init = . #<- right here!\r\n  ) +\r\n  \r\n  scale_x_discrete(expand = expansion(.2)) +\r\n  scale_y_continuous(expand = expansion(.2)) +\r\n  theme_void() +\r\n  theme(panel.background = element_rect(fill = \"grey20\"))\r\n\r\n\r\n\r\n\r\n\r\nMethod 3\r\nMethod 3: Move all the “constant” parts to the top, wrap it in parentheses, and pass the whole thing into .init using the {magrittr} dot .\r\n\r\n\r\n(mtcars %>% \r\n  ggplot(aes(hp, mpg)) +\r\n  scale_x_discrete(expand = expansion(.2)) +\r\n  scale_y_continuous(expand = expansion(.2)) +\r\n  theme_void() +\r\n  theme(panel.background = element_rect(fill = \"grey20\"))) %>% \r\n  \r\n  reduce(\r\n    10L:1L,\r\n    ~ .x + geom_point(size = .y * 2, color = viridis_colors[.y]),\r\n    .init = . #<- right here!\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nAll in all, we see that reduce() allows us to write more succinct code!\r\nAn obvious advantage to this is that it is now really easy to make a single change that applies to all the repeated calls.\r\nFor example, if I want to make the radius of the points grow/shrink exponentially, I just need to modify the anonymous function in the second argument of reduce():\r\n\r\n\r\n# Using Method 3\r\n(mtcars %>% \r\n  ggplot(aes(hp, mpg)) +\r\n  scale_x_discrete(expand = expansion(.2)) +\r\n  scale_y_continuous(expand = expansion(.2)) +\r\n  theme_void() +\r\n  theme(panel.background = element_rect(fill = \"grey20\"))) %>% \r\n  reduce(\r\n    10L:1L,\r\n    ~ .x + geom_point(size = .y ^ 1.5, color = viridis_colors[.y]),  # exponential!\r\n    .init = .\r\n  )\r\n\r\n\r\n\r\n\r\nYay, we collapsed ten layers of geom_point()!\r\nfeat. accumulate()\r\nThere’s actually one more thing I want to show here, which is holding onto intermediate values using accumulate().\r\naccumulate() is like reduce(), except instead of returning a single value which is the output of the very last function call, it keeps all intermediate values and returns them in a list.\r\n\r\n\r\naccumulate(1:5, `+`)\r\n\r\n\r\n  [1]  1  3  6 10 15\r\n\r\nCheck out what happens if I change reduce() to accumulate() and return each element of the resulting list:\r\n\r\n\r\nplots <- (mtcars %>% \r\n  ggplot(aes(hp, mpg)) +\r\n  scale_x_discrete(expand = expansion(.2)) +\r\n  scale_y_continuous(expand = expansion(.2)) +\r\n  theme_void() +\r\n  theme(panel.background = element_rect(fill = \"grey20\"))) %>% \r\n  accumulate(\r\n    10L:1L,\r\n    ~ .x + geom_point(size = .y ^ 1.5, color = viridis_colors[.y]),\r\n    .init = .\r\n  )\r\n\r\nfor (i in plots) { plot(i) }\r\n\r\n\r\n\r\n\r\nWe got back the intermediate plots!\r\nAre you thinking what I’m thinking? Let’s animate this!\r\n\r\n\r\nlibrary(magick)\r\n\r\n# change ggplot2 objects into images\r\nimgs <- map(1:length(plots), ~ {\r\n  img <- image_graph(width = 672, height = 480)\r\n  plot(plots[[.x]])\r\n  dev.off()\r\n  img\r\n})\r\n\r\n# combine images as frames\r\nimgs <- image_join(imgs)\r\n\r\n# animate\r\nimage_animate(imgs)\r\n\r\n\r\n\r\n\r\n\r\n\r\nNeat!11\r\nExample 2: {kableExtra}\r\nA reduce2() solution\r\nRecall that we had this sad code:\r\n\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  kbl() %>% \r\n  kable_classic(html_font = \"Roboto\") %>% \r\n  column_spec(3, background = \"skyblue\") %>% \r\n  column_spec(4, background = \"forestgreen\") %>% \r\n  column_spec(5, background = \"chocolate\")\r\n\r\n\r\n\r\nWe’ve got two things varying here: the column location 3:5 and the background color c(\"skyblue\", \"forestgreen\", \"chocolate\"). We could do the same trick I sneaked into the previous section by just passing one vector to reduce() that basically functions as an index:12\r\n\r\n\r\nnumbers <- 3:5\r\nbackground_colors <- c(\"skyblue\", \"forestgreen\", \"chocolate\")\r\n\r\n(mtcars %>% \r\n  head() %>% \r\n  kbl() %>% \r\n  kable_classic(html_font = \"Roboto\")) %>% \r\n  reduce(\r\n    1:3,\r\n    ~ .x %>% column_spec(numbers[.y], background = background_colors[.y]),\r\n    .init = .\r\n  )\r\n\r\n\r\n\r\nBut I want to use this opportunity to showcase reduce2(), which explicitly takes a second varying argument to the function that you are reduce()-ing over.\r\nHere, ..1 is like the .x and ..2 is like the .y from reduce(). The only new part is ..3 which refers to the second varying argument.\r\n\r\n\r\n(mtcars %>% \r\n  head() %>% \r\n  kbl() %>% \r\n  kable_classic(html_font = \"Roboto\")) %>% \r\n  reduce2(\r\n    3:5,                                           # 1st varying argument (represented by ..2)\r\n    c(\"skyblue\", \"forestgreen\", \"chocolate\"),      # 2nd varying argument (represented by ..3)\r\n    ~ ..1 %>% column_spec(..2, background = ..3),\r\n    .init = .\r\n  )\r\n\r\n\r\n\r\nWe’re not done yet! We can actually skip the {magrittr} pipe %>% and just stick ..1 as the first argument inside column_spec().13 This actually improves performance because you’re removing the overhead from evaluating the pipe!\r\nAdditionally, because the pipe forces evaluation with each call unlike + in {ggplot2}, we don’t need the parantheses wrapped around the top part of the code for the {magrittr} dot . to work!\r\nHere is the final reduce2() solution for our sad code:\r\n\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  kbl() %>% \r\n  kable_classic(html_font = \"Roboto\") %>%       # No need to wrap in parentheses!\r\n  reduce2(\r\n    3:5,                                          \r\n    c(\"skyblue\", \"forestgreen\", \"chocolate\"),  \r\n    ~ column_spec(..1, ..2, background = ..3),  # No need for the pipe!\r\n    .init = .\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nmpg\r\n\r\n\r\ncyl\r\n\r\n\r\ndisp\r\n\r\n\r\nhp\r\n\r\n\r\ndrat\r\n\r\n\r\nwt\r\n\r\n\r\nqsec\r\n\r\n\r\nvs\r\n\r\n\r\nam\r\n\r\n\r\ngear\r\n\r\n\r\ncarb\r\n\r\n\r\nMazda RX4\r\n\r\n\r\n21.0\r\n\r\n\r\n6\r\n\r\n\r\n160\r\n\r\n\r\n110\r\n\r\n\r\n3.90\r\n\r\n\r\n2.620\r\n\r\n\r\n16.46\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nMazda RX4 Wag\r\n\r\n\r\n21.0\r\n\r\n\r\n6\r\n\r\n\r\n160\r\n\r\n\r\n110\r\n\r\n\r\n3.90\r\n\r\n\r\n2.875\r\n\r\n\r\n17.02\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nDatsun 710\r\n\r\n\r\n22.8\r\n\r\n\r\n4\r\n\r\n\r\n108\r\n\r\n\r\n93\r\n\r\n\r\n3.85\r\n\r\n\r\n2.320\r\n\r\n\r\n18.61\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\nHornet 4 Drive\r\n\r\n\r\n21.4\r\n\r\n\r\n6\r\n\r\n\r\n258\r\n\r\n\r\n110\r\n\r\n\r\n3.08\r\n\r\n\r\n3.215\r\n\r\n\r\n19.44\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nHornet Sportabout\r\n\r\n\r\n18.7\r\n\r\n\r\n8\r\n\r\n\r\n360\r\n\r\n\r\n175\r\n\r\n\r\n3.15\r\n\r\n\r\n3.440\r\n\r\n\r\n17.02\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\nValiant\r\n\r\n\r\n18.1\r\n\r\n\r\n6\r\n\r\n\r\n225\r\n\r\n\r\n105\r\n\r\n\r\n2.76\r\n\r\n\r\n3.460\r\n\r\n\r\n20.22\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nAnd of course, we now have the flexibilty to do much more complicated manipulations!\r\n\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  kbl() %>% \r\n  kable_classic(html_font = \"Roboto\") %>% \r\n  reduce2(\r\n    1:12,                                          \r\n    viridis::viridis(12),  \r\n    ~ column_spec(..1, ..2, background = ..3, color = if(..2 < 5){\"white\"}),\r\n    .init = .\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nmpg\r\n\r\n\r\ncyl\r\n\r\n\r\ndisp\r\n\r\n\r\nhp\r\n\r\n\r\ndrat\r\n\r\n\r\nwt\r\n\r\n\r\nqsec\r\n\r\n\r\nvs\r\n\r\n\r\nam\r\n\r\n\r\ngear\r\n\r\n\r\ncarb\r\n\r\n\r\nMazda RX4\r\n\r\n\r\n21.0\r\n\r\n\r\n6\r\n\r\n\r\n160\r\n\r\n\r\n110\r\n\r\n\r\n3.90\r\n\r\n\r\n2.620\r\n\r\n\r\n16.46\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nMazda RX4 Wag\r\n\r\n\r\n21.0\r\n\r\n\r\n6\r\n\r\n\r\n160\r\n\r\n\r\n110\r\n\r\n\r\n3.90\r\n\r\n\r\n2.875\r\n\r\n\r\n17.02\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\nDatsun 710\r\n\r\n\r\n22.8\r\n\r\n\r\n4\r\n\r\n\r\n108\r\n\r\n\r\n93\r\n\r\n\r\n3.85\r\n\r\n\r\n2.320\r\n\r\n\r\n18.61\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\nHornet 4 Drive\r\n\r\n\r\n21.4\r\n\r\n\r\n6\r\n\r\n\r\n258\r\n\r\n\r\n110\r\n\r\n\r\n3.08\r\n\r\n\r\n3.215\r\n\r\n\r\n19.44\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nHornet Sportabout\r\n\r\n\r\n18.7\r\n\r\n\r\n8\r\n\r\n\r\n360\r\n\r\n\r\n175\r\n\r\n\r\n3.15\r\n\r\n\r\n3.440\r\n\r\n\r\n17.02\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\nValiant\r\n\r\n\r\n18.1\r\n\r\n\r\n6\r\n\r\n\r\n225\r\n\r\n\r\n105\r\n\r\n\r\n2.76\r\n\r\n\r\n3.460\r\n\r\n\r\n20.22\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\nfeat. accumulate2()\r\nYep, that’s right - more animations with accumulate() and {magick}!\r\nActually, to be precise, we’re going to use the accumuate2() here to replace our reduce2().\r\nFirst, we save the list of intermediate outputs to tables:\r\n\r\n\r\ntables <- mtcars %>% \r\n  head() %>% \r\n  kbl() %>% \r\n  kable_classic(html_font = \"Roboto\") %>% \r\n  kable_styling(full_width = FALSE) %>% # Added to keep aspect ratio constant when saving\r\n  accumulate2(\r\n    1:(length(mtcars)+1),                                          \r\n    viridis::viridis(length(mtcars)+1),  \r\n    ~ column_spec(..1, ..2, background = ..3, color = if(..2 < 5){\"white\"}),\r\n    .init = .\r\n  )\r\n\r\n\r\n\r\nThen, we save each table in tables as an image:\r\n\r\n\r\niwalk(tables, ~ save_kable(.x, file = here::here(\"img\", paste0(\"table\", .y, \".png\")), zoom = 2))\r\n\r\n\r\n\r\nFinally, we read them in and animate:\r\n\r\n\r\ntables <- map(\r\n  paste0(\"table\", 1:length(tables), \".png\"),\r\n  ~ image_read(here::here(\"img\", .x))\r\n)\r\n\r\ntables <- image_join(tables)\r\n\r\nimage_animate(tables)\r\n\r\n\r\n\r\n\r\n\r\n\r\nBet you don’t see animated tables often!\r\nExample 3: {dplyr}\r\nA reduce() solution\r\nRecall that we had this sad code:\r\n\r\n\r\nnew_cols <- c(\"a\", \"b\", \"c\")\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  mutate(!!new_cols[1] := NA) %>% \r\n  mutate(!!new_cols[2] := NA) %>% \r\n  mutate(!!new_cols[3] := NA)\r\n\r\n\r\n     mpg  a  b  c\r\n  1 21.0 NA NA NA\r\n  2 21.0 NA NA NA\r\n  3 22.8 NA NA NA\r\n  4 21.4 NA NA NA\r\n  5 18.7 NA NA NA\r\n  6 18.1 NA NA NA\r\n\r\nYou know the drill - a simple call to reduce() gives us three new columns with names corresponding to the elements of the new_cols character vector we defined above:\r\n\r\n\r\n# Converting to tibble for nicer printing\r\nmtcars <- as_tibble(mtcars)\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  reduce(\r\n    new_cols,\r\n    ~ mutate(.x, !!.y := NA),\r\n    .init = .\r\n  )\r\n\r\n\r\n  # A tibble: 6 x 4\r\n      mpg a     b     c    \r\n    <dbl> <lgl> <lgl> <lgl>\r\n  1  21   NA    NA    NA   \r\n  2  21   NA    NA    NA   \r\n  3  22.8 NA    NA    NA   \r\n  4  21.4 NA    NA    NA   \r\n  5  18.7 NA    NA    NA   \r\n  6  18.1 NA    NA    NA\r\n\r\nAgain, this gives you a lot of flexibility, like the ability to dynamically assign values to each new column:\r\n\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  reduce(\r\n    new_cols,\r\n    ~ mutate(.x, !!.y := paste0(.y, \"-\", row_number())),\r\n    .init = .\r\n  )\r\n\r\n\r\n  # A tibble: 6 x 4\r\n      mpg a     b     c    \r\n    <dbl> <chr> <chr> <chr>\r\n  1  21   a-1   b-1   c-1  \r\n  2  21   a-2   b-2   c-2  \r\n  3  22.8 a-3   b-3   c-3  \r\n  4  21.4 a-4   b-4   c-4  \r\n  5  18.7 a-5   b-5   c-5  \r\n  6  18.1 a-6   b-6   c-6\r\n\r\nWe can take this even further using context dependent expressions like cur_data(), and do something like keeping track of the columns present at each point a new column has been created via mutate():\r\n\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  reduce(\r\n    new_cols,\r\n    ~ mutate(.x, !!.y := paste(c(names(cur_data()), .y), collapse = \"-\")),\r\n    .init = .\r\n  )\r\n\r\n\r\n  # A tibble: 6 x 4\r\n      mpg a     b       c        \r\n    <dbl> <chr> <chr>   <chr>    \r\n  1  21   mpg-a mpg-a-b mpg-a-b-c\r\n  2  21   mpg-a mpg-a-b mpg-a-b-c\r\n  3  22.8 mpg-a mpg-a-b mpg-a-b-c\r\n  4  21.4 mpg-a mpg-a-b mpg-a-b-c\r\n  5  18.7 mpg-a mpg-a-b mpg-a-b-c\r\n  6  18.1 mpg-a mpg-a-b mpg-a-b-c\r\n\r\nHere’s another example just for fun - an “addition matrix”:14\r\n\r\n\r\nmtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  reduce(\r\n    pull(., mpg),\r\n    ~ mutate(.x, !!as.character(.y) := .y + mpg),\r\n    .init = .\r\n  )\r\n\r\n\r\n  # A tibble: 6 x 6\r\n      mpg  `21` `22.8` `21.4` `18.7` `18.1`\r\n    <dbl> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\r\n  1  21    42     43.8   42.4   39.7   39.1\r\n  2  21    42     43.8   42.4   39.7   39.1\r\n  3  22.8  43.8   45.6   44.2   41.5   40.9\r\n  4  21.4  42.4   44.2   42.8   40.1   39.5\r\n  5  18.7  39.7   41.5   40.1   37.4   36.8\r\n  6  18.1  39.1   40.9   39.5   36.8   36.2\r\n\r\nLet’s now look at a more practical application of this: explicit dummy coding!\r\nIn R, the factor data structure allows implicit dummy coding, which you can access using contrasts().\r\nHere, in our data penguins from the {palmerpenguins} package, we see that the 3-way contrast between “Adelie”, “Chinstrap”, and “Gentoo” in the species factor column is treatment coded, with “Adelie” set as the reference level:\r\n\r\n\r\ndata(\"penguins\", package = \"palmerpenguins\")\r\n\r\npenguins_implicit <- penguins %>% \r\n  na.omit() %>% \r\n  select(species, flipper_length_mm) %>% \r\n  mutate(species = factor(species))\r\n\r\ncontrasts(penguins_implicit$species)\r\n\r\n\r\n            Chinstrap Gentoo\r\n  Adelie            0      0\r\n  Chinstrap         1      0\r\n  Gentoo            0      1\r\n\r\nWe can also infer that from the output of this simple linear model:15\r\n\r\n\r\nbroom::tidy(lm(flipper_length_mm ~ species, data = penguins_implicit))\r\n\r\n\r\n  # A tibble: 3 x 5\r\n    term             estimate std.error statistic   p.value\r\n    <chr>               <dbl>     <dbl>     <dbl>     <dbl>\r\n  1 (Intercept)        190.       0.552    344.   0.       \r\n  2 speciesChinstrap     5.72     0.980      5.84 1.25e-  8\r\n  3 speciesGentoo       27.1      0.824     32.9  2.68e-106\r\n\r\nWhat’s cool is that you can make this 3-way treatment coding explicit by expanding the matrix into actual columns of the data!\r\nHere’s a reduce() solution:\r\n\r\n\r\npenguins_explicit <- \r\n  reduce(\r\n    levels(penguins_implicit$species)[-1],\r\n    ~ mutate(.x, !!paste0(\"species\", .y) := as.integer(species == .y)),\r\n    .init = penguins_implicit\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nspecies\r\n\r\n\r\nflipper_length_mm\r\n\r\n\r\nspeciesChinstrap\r\n\r\n\r\nspeciesGentoo\r\n\r\n\r\nAdelie\r\n\r\n\r\n181\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n186\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n181\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n182\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n198\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n185\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n197\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n184\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n194\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n174\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n180\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n189\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n185\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n180\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n183\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n172\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n180\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n178\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n178\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n188\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n184\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n196\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n180\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n181\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n184\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n182\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n186\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n196\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n185\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n182\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n186\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n188\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n200\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n186\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n181\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n194\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n185\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n185\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n192\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n184\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n192\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n188\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n198\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n196\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n197\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n184\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n189\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n196\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n194\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n189\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n189\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n202\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n205\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n185\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n186\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n208\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n196\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n178\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n192\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n192\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n203\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n183\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n184\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n199\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n181\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n197\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n198\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n197\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n196\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n188\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n199\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n189\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n189\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n198\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n176\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n202\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n186\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n199\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n197\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n199\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n191\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n200\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n185\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n188\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n192\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n185\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n190\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n184\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n195\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n193\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nAdelie\r\n\r\n\r\n201\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nGentoo\r\n\r\n\r\n211\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n230\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n218\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n211\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n219\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n209\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n214\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n216\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n214\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n213\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n217\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n221\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n209\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n222\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n218\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n213\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n220\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n222\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n209\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n207\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n230\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n220\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n220\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n213\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n219\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n208\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n208\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n208\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n225\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n216\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n222\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n217\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n225\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n213\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n220\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n225\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n217\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n220\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n208\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n220\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n208\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n224\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n208\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n221\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n214\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n231\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n219\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n230\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n229\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n220\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n223\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n216\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n221\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n221\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n217\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n216\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n230\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n209\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n220\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n223\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n212\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n221\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n212\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n224\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n212\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n228\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n218\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n218\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n212\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n230\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n218\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n228\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n212\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n224\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n214\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n226\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n216\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n222\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n203\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n225\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n219\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n228\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n228\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n210\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n219\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n208\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n209\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n216\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n229\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n213\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n230\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n217\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n230\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n222\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n214\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n215\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n222\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n212\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nGentoo\r\n\r\n\r\n213\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nChinstrap\r\n\r\n\r\n192\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n196\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n193\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n188\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n197\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n198\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n178\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n197\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n195\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n198\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n193\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n194\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n185\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n201\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n190\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n201\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n197\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n181\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n190\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n195\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n181\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n191\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n187\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n193\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n195\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n197\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n200\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n200\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n191\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n205\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n187\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n201\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n187\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n203\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n195\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n199\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n195\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n210\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n192\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n205\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n210\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n187\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n196\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n196\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n196\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n201\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n190\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n212\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n187\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n198\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n199\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n201\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n193\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n203\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n187\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n197\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n191\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n203\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n202\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n194\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n206\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n189\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n195\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n207\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n202\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n193\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n210\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nChinstrap\r\n\r\n\r\n198\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n\r\nAnd we get the exact same output from lm() when we throw in the new columns speciesChinstrap and speciesGentoo as the predictors!\r\n\r\n\r\nbroom::tidy(lm(flipper_length_mm ~ speciesChinstrap + speciesGentoo, data = penguins_explicit))\r\n\r\n\r\n  # A tibble: 3 x 5\r\n    term             estimate std.error statistic   p.value\r\n    <chr>               <dbl>     <dbl>     <dbl>     <dbl>\r\n  1 (Intercept)        190.       0.552    344.   0.       \r\n  2 speciesChinstrap     5.72     0.980      5.84 1.25e-  8\r\n  3 speciesGentoo       27.1      0.824     32.9  2.68e-106\r\n\r\nBy the way, if you’re wondering how this is practical, some modeling packages in R (like {lavaan} for structural equation modeling) only accept dummy coded variables that exist as independent columns/vectors, not as a metadata of a factor vector.16 This is common enough that some packages like {psych} have a function that does the same transformation we just did, called dummy.code()17:\r\n\r\n\r\nbind_cols(\r\n  penguins_implicit,\r\n  psych::dummy.code(penguins_implicit$species)\r\n)\r\n\r\n\r\n  # A tibble: 333 x 3\r\n     species flipper_length_mm ...3[,\"Adelie\"] [,\"Gentoo\"] [,\"Chinstrap\"]\r\n     <fct>               <int>           <dbl>       <dbl>          <dbl>\r\n   1 Adelie                181               1           0              0\r\n   2 Adelie                186               1           0              0\r\n   3 Adelie                195               1           0              0\r\n   4 Adelie                193               1           0              0\r\n   5 Adelie                190               1           0              0\r\n   6 Adelie                181               1           0              0\r\n   7 Adelie                195               1           0              0\r\n   8 Adelie                182               1           0              0\r\n   9 Adelie                191               1           0              0\r\n  10 Adelie                198               1           0              0\r\n  # ... with 323 more rows\r\n\r\nfeat. {data.table}\r\nOf course, you could do all of this without reduce() in {data.table} because its walrus := is vectorized.\r\nHere’s the {data.table} solution for our sad code:\r\n\r\n\r\nlibrary(data.table)\r\nnew_cols <- c(\"a\", \"b\", \"c\")\r\n\r\nmtcars_dt <- mtcars %>% \r\n  head() %>% \r\n  select(mpg) %>% \r\n  as.data.table()\r\n\r\nmtcars_dt[, (new_cols) := NA][]\r\n\r\n\r\n      mpg  a  b  c\r\n  1: 21.0 NA NA NA\r\n  2: 21.0 NA NA NA\r\n  3: 22.8 NA NA NA\r\n  4: 21.4 NA NA NA\r\n  5: 18.7 NA NA NA\r\n  6: 18.1 NA NA NA\r\n\r\nAnd here’s a {data.table} solution for the explicit dummy coding example:\r\n\r\n\r\npenguins_dt <- as.data.table(penguins_implicit)\r\n\r\ntreatment_lvls <- levels(penguins_dt$species)[-1]\r\ntreatment_cols <- paste0(\"species\", treatment_lvls)\r\n\r\npenguins_dt[, (treatment_cols) := lapply(treatment_lvls, function(x){as.integer(species == x)})][]\r\n\r\n\r\n         species flipper_length_mm speciesChinstrap speciesGentoo\r\n    1:    Adelie               181                0             0\r\n    2:    Adelie               186                0             0\r\n    3:    Adelie               195                0             0\r\n    4:    Adelie               193                0             0\r\n    5:    Adelie               190                0             0\r\n   ---                                                           \r\n  329: Chinstrap               207                1             0\r\n  330: Chinstrap               202                1             0\r\n  331: Chinstrap               193                1             0\r\n  332: Chinstrap               210                1             0\r\n  333: Chinstrap               198                1             0\r\n\r\nI personally default to using {data.table} over {dplyr} in these cases.\r\nMisc.\r\nYou can also pass in a list of functions instead of a list of arguments because why not.\r\nFor example, this replicates the very first code I showed in this blog post:\r\n\r\n\r\nmy_funs <- list(deviation, square, mean, sqrt)\r\n\r\nreduce(\r\n  my_funs,\r\n  ~ .y(.x),\r\n  .init = nums\r\n)\r\n\r\n\r\n  [1] 0.3039881\r\n\r\nYou could also pass in both a list of functions and a list of their arguments if you really want to abstract away from, like, literally everything:\r\n\r\n\r\nLawful Good\r\n\r\n\r\nlibrary(janitor)\r\n\r\nmtcars %>% \r\n  clean_names(case = \"title\") %>% \r\n  tabyl(2) %>% \r\n  adorn_rounding(digits = 2) %>% \r\n  adorn_totals()\r\n\r\n\r\n     Cyl  n percent\r\n       4 11    0.34\r\n       6  7    0.22\r\n       8 14    0.44\r\n   Total 32    1.00\r\n\r\n\r\n\r\nChaotic Evil\r\n\r\n\r\njanitor_funs <- list(clean_names, tabyl, adorn_rounding, adorn_totals)\r\njanitor_args <- list(list(case = \"title\"), list(2), list(digits = 2), NULL)\r\n\r\nreduce2(\r\n  janitor_funs,\r\n  janitor_args,\r\n  ~ do.call(..2, c(list(dat = ..1), ..3)),\r\n  .init = mtcars\r\n)\r\n\r\n\r\n     Cyl  n percent\r\n       4 11    0.34\r\n       6  7    0.22\r\n       8 14    0.44\r\n   Total 32    1.00\r\n\r\n\r\n\r\nHave fun reducing repetitions in your code with reduce()!\r\n\r\nSo much so that there’s going to be a native pipe operator!↩︎\r\nTaken from Advanced R Ch. 6↩︎\r\nIf you aren’t familiar with {kableExtra}, you just need to know that column_spec() can take a column index as its first argument and a color as the background argument to set the background color of a column to the provided color. And as we see here, if a color vector is passed into background, it’s just recycled to color the rows which is not what we want.↩︎\r\nIf this is your first time seeing the “bang bang” !! operator and the “walrus” := operator being used this way, check out the documentation on quasiquotation.↩︎\r\nFor those of you more familiar with quasiquation in {dplyr}, I should also mention that using “big bang” !!! like in mutate(!!!new_cols := NA) doesn’t work either. As far as I know, := is just an alias of = for the {rlang} parser, and as we know = cannot assign more than one variable at once (unlike Python, for example), which explains the error.↩︎\r\nNote that there are more motivated usescases of reduce() out there, mostly in doing mathy-things, and I’m by no means advocating that you should always use reduce() in our context - I just think it’s fun to play around with!↩︎\r\nThere’s also .dir argument that allows you to specify the direction, but not relevant here because when you pipe, the left-hand side is always the first input to the next function.↩︎\r\nIf it helps, think of it like ((((1 + 2) + 3) + 4) + 5)↩︎\r\nThe function passed into reduce() doesn’t have to be in {rlang} anonymous function syntax, but I like it so I’ll keep using it here.↩︎\r\nBy the way, we could also do this with purrr::map() since multiple ggplot2 layers can be stored into a list and added all together in one step. But then we can’t do this cool thing I’m going to show with accumulate() next!↩︎\r\nBy the way, if you want a whole package dedicated to animating and incrementally building {ggplot2} code, check out @EvaMaeRey’s {flipbookr} package!↩︎\r\nWe are still “iterating” over the numbers and background_colors vectors but in a round-about way by passing a vector of indices for reduce() to iterate over instead and using the indices to access elements of the two vectors. This actually seems like the way to go when you have more than two varying arguments because there’s no pmap() equavalent for reduce() like preduce().↩︎\r\nNote that we couldn’t do this with + in our {ggplot2} example because geom_point() doesn’t take a ggplot object as its first argument. Basically, the + operator is re-purposed as a class method for ggplot objects but it’s kinda complicated so that’s all I’ll say about that.↩︎\r\nNote the use of as.character() to make sure that the left-hand side of the walrus := is converted from numeric to character. Alternatively, using the new glue syntax support from dplyr > 1.0.0, we can simplify !!as.character(.y) := to \"{.y}\" :=↩︎\r\nIf you aren’t familiar with linear models in R, we know that “Adelie” is the reference level because there is no “speciesAdelie” term. The estimate for “Adelie” is represented by the “(Intercept)”!↩︎\r\nFiguring this out has caused some headaches and that’s what I get for not carefully reading the docs↩︎\r\nExcept dummy.code() also returns a column for the reference level whose value is always 1, which is kinda pointless↩︎\r\n",
    "preview": "posts/2020-12-13-collapse-repetitive-piping-with-reduce/reduce_ggplot.png",
    "last_modified": "2021-01-19T11:20:07+09:00",
    "input_file": {},
    "preview_width": 1233,
    "preview_height": 775
  },
  {
    "path": "posts/2020-11-08-plot-makeover-2/",
    "title": "Plot Makeover #2",
    "description": "Making a dodged-stacked hybrid bar plot in {ggplot2}",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-11-08",
    "categories": [
      "plot makeover",
      "data visualization",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nBefore\r\nMy Plan\r\nAfter\r\nFirst draft\r\nFinal touch-up\r\n\r\n\r\n\r\n\r\nknitr::opts_chunk$set(\r\n  comment = \" \",\r\n  echo = TRUE,\r\n  message = TRUE,\r\n  warning = TRUE,\r\n  R.options = list(width = 80)\r\n)\r\nxaringanExtra::use_clipboard()\r\n\r\n\r\n\r\nxaringanExtra::use_panelset()\r\n\r\n\r\n\r\nThis is the second installment of plot makeover where I take a plot in the wild and make very opinionated modifications to it.\r\nBefore\r\nOur plot-in-the-wild comes from (Yurovsky and Yu 2008), a paper on statistical word learning. The plot that I’ll be looking at here is Figure 2, a bar plot of accuracy in a 3-by-3 experimental design.\r\n\r\n\r\n\r\nFigure 1: Plot from Yurovsky and Yu (2008)\r\n\r\n\r\n\r\nAs you might notice, there’s something interesting going on in this bar plot. It looks like the red and green bars stack together but dodge from the blue bar. It’s looks a bit weird for me as someone who mainly uses {ggplot2} because this kind of a hybrid design is not explicitly supported in the API.\r\nFor this plot makeover, I’ll leave aside the issue of whether having a half-stacked, half-dodged bar plot is a good idea.1 In fact, I’m not even gonna focus much on the “makeover” part. Instead I’m just going to take a shot at recreating this plot (likely made in MATLAB with post-processing in PowerPoint) in {ggplot2}.\r\nMy Plan\r\nAgain, my primary goal here is replication. But I do want to touch up on some aesthetics while I’m at it.\r\nMajor Changes:\r\nMove the title to above the plot\r\nMove the legend inside the plot\r\nMove/remove the y-axis title so it’s not vertically aligned\r\nMinor Changes:\r\nRemove grid lines\r\nPut y-axis in percentages\r\nAdd white borders around the bars for clearer color contrast\r\nAfter\r\nFirst draft\r\nFor a first pass on the makeover, I wanted to get the hybrid design right.\r\nThe plot below isn’t quite there in terms of covering everything I laid out in my plan, but it does replicate the bar plot design specifically.\r\n\r\n\r\nPlot\r\n\r\n\r\n\r\n\r\n\r\nCode\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(extrafont)\r\n\r\ndf <- tribble(\r\n  ~Condition, ~Referent, ~Accuracy,\r\n  \"Primacy\",  \"Single\",  0.63,\r\n  \"Primacy\",  \"Primacy\", 0.59,\r\n  \"Recency\",  \"Single\",  0.63,\r\n  \"Recency\",  \"Recency\", 0.5,\r\n  \"Both\",     \"Single\",  0.63,\r\n  \"Both\",     \"Primacy\", 0.5,\r\n  \"Both\",     \"Recency\", 0.31\r\n) %>% \r\n  mutate(\r\n    error_low = runif(7, .04, .06),\r\n    error_high = runif(7, .04, .06),\r\n    Condition_name = factor(Condition, levels = c(\"Primacy\", \"Recency\", \"Both\")),\r\n    Condition = as.numeric(Condition_name),\r\n    Referent = factor(Referent, levels = c(\"Single\", \"Recency\", \"Primacy\")),\r\n    left = Referent == \"Single\",\r\n    color = case_when(\r\n      Referent == \"Single\" ~ \"#29476B\",\r\n      Referent == \"Primacy\" ~ \"#AD403D\",\r\n      Referent == \"Recency\" ~ \"#9BBB58\"\r\n    )\r\n  )\r\n\r\n\r\nggplot(mapping = aes(x = Condition, y = Accuracy, fill = color)) +\r\n  geom_col(\r\n    data = filter(df, left),\r\n    width = .3,\r\n    color = \"white\",\r\n    position = position_nudge(x = -.3)\r\n  ) +\r\n  geom_errorbar(\r\n    aes(ymin = Accuracy - error_low, ymax = Accuracy + error_high),\r\n    data = filter(df, left),\r\n    width = .1,\r\n    position = position_nudge(x = -.3)\r\n  ) +\r\n  geom_col(\r\n    data = filter(df, !left),\r\n    color = \"white\",\r\n    width = .3,\r\n  ) +\r\n  geom_errorbar(\r\n    aes(y = y, ymin = y - error_low, ymax = y + error_high),\r\n    data = filter(df, !left) %>% \r\n      group_by(Condition) %>% \r\n      mutate(y = accumulate(Accuracy, sum)),\r\n    width = .1\r\n  ) +\r\n  scale_fill_identity(\r\n    labels = levels(df$Referent),\r\n    guide = guide_legend(title = \"Referent\")\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = 1:3 - .15,\r\n    labels = levels(df$Condition_name),\r\n    expand = expansion(.1)\r\n  ) +\r\n  scale_y_continuous(\r\n    breaks = scales::pretty_breaks(6),\r\n    labels = str_remove(scales::pretty_breaks(6)(0:1), \"\\\\.0+\"),\r\n    limits = 0:1,\r\n    expand = expansion(0)\r\n  ) +\r\n  labs(\r\n    title = \"Exp1: Accuracy by Condition and Word Type\"\r\n  ) +\r\n  theme_classic(\r\n    base_family = \"Roboto\",\r\n    base_size = 16\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nAs you might guess from my two calls to geom_col() and geom_errorbar(), I actually split the plotting of the bars into two parts. First I drew the blue bars and their errorbars, then I drew the green and red bars and their errorbars.\r\nEffectively, the above plot is a combination of these two:2\r\n\r\n\r\n\r\nA bit hacky, I guess, but it works!\r\n\r\n\r\n\r\n\r\nFinal touch-up\r\n\r\n\r\n\r\n\r\n\r\nggplot(mapping = aes(x = Condition, y = Accuracy, fill = color)) +\r\n  geom_col(\r\n    data = filter(df, left),\r\n    width = .3,\r\n    color = \"white\",\r\n    position = position_nudge(x = -.3),\r\n  ) +\r\n  geom_errorbar(\r\n    aes(ymin = Accuracy - error_low, ymax = Accuracy + error_high),\r\n    data = filter(df, left),\r\n    width = .1,\r\n    position = position_nudge(x = -.3)\r\n  ) +\r\n  geom_col(\r\n    data = filter(df, !left),\r\n    color = \"white\",\r\n    width = .3, \r\n  ) +\r\n  geom_errorbar(\r\n    aes(y = y, ymin = y - error_low, ymax = y + error_high),\r\n    data = filter(df, !left) %>% \r\n      group_by(Condition) %>% \r\n      mutate(y = accumulate(Accuracy, sum)),\r\n    width = .1\r\n  ) +\r\n  geom_hline(\r\n    aes(yintercept = .25),\r\n    linetype = 2,\r\n    size = 1,\r\n  ) +\r\n  geom_text(\r\n    aes(x = 3.4, y = .29),\r\n    label = \"Chance\",\r\n    family = \"Adelle\",\r\n    color = \"grey20\",\r\n    inherit.aes = FALSE\r\n  ) +\r\n  scale_fill_identity(\r\n    labels = c(\"Single\", \"Primacy\", \"Recency\"),\r\n    guide = guide_legend(\r\n      title = NULL,\r\n      direction = \"horizontal\",\r\n      override.aes = list(fill = c(\"#29476B\", \"#AD403D\", \"#9BBB58\"))\r\n    )\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = 1:3 - .15,\r\n    labels = levels(df$Condition_name),\r\n    expand = expansion(c(.1, .05))\r\n  ) +\r\n  scale_y_continuous(\r\n    breaks = scales::pretty_breaks(6),\r\n    labels = scales::percent_format(1),\r\n    limits = 0:1,\r\n    expand = expansion(0)\r\n  ) +\r\n  labs(\r\n    title = \"Accuracy by Condition and Referent\",\r\n    y = NULL\r\n  ) +\r\n  theme_classic(\r\n    base_family = \"Roboto\",\r\n    base_size = 16\r\n  ) +\r\n  theme(\r\n    plot.title.position = \"plot\",\r\n    plot.title = element_text(\r\n      family = \"Roboto Slab\",\r\n      margin = margin(0, 0, 1, 0, \"cm\")\r\n    ),\r\n    legend.position = c(.35, .9),\r\n    axis.title.x = element_text(margin = margin(t = .4, unit = \"cm\")),\r\n    plot.margin = margin(1, 1, .7, 1, \"cm\")\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nYurovsky, Daniel, and C. Yu. 2008. Mutual Exclusivity in Cross-Situational Statistical Learning. https://dll.sitehost.iu.edu/papers/Yurovsky_cs08.pdf.\r\n\r\n\r\nI actually don’t even have a strong feeling about this. It does look kinda cool.↩︎\r\nI used a neat trick from the R Markdown Cookbook to get the plots printed side-by-side↩︎\r\n",
    "preview": "posts/2020-11-08-plot-makeover-2/plot-makeover-2_files/figure-html5/final-1.png",
    "last_modified": "2021-02-14T03:42:02+09:00",
    "input_file": "plot-makeover-2.utf8.md",
    "preview_width": 1344,
    "preview_height": 1152
  },
  {
    "path": "posts/2020-11-03-tidytuesday-2020-week-45/",
    "title": "TidyTuesday 2020 week 45",
    "description": "Waffle chart of IKEA furnitures in stock",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-11-03",
    "categories": [
      "ggplot2",
      "data visualization",
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nVisualization\r\nThings I learned\r\nThings to improve\r\n\r\nCode\r\n\r\n\r\n\r\n\r\nVisualization\r\n\r\n\r\n\r\nThings I learned\r\nHow to make waffle charts with {waffle} (finally!)\r\nUsing {patchwork} for a large list of plots using wrap_plots() and theme styling inside plot_annotation()\r\nWorking with a long canvas using the cairo_pdf() device\r\nUsing {ggfittext} for dynamically re-sizing annotations.\r\nThings to improve\r\nCouldn’t figure out background color for the entire visual and white ended up looking a bit too harsh on the eye\r\nIdeally would like to replace the squares with icons. Maybe I could’ve pursued that if I only plotted a couple furnitures.\r\nThe plot ended up being a bit too long. Again could’ve cut down a bit there, but I don’t mind it for this submission because I was more focused on learning how to make waffle charts at all.\r\nOops forgot to put in the data source\r\nCode\r\nAlso available on github\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(waffle)\r\nlibrary(extrafont)\r\nlibrary(patchwork)\r\n\r\ntuesdata <- tidytuesdayR::tt_load(2020, week = 45)\r\n\r\nikea_counts <- tuesdata$ikea %>% \r\n  count(category) %>% \r\n  mutate(n = round(n/5)) %>% \r\n  arrange(-n)\r\n\r\nikea_colors <- c(nord::nord_palettes$algoma_forest, dutchmasters::dutchmasters_pal()(13)[-c(1, 8, 12)])\r\n\r\nikea_waffles <- map(1:nrow(ikea_counts), ~ {\r\n  df <- slice(ikea_counts, .x)\r\n  ggplot(df) +\r\n    geom_waffle(\r\n      aes(fill = category, values = n),\r\n      n_rows = 20,\r\n      size = 1.5,\r\n      flip = TRUE,\r\n      show.legend = FALSE\r\n    ) +\r\n    scale_fill_manual(values = ikea_colors[.x]) +\r\n    ggfittext::geom_fit_text(\r\n      aes(xmin = -15, xmax = -5, ymin = .5, ymax = .5 + ceiling(df$n/20), label = category),\r\n      size = 54, grow = FALSE, fullheight = FALSE, place = \"left\" ,\r\n      family = \"Roboto Slab\", fontface = \"bold\"\r\n    ) +\r\n    coord_equal(xlim = c(-16, 21)) +\r\n    theme_void()\r\n})\r\n\r\nlegend_key <- ggplot() +\r\n  annotation_custom(rectGrob(0.5, 0.5, height = .02, width = .02, gp = gpar(fill = \"grey50\", color = \"black\", lwd = 1))) +\r\n  annotation_custom(textGrob(\"=  5 units\", gp = gpar(fontfamily = \"Roboto Slab\", fontface = \"bold\", fontsize = 12)), 3, 2.6) +\r\n  coord_equal(xlim = c(0, 5), ylim = c(0, 5)) +\r\n  theme_void()\r\n\r\npatched <- wrap_plots(ikea_waffles, ncol = 1) +\r\n  plot_annotation(\r\n    title = \"<span style='color:#997A00'>IKEA<\/span> <span style='color:#001F5C'>Furnitures in Stock<\/span>\",\r\n    caption = \"@yjunechoe\",\r\n    theme = theme(\r\n      plot.title = ggtext::element_markdown(\r\n        size = 100,\r\n        family = \"Noto\",\r\n        face = \"bold\",\r\n        hjust = .5,\r\n        margin = margin(t = 1.5, b = 2, unit = \"in\")\r\n      ),\r\n      plot.caption = element_text(\r\n        size = 32,\r\n        family = \"IBM Plex Mono\",\r\n        face = \"bold\",\r\n        margin = margin(t = 1, b = 1, unit = \"in\")\r\n      ),\r\n      plot.margin = margin(2, 2, 2, 2, unit = \"in\")\r\n    )\r\n  ) &\r\n  theme(plot.margin = margin(t = .5, b = .5, unit = \"in\")) \r\n\r\n\r\nggsave(\"tidytuesday_2020_45.pdf\", patched, device = cairo_pdf, scale = 2, width = 12, height = 26, limitsize = FALSE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-03-tidytuesday-2020-week-45/preview.png",
    "last_modified": "2020-12-19T23:05:08+09:00",
    "input_file": {},
    "preview_width": 4443,
    "preview_height": 2950
  },
  {
    "path": "posts/2020-10-28-tidytuesday-2020-week-44/",
    "title": "TidyTuesday 2020 week 44",
    "description": "Patched animation of the location and cumulative capacity of wind turbines in Canada",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-10-28",
    "categories": [
      "ggplot2",
      "gganimate",
      "spatial",
      "data visualization",
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nVisualization\r\nThings I learned\r\nThings to improve\r\n\r\nCode\r\n\r\n\r\n\r\n\r\nVisualization\r\n\r\n\r\n\r\nThings I learned\r\nUsing {magick} for animation composition, thanks to the {gganimate} wiki\r\nThe very basics of working with spatial data with {rnaturalearth} and {sf}1\r\nA bit about color schemes for maps (I particularly love this color  as a way of de-emphasizing territories in the background)\r\nThings to improve\r\nI couldn’t figure out how to add margins to the bottom, but I now realize that I could’ve just played around with expansion() for the y-axis of the bar animation plot.\r\nImage composition took a while to render, which was a bit frustrating. Need to find a way to speed that up.\r\nCode\r\nAlso available on github\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(gganimate)\r\nlibrary(extrafont)\r\n\r\ntuesdata <- tidytuesdayR::tt_load(2020, week = 44)\r\n\r\nwind_turbine <- tuesdata$`wind-turbine` %>% \r\n  select(\r\n    ID = objectid,\r\n    Province = province_territory,\r\n    Capacity = total_project_capacity_mw,\r\n    Diameter = rotor_diameter_m,\r\n    Height = hub_height_m,\r\n    Year = commissioning_date,\r\n    Lat = latitude,\r\n    Lon = longitude\r\n  ) %>% \r\n  arrange(Year, -Diameter) %>% \r\n  mutate(\r\n    Year = as.integer(str_match(Year, \"^\\\\d{4}\")[,1])\r\n  )\r\n\r\n\r\n\r\nne_map <- rnaturalearth::ne_countries(scale='medium', returnclass = 'sf')\r\n\r\nturbine_anim <- wind_turbine %>% \r\n  ggplot() +\r\n  geom_rect(\r\n    aes(xmin = -150, xmax = -50, ymin = 40, ymax = 72),\r\n    fill = \"#B6D0D1\"\r\n  ) +\r\n  geom_sf(\r\n    aes(fill = ifelse(admin == \"Canada\", \"#7BC86C\", \"#FFF8DC\")),\r\n    show.legend = FALSE,\r\n    data = filter(ne_map, admin %in% c(\"Canada\", \"United States of America\"))\r\n  ) +\r\n  scale_fill_identity() +\r\n  geom_point(\r\n    aes(Lon, Lat, group = ID, size = Capacity),\r\n    show.legend = FALSE, alpha = 0.5, color = \"#3C59FF\"\r\n  ) +\r\n  geom_text(\r\n    aes(x = -138, y = 43, label = as.character(Year)),\r\n    size = 24, color = \"grey35\", family = \"Roboto Slab\"\r\n  ) +\r\n  geom_rect(\r\n    aes(xmin = -150, xmax = -50, ymin = 40, ymax = 72),\r\n    fill = \"transparent\", color = \"black\"\r\n  ) +\r\n  coord_sf(\r\n    xlim = c(-150, -50),\r\n    ylim = c(40, 72),\r\n    expand = FALSE,\r\n    clip = \"on\"\r\n  ) +\r\n  ggtitle(\"Canadian Wind Turbines\") +\r\n  theme_void() +\r\n  theme(\r\n    plot.title = element_text(family = \"Adelle\", s),\r\n    plot.margin = margin(1, 1, 1, 1, \"cm\")\r\n  ) +\r\n  transition_reveal(Year)\r\n\r\nanimate(turbine_anim, width = 1000, height = 600, nframes = 100)\r\n\r\n\r\n\r\ncapacity_data <- wind_turbine %>% \r\n  group_by(Year) %>% \r\n  summarize(\r\n    Capacity = sum(Capacity),\r\n    .groups = 'drop'\r\n  ) %>% \r\n  mutate(\r\n    Capacity = accumulate(Capacity, sum),\r\n    width = (Capacity/max(Capacity)) * 70\r\n  )\r\n\r\ncapacity_anim <- capacity_data %>% \r\n  ggplot(aes(x = 1, y = Capacity)) +\r\n  geom_col(\r\n    fill = \"#3C59FF\",\r\n  ) +\r\n  geom_text(\r\n    aes(label = paste(as.character(round(Capacity * 0.001)), \"GW\")),\r\n    hjust = -.2,\r\n    family = \"IBM Plex Mono\"\r\n  ) +\r\n  scale_y_continuous(expand = expansion(c(.1, .4))) +\r\n  coord_flip() +\r\n  theme_void() +\r\n  transition_states(Year)\r\n\r\nanimate(capacity_anim, res = 300, width = 1000, height = 100, nframes = 100)\r\n\r\n\r\nlibrary(magick)\r\n\r\nmap_gif <- image_read(\"turbine_map.gif\")\r\nbar_gif <- image_read(\"capacity_bar.gif\")\r\n\r\nnew_gif <- image_append(c(map_gif[1], bar_gif[1]), stack = TRUE)\r\n\r\nfor(i in 2:100){\r\n  combined <- image_append(c(map_gif[i], bar_gif[i]), stack = TRUE)\r\n  new_gif <- c(new_gif, combined)\r\n}\r\n\r\nnew_gif\r\n\r\n\r\n\r\n\r\nIf I don’t count all the convenient US-centric data/packages I’ve used to plot American maps before, this would be the first map I’ve made from scratch.↩︎\r\n",
    "preview": "posts/2020-10-28-tidytuesday-2020-week-44/preview.png",
    "last_modified": "2020-11-05T07:45:58+09:00",
    "input_file": {},
    "preview_width": 735,
    "preview_height": 541
  },
  {
    "path": "posts/2020-10-22-analysis-of-everycolorbots-tweets/",
    "title": "Analysis of @everycolorbot's tweets",
    "description": "And why you should avoid neon colors",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-10-22",
    "categories": [
      "data visualization",
      "ggplot2",
      "rtweet",
      "colors"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetup\r\nAnalysis\r\nConclusion\r\n\r\nIntroduction\r\n\r\n\r\n.column {\r\n  float: left;\r\n  width: 50%;\r\n}\r\n\r\n.row:after {\r\n  content: \"\";\r\n  display: table;\r\n  clear: both;\r\n}\r\n\r\n.sc {\r\n  font-variant: small-caps;\r\n  letter-spacing: 0.1em;\r\n}\r\n\r\nI, along with nearly two-hundred thousand other people, follow @everycolorbot on twitter. @everycolorbot is a twitter bot that tweets an image of a random color every hour (more details on github). It’s 70% a source of inspiration for new color schemes, and 30% a comforting source of constant in my otherwise hectic life.\r\nWhat I’ve been noticing about @everycolorbot’s tweets is that bright, highly saturated neon colors (yellow~green) tend to get less likes compared to cool blue colors and warm pastel colors. You can get a feel of this difference in the number of likes between the two tweets below, tweeted an hour apart:\r\n\r\n\r\n\r\n\r\n0x54e14b pic.twitter.com/Aw0cwm7uy8\r\n\r\n— Every Color (@everycolorbot) October 15, 2020\r\n\r\n\r\n\r\n\r\n\r\n0xaa70a5 pic.twitter.com/NMBF3mffS4\r\n\r\n— Every Color (@everycolorbot) October 15, 2020\r\n\r\n\r\n\r\nThis is actually not a big surprise. Bright pure colors are very harsh and straining to the eye, especially on a white background.1 For this reason bright colors are almost never used in professional web design, and are also discouraged in data visualization.\r\nSo here’s a mini experiment testing that claim: I’ll use @everycolorbot’s tweets (more specifically, the likes on the tweets) as a proxy for likeability/readability/comfortableness/etc. It’ll be a good exercise for getting more familiar with different colors! I’m also going to try a simple descriptive analysis using the HSV color representation, which is a psychologically-motivated mental model of color that I like a lot (and am trying to get a better feel for).\r\n\r\n\r\n\r\nFigure 1: HSV cylinder\r\n\r\n\r\n\r\nSetup\r\nUsing {rtweet} requires authentication from twitter. The steps to do so are very well documented on the package website so I wouldn’t expect too much trouble setting it up if it’s your first time using it. But just for illustration, here’s what my setup looks like:\r\n\r\n\r\napi_key <- 'XXXXXXXXXXXXXXXXXXXXX'\r\napi_secret_key <- 'XXXXXXXXXXXXXXXXXXXXX'\r\naccess_token <- \"XXXXXXXXXXXXXXXXXXXXX\"\r\naccess_token_secret <- \"XXXXXXXXXXXXXXXXXXXXX\"\r\n\r\ntoken <- create_token(\r\n  app = \"XXXXXXXXXXXXXXXXXXXXX\",\r\n  consumer_key = api_key,\r\n  consumer_secret = api_secret_key,\r\n  access_token = access_token,\r\n  access_secret = access_token_secret\r\n)\r\n\r\n\r\n\r\nAfter authorizing, I queried the last 10,000 tweets made by @everycolorbot. It ended up only returning about a 1/3 of that because the twitter API only allows you to go back so far in time, but that’s plenty for my purposes here.\r\n\r\n\r\n\r\n\r\n\r\ncolortweets <- rtweet::get_timeline(\"everycolorbot\", 10000)\r\n\r\ndim(colortweets)\r\n\r\n\r\n\r\n\r\n  [1] 3238   90\r\n\r\nAs you see above, I also got back 90 variables (columns). I only care about the time of the tweet, the number of likes it got, and the color it tweeted, so those are what I’m going to grab. I also want to clean things up a bit for plotting, so I’m going to grab just the hour from the time and just the hex code from the text.\r\n\r\n\r\ncolortweets_df <- colortweets %>% \r\n  select(created_at, text, favorite_count) %>%\r\n  mutate(\r\n    created_at = lubridate::hour(created_at),\r\n    text = paste0(\"#\", str_extract(text, \"(?<=0x).*(?= )\"))\r\n  ) %>% \r\n  rename(\r\n    likes = favorite_count,\r\n    hour = created_at,\r\n    hex = text\r\n  )\r\n\r\n\r\n\r\nAnd here’s what we end up with:\r\n\r\n\r\nlikes\r\n\r\n\r\nhour\r\n\r\n\r\nhex\r\n\r\n\r\n36\r\n\r\n\r\n15\r\n\r\n\r\n#65f84e\r\n\r\n\r\n65\r\n\r\n\r\n14\r\n\r\n\r\n#32fc27\r\n\r\n\r\n89\r\n\r\n\r\n13\r\n\r\n\r\n#997e13\r\n\r\n\r\n140\r\n\r\n\r\n12\r\n\r\n\r\n#ccbf09\r\n\r\n\r\n303\r\n\r\n\r\n11\r\n\r\n\r\n#665f84\r\n\r\n\r\n75\r\n\r\n\r\n10\r\n\r\n\r\n#b32fc2\r\n\r\n\r\nHere is the link to this data if you’d like to replicate or extend this analysis yourself.\r\nAnalysis\r\nBelow is a bar plot of colors where the height corresponds to the number of likes. It looks cooler than your usual bar plot because I transformed the x dimension into polar coordinates. My intent in doing this was to control for the hour of day in my analysis and visualize it like a clock (turned out better than expected!)\r\n\r\n\r\ncolortweets_df %>% \r\n  arrange(-likes) %>% \r\n  ggplot(aes(hour, likes, color = hex)) +\r\n  geom_col(\r\n    aes(size = likes),\r\n    position = \"dodge\",\r\n    show.legend = FALSE\r\n  ) +\r\n  scale_color_identity() +\r\n  theme_void() +\r\n  theme(\r\n    plot.background = element_rect(fill = \"#222222\", color = NA),\r\n  ) +\r\n  coord_polar()\r\n\r\n\r\n\r\n\r\nCheck out my use of arrange() here: it’s how I tell ggplot to plot the longer bars first then the smaller bars, minimizing the overlap!\r\n\r\n\r\n\r\nI notice at least two interesting contrasts in this visualization:\r\nNeon colors (yellow, green, pink) and dark brown and black seems to dominate the center (least liked colors) while warm red~blue pastel colors dominate around the edges (most liked colors)\r\nThere also seems to be a distinction between pure blue and red in the inner-middle circle vs. the green~blue pastel colors in the outer-middle circle.\r\nSo maybe we can say that there are four clusters here:\r\nLeast liked: Bright neon colors + highly saturated dark colors\r\nLesser liked: Bright pure/near-pure colors\r\nMore liked: Darker pastel RGB\r\nMost liked: Lighter pastel mixed colors\r\nNow’s let’s try to quantitatively describe each cluster.\r\nFirst, as a sanity check, I’m just gonna eyeball the range of likes for each cluster using an un-transformed version of the above plot with units. I think we can roughly divide up the clusters at 100 likes, 200 likes, and 400 likes.\r\n\r\n\r\ncolortweets_df %>% \r\n  arrange(-likes) %>% \r\n  ggplot(aes(hour, likes, color = hex)) +\r\n  geom_col(\r\n    aes(size = likes),\r\n    position = \"dodge\",\r\n    show.legend = FALSE\r\n  ) +\r\n  geom_hline(\r\n    yintercept = c(100, 200, 400), \r\n    color = \"white\", \r\n    linetype = 2, \r\n    size = 2\r\n  ) +\r\n  scale_y_continuous(breaks = scales::pretty_breaks(10)) +\r\n  scale_color_identity() +\r\n  theme_void() +\r\n  theme(\r\n    plot.background = element_rect(fill = \"#222222\", color = NA),\r\n    axis.line.y = element_line(color = \"white\"),\r\n    axis.text.y = element_text(\r\n      size = 14,\r\n      color = \"white\",\r\n      margin = margin(l = 3, r = 3, unit = \"mm\")\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\nIf our initial hypothesis about the four clusters are true, we should see these clusters having distinct profiles. Here, I’m going to use the HSV representation to quantitatively test this. To convert our hex values into HSV, I use the as.hsv() function from the {chroma} package - an R wrapper for the javascript library of the same name.\r\n\r\n\r\ncolortweets_df_hsv <- colortweets_df %>% \r\n  mutate(hsv = map(hex, ~as_tibble(chroma::as.hsv(.x)))) %>% \r\n  unnest(hsv)\r\n\r\n\r\n\r\n\r\nActually, I used furrr::future_map() here myself because I found the hex-hsv conversion to be sorta slow.\r\nAnd now we have the HSV values (hue, saturation, value)!\r\n\r\n\r\nlikes\r\n\r\n\r\nhour\r\n\r\n\r\nhex\r\n\r\n\r\nh\r\n\r\n\r\ns\r\n\r\n\r\nv\r\n\r\n\r\n36\r\n\r\n\r\n15\r\n\r\n\r\n#65f84e\r\n\r\n\r\n111.88235\r\n\r\n\r\n0.6854839\r\n\r\n\r\n0.9725490\r\n\r\n\r\n65\r\n\r\n\r\n14\r\n\r\n\r\n#32fc27\r\n\r\n\r\n116.90141\r\n\r\n\r\n0.8452381\r\n\r\n\r\n0.9882353\r\n\r\n\r\n89\r\n\r\n\r\n13\r\n\r\n\r\n#997e13\r\n\r\n\r\n47.91045\r\n\r\n\r\n0.8758170\r\n\r\n\r\n0.6000000\r\n\r\n\r\n140\r\n\r\n\r\n12\r\n\r\n\r\n#ccbf09\r\n\r\n\r\n56.00000\r\n\r\n\r\n0.9558824\r\n\r\n\r\n0.8000000\r\n\r\n\r\n303\r\n\r\n\r\n11\r\n\r\n\r\n#665f84\r\n\r\n\r\n251.35135\r\n\r\n\r\n0.2803030\r\n\r\n\r\n0.5176471\r\n\r\n\r\n75\r\n\r\n\r\n10\r\n\r\n\r\n#b32fc2\r\n\r\n\r\n293.87755\r\n\r\n\r\n0.7577320\r\n\r\n\r\n0.7607843\r\n\r\n\r\nWhat do we get if we average across the dimensions of HSV for each cluster?\r\n\r\n\r\ncolortweets_df_hsv <- colortweets_df_hsv %>% \r\n  mutate(\r\n    cluster = case_when(\r\n      likes < 100 ~ \"Center\",\r\n      between(likes, 100, 200) ~ \"Inner-Mid\",\r\n      between(likes, 201, 400) ~ \"Outer-Mid\",\r\n      likes > 400 ~ \"Edge\"\r\n    ),\r\n    cluster = fct_reorder(cluster, likes)\r\n  )\r\n\r\ncolortweets_df_hsv %>% \r\n  group_by(cluster) %>% \r\n  summarize(across(h:v, mean), .groups = 'drop')\r\n\r\n\r\n  # A tibble: 4 x 4\r\n    cluster       h     s     v\r\n  * <fct>     <dbl> <dbl> <dbl>\r\n  1 Center     121. 0.770 0.710\r\n  2 Inner-Mid  191. 0.734 0.737\r\n  3 Outer-Mid  197. 0.589 0.710\r\n  4 Edge       249. 0.304 0.832\r\n\r\nThis actually matches up pretty nicely with our initial analysis! We find a general dislike for green colors (h value close to 120) over blue colors (h value close to 240), as well as a dislike for highly saturated colors (intense, bright) over those with low saturation (which is what gives off the “pastel” look). To help make the hue values more interpretable, here’s a color wheel with angles that correspond to the hue values in HSV.2\r\n\r\n\r\n\r\nFigure 2: Hue color wheel\r\n\r\n\r\n\r\nBut we also expect to find within-cluster variation along HSV. In particular, hue is kind of uninterpretable on a scale so it probably doesn’t make a whole lot of sense to take a mean of that. So back to the drawing plotting board!\r\nSince saturation and value do make more sense on a continuous scale, let’s draw a scatterplot for each cluster with saturation on the x-axis and value on the y-axis. I’m also going to map hue to the color of each point, but since hue is abstract on its own, I’m actually just going to replace it with the hex values (i.e., the actual color).\r\n\r\n\r\ncolortweets_df_hsv %>% \r\n  ggplot(aes(s, v, color = hex)) +\r\n  geom_point() +\r\n  scale_color_identity() +\r\n  lemon::facet_rep_wrap(~cluster) +\r\n  theme_void(base_size = 16, base_family = \"Montserrat Medium\") +\r\n  theme(\r\n    plot.margin = margin(3, 5, 5, 5, \"mm\"),\r\n    strip.text = element_text(margin = margin(b = 3, unit = \"mm\")),\r\n    panel.border = element_rect(color = \"black\", fill = NA),\r\n    panel.background = element_rect(fill = \"grey75\", color = NA)\r\n  )\r\n\r\n\r\n\r\n\r\nHere’s the mappings spelled out again:\r\nsaturation (how colorful a color is) is mapped to the X-dimension\r\nvalue (how light a color is) is mapped to the Y-dimension\r\nhex (the actual color itself) is mapped to the COLOR dimension\r\n\r\nOur plot above reinforce what we’ve found before. Colors are more likeable (literally) the more they…\r\nMove away from green: Neon-green dominates the least-liked cluster, and that’s a blatant fact. Some forest-greens survive to the lesser-liked cluster, but is practically absent in the more-liked cluster and most-liked cluster. It looks like the only way for green to be redeemable is to either mix in with blue to become cyan and turquoise, which dominates the more-liked cluster, or severly drop in saturation to join the ranks of other pastel colors in the most-liked cluster.\r\nIncrease in value and decrease in saturation: It’s clear that the top-left corner is dominated by the more-liked and the most-liked cluster. That region is, again, where pastel colors live. They’re calmer than the bright neon colors that plague the least-liked cluster, and are more liked than highly-saturated and intense colors like those in the top right of the Outer-Mid panel. So perhaps this is a lesson that being “colorful” can only get you so far.\r\nConclusion\r\nObviously, all of this should be taken with a grain of salt. We don’t know the people behind the likes - their tastes, whether they see color differently, what medium they saw the tweet through, their experiences, etc.\r\nAnd of course, we need to remind ourselves that we rarely see a color just by itself in the world. It contrasts and harmonizes with other colors in the environment in very complex ways.\r\nBut that’s what kinda makes our analysis cool - despite all these complexities, we see evidence for many things that experts working with color emphasize: avoid pure neon, mix colors, etc. This dataset also opens us up to many more types of analyses (like an actual cluster analysis) that might be worth looking into.\r\nGood stuff.\r\n\r\nDark mode ftw!↩︎\r\nWhile all color wheels look the same, they aren’t all oriented the same. When using HSV, make sure to reference the color wheel where the red is at 0, green is as 120, and blue is at 240.↩︎\r\n",
    "preview": "posts/2020-10-22-analysis-of-everycolorbots-tweets/preview.png",
    "last_modified": "2021-02-14T03:28:58+09:00",
    "input_file": "analysis-of-everycolorbots-tweets.utf8.md",
    "preview_width": 2433,
    "preview_height": 2259
  },
  {
    "path": "posts/2020-10-13-designing-guiding-aesthetics/",
    "title": "Designing guiding aesthetics",
    "description": "The fine line between creativity and noise",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-10-13",
    "categories": [
      "data visualization",
      "ggplot2",
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nVisualization\r\nReflections on guiding aesthetics\r\nWhy bother with guiding aesthetics?\r\nDesigning guiding aesthetics\r\nMy thought process\r\nCode\r\n\r\n\r\n\r\nknitr::opts_chunk$set(\r\n  comment = \" \",\r\n  echo = TRUE,\r\n  message = TRUE,\r\n  warning = TRUE,\r\n  R.options = list(width = 80)\r\n)\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nVisualization\r\n\r\n\r\n\r\nReflections on guiding aesthetics\r\nAdmittedly, the plot itself is quite simple, but I learned a lot from this process. So, breaking from the usual format of my tidytuesday blogposts, I want to talk about the background and motivation behind the plot as this was a big step in a new (and exciting!) direction for me that I’d like to document.\r\nJust for clarification, I’m using the term guiding aesthetics to refer to elements of the plot that do not represent a variable in the data, but serves to emphasize the overall theme or topic of the being visualized. So the mountains in my plot do not themselves contain any data, but it’s the thing that tells readers that the plot is about mountains (a valuable, but different kind of info!). But more on that later.\r\nWhy bother with guiding aesthetics?\r\nThis was my first time adding a huge element to a plot that wasn’t meaningful, in the sense of representing the data. As someone in academia working on obscure topics (as one does in academia), I’m a firm believer in making your plots as simple and minimal as possible. So I, like many others, think that it’s always a huge risk to add elements that are not absolutely necessary.\r\nBut as you might imagine, I first fell in love with data visualization not because of how objective and straightforward they are, but because of how eye-catching they can be. Like, when I was young I used to be really into insects. In fact, I read insect encyclopedias as a hobby. That TMI is relevant because those are full of data(!) visualizations that employ literal mappings of data, since those are easy to interpret for children. For example, consider the following diagram of the life cycle of the Japanese beetle from the USDA.\r\n\r\n\r\n\r\nFigure 1: Diagram of the life cycle of the Japanese beetle\r\n\r\n\r\n\r\nAs a child, I never appreciated/realized all the data that was seamlessly packed into diagrams like this. But with my Grammar of Graphics lens on, I can now see that:\r\nThe developmental stage at each month is mapped to x\r\nThe depth at which the developing beetle lives at each stage is mapped to y\r\nThe appearance of the beetle at each developmental stage is mapped to shape\r\nThe size of the beetle at each developmental stage is mapped to, well, size\r\nBut I could have easily plotted something like this instead:\r\n\r\n\r\nbeetles <- tibble::tribble(\r\n  ~Month, ~Depth,   ~Stage, ~Size,\r\n   \"JAN\",    -10,  \"Larva\",    10,\r\n   \"FEB\",     -8,  \"Larva\",    12,\r\n   \"MAR\",     -7,  \"Larva\",    14,\r\n   \"APR\",     -7,  \"Larva\",    14,\r\n   \"MAY\",     -6,   \"Pupa\",    11,\r\n   \"JUN\",      0, \"Beetle\",    12,\r\n   \"JUL\",      0, \"Beetle\",    12,\r\n   \"AUG\",     -3,  \"Larva\",     1,\r\n   \"SEP\",     -2,  \"Larva\",     2,\r\n   \"OCT\",     -1,  \"Larva\",     4,\r\n   \"NOV\",     -3,  \"Larva\",     5,\r\n   \"DEC\",     -8,  \"Larva\",     7\r\n)\r\n\r\nbeetles$Month <- fct_inorder(beetles$Month)\r\n\r\nggplot(beetles, aes(x = Month, y = Depth, size = Size, shape = Stage)) +\r\n  geom_point() +\r\n  ggtitle(\"The lifecycle of the Japanese beetle\")\r\n\r\n\r\n\r\n\r\nBoth visuals represent the data accurately, but of course the diagram looks better. And not just because it’s complex, but also because it exploits the associations between aesthetic dimensions and their meanings, as well as the strength of those associations.\r\nFor example, the shape dimension literally corresponds to shape, and the shape of the different developmental stages of the beetle are unique enough for there to be interesting within-stage variation and still be recognizable - e.g., the beetle looks different crawling out the ground in June and entering back in August, but it’s still recognizable as a beetle (and the same beetle at that!). This would’ve been difficult if the variable being mapped to shape was something arbitrary and abstract, like the type of protein that’s most produced at a particular stage. The diagram thus exploits the strength of the association between the shape dimension and the literal shapes of the beetle to represent the developmental stages. And it should be clear now that the same goes for y and size.\r\nHow about x? There’s no such strong/literal interpretation of the x-dimension - at best it just means something horizontal. So it’s actually fitting that a similarly abstract concept like the passage of time is mapped to x. We understand time linearly, and often see time as the x-axis in other plots, so it fits pretty naturally here.\r\nLastly, let’s talk about the color dimension. Even though no information was actually mapped to color, we certainly are getting some kind of information from the colors used in the plot. Literally put, we’re getting the information that the grass is green and the soil is brown. Now, that information is actually not representing any data that we care about so it’s technically visual noise, but it helps bring forward the overall theme of the diagram. While this worked out in the end, notice now that you have effectively thrown out color as a dimension that can convey meaningful information. That was a necessary tradeoff, but a well motivated one, since the information that the diagram is trying to convey doesn’t really need color.\r\nDesigning guiding aesthetics\r\nI’m hardly the expert, but I found it helpful to think of the process as mediating the tug of war between the guiding aesthetic and the variables in the data as they fight over space in different mapping dimensions.\r\nThis meant I had to make some changes to my usual workflow for explanatory data visualization, which mostly goes something like this:\r\nTake my response variable and map it to y\r\nFigure out the distribution of my response variables and choose the geom (e.g., boxplot, histogram).\r\nMap my dependent variables to other dimensions - this usually ends at either just x or x + facet groupings\r\nBut if I’m trying to incorporate guiding aesthetics, my workflow would look more like this:\r\nStart with a couple ideas for a visual representation of the topic (scenes, objects, etc.)\r\nFigure out the dimensions that the variables in the data can be mapped to\r\nFigure out the dimensions that each visual representation would intrude in\r\nMake compromises between (2) and (3) in a way that maximizes the quality of the data and the visual representation\r\nOf course, this kind of flexibility is unique to exploratory data visualization, in particular to the kinds where none of the variable is significant or interesting a priori. Of course in real life there will be a lot more constraints, but because we can assume a great degree of naivety towards the data for #tidytuesday, I get to pick and choose what I want to plot (which makes #tidytuesday such a great place to practice baby steps)!\r\nFor illustration, here’s my actual thought process while I was making the #tidytuesday plot.\r\nWARNING: a very non-linear journey ahead!\r\nMy thought process\r\nThe topic was about Himalayan climbing expeditions, so I wanted my visual to involve a mountain shape. The most obvious idea that came to mind was to draw a mountain where the peak of that mountain corresponded with their height_metres, a variable from the peaks data. It’s straightforward and intuitive! So I drew a sketch (these are the original rough sketches - please forgive the quality!)\r\n\r\n\r\n\r\nFigure 2: The first guiding aesthetic idea\r\n\r\n\r\n\r\nBut this felt… a bit empty. I threw in Mount Everest, but now what? I still had data on hundreds of more Himalayan peaks that were in the dataset. Just visualizing Mount Everest is not badly motivated per se (it’s the most famous peak afterall), but it wouldn’t make an interesting plot since there wasn’t much data on individual peaks. I wanted to add a few more mountain shapes, but I struggled to find a handful of peaks that formed a coherent group. I knew that if I wanted to go with the idea of mountain shapes as the guiding aesthetic, I could only manage to fit about a dozen or so without it looking too crowded.\r\nI put that issue aside for the moment and moved on while trying to accommodate for the possibility that I may have to fit in many more peaks. I thought about having a single mountain shape just for Mount Everest, and a point with a vertical spikeline for all other peaks to emphasize the y-axis representing height_metres.\r\n\r\n\r\n\r\nFigure 3: The second guiding aesthetic idea\r\n\r\n\r\n\r\nAt this point I started thinking about the x-axis. If I do use points to represent peaks (specifically, peak height), where would I actually position each point? Just randomly along the x-axis? It really started hitting me at this point that the quality of my data was pretty abysmal. Even if I ended up with a pretty visualization, I didn’t think I could justify calling it a data visualization. I felt that it’d be a reach to use complex visuals just to communicate a single variable.\r\nI toyed around with enriching the data being plotted. What if I use size of the dots to represent the average age of climbers who reached the peak, from the memmbers data? Or what if I used shape of country flags on top of the dots to represent the country that first reached the peak, from the expeditions data?\r\nThese were all cool ideas, but I kept coming back to the need to make the x-dimension meaningful. It just stood out too much. I didn’t think I could prevent the reader from expecting some sort of a meaning from the positioning of the dots along the x-axis.\r\nSo I went back to Step #2. I gathered up all the continuous variables across the three data in the #tidytuesday dataset (peaks, members, expeditions) and evaluated how good of a candidate each of them were for being mapped to x. This was the most time-consuming part of the process, and I narrowed it down to three candidates:\r\nexpeditions$members: looked okay at first, but once I started aggregating (averaging) by peak, the distribution became quite narrow. That made it less interesting and not very ideal for mountain shapes (the typical mountain shape is wider than they are tall).\r\nmembers$age: has a nice distribution and a manageable range with no extreme outliers.\r\npeaks$first_ascent_year: also has the above features + doesn’t need to be aggregated in some way, so the x-axis would have a very straight forward interpretation.\r\nThe first_ascent_year variable looked the most promising, so that’s what I pursued (and ended up ultimately adopting!).\r\n\r\n\r\n\r\nFigure 4: The third guiding aesthetic idea\r\n\r\n\r\n\r\nNow I felt like I had more direction to tackle the very first issue that I ran into during this process: the problem of picking out a small set of peaks that were interesting and well-motivated. I played around more with several options, but I ultimately settled on something very simple - the top 10 most popular peaks. Sure it’s overused and not particularly exciting, but that was a sacrifice that my over-worked brain was willing to make at the time.\r\nAnd actually, it turned out to be a great fit with my new x variable! It turns out that the top 10 most climbed peaks are also those that were among the first to be climbed (a correlation that sorta makes sense), so this set of peaks had an additional benefit of localizing the range of x to between the 1940s-1960s. And because 10 was a manageable number, I went ahead with my very first idea of having a mountain accompanying each point, where the peaks represent the peak of the guiding aesthetic (the mountain shape) as well as the height_metres and first_ascent_year.\r\nFinally, it came time for me to polish up on the mountains. I needed to decide on features of the mountains like how wide the base is, how many valleys and peaks it has, how tall the peaks are relative to each other, etc. I had to be careful that these superfluous features do not encroach on the dimensions where I mapped my data to - the x and y. Here, I had concerns about two of the mountain features in particular: base width and smaller peaks:\r\nThe base width was troubling because how wide the base of the mountain stretches could be interpreted as representing another variable that has to do with year (like the first and last time it was climbed, for example). This was a bit difficult to deal with, but I settled on a solution which was to keep the base width constant. By not having that feature vary at all, I could suppress any expectation for it to carry some sort of meaning. It’s kind of like how when you make a scatterplot with variables mapped to x and y, you don’t imbue any special meaning to the fact that the observations are represented by a circle (point), beacuse all of them are that shape. If they varied in any way, say you also have some rectangles and triangles, then you’d start expecting the shape to represent something meaningful.\r\nThe smaller peaks of the mountain shapes were troubling because I was already using the peak to represent the height. It helped that the actual peaks representing the data were also marked by a point and a label of the peak name. But to make it extra clear that the they were pure noise, I decided to randomly generate peaks and valleys, and tried to make that obvious. In the code attached at the bottom of this post, several parameters of the mountain-generating function allowed me to do this. It also helped that I added a note saying that the mountains were randomly generated when I tweeted it, which is kind of cheating perhaps, but it worked!\r\n\r\n\r\nI've been feeling particularly inspired by this week's #TidyTuesday so I made another plot! This is a simple scatterplot of peak height by year of first ascent, but with a twist: each point is also represented by the peak of a randomly generated mountain! #rstats pic.twitter.com/CqNQjdMYXP\r\n\r\n— June (@yjunechoe) September 25, 2020\r\n\r\nThat wraps up my long rant on how I made my mountains plot! For more context, making the plot took about a half a day worth of work, which isn’t too bad for a first attempt! Definitely looking forward to getting more inspirations like this in the future.\r\nCode\r\nAlso available on github\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\nmake_mountain <- function(x_start, x_end, base = 0, peak_x, peak_y, n_peaks = 3, peaks_ratio = 0.3, side.first = \"left\") {\r\n  \r\n  midpoint_abs <- (peak_y - base)/2 + base\r\n  midpoint_rel <- (peak_y - base)/2\r\n  \r\n  side_1_n_peaks <- floor(n_peaks/2)\r\n  side_2_n_peaks <- n_peaks - side_1_n_peaks -1\r\n  \r\n  side_1_x <- seq(x_start, peak_x, length.out = side_1_n_peaks * 2 + 2)\r\n  side_1_x <- side_1_x[-c(1, length(side_1_x))]\r\n  \r\n  side_2_x <- seq(peak_x, x_end, length.out = side_2_n_peaks * 2 + 2)\r\n  side_2_x <- side_2_x[-c(1, length(side_2_x))]\r\n  \r\n  side_1_y <- numeric(length(side_1_x))\r\n  side_1_y[c(TRUE, FALSE)] <- runif(length(side_1_y)/2, midpoint_abs, midpoint_abs + midpoint_rel * peaks_ratio)\r\n  side_1_y[c(FALSE, TRUE)] <- runif(length(side_1_y)/2, midpoint_abs - midpoint_rel * peaks_ratio, midpoint_abs)\r\n  \r\n  side_2_y <- numeric(length(side_2_x))\r\n  side_2_y[c(TRUE, FALSE)] <- runif(length(side_2_y)/2, midpoint_abs, midpoint_abs + midpoint_rel * peaks_ratio)\r\n  side_2_y[c(FALSE, TRUE)] <- runif(length(side_2_y)/2, midpoint_abs - midpoint_rel * peaks_ratio, midpoint_abs)\r\n  \r\n  if (side.first == \"left\") {\r\n    side_left <- data.frame(x = side_1_x, y = side_1_y)\r\n    side_right <- data.frame(x = side_2_x, y = rev(side_2_y))\r\n  } else if (side.first == \"right\") {\r\n    side_left <- data.frame(x = side_2_x, y = side_2_y)\r\n    side_right <- data.frame(x = side_1_x, y = rev(side_1_y))\r\n  } else {\r\n    error('Inavlid value for side.first - choose between \"left\" (default) or \"right\"')\r\n  }\r\n  \r\n  polygon_points <- rbind(\r\n    data.frame(x = c(x_start, peak_x, x_end), y = c(base, peak_y, base)),\r\n    side_left,\r\n    side_right\r\n  )\r\n  \r\n  polygon_points[order(polygon_points$x),]\r\n\r\n}\r\n\r\ntuesdata <- tidytuesdayR::tt_load(\"2020-09-22\")\r\n\r\npeaks <- tuesdata$peaks\r\nexpeditions <- tuesdata$expeditions\r\n\r\ntop_peaks <- expeditions %>% \r\n  count(peak_name) %>% \r\n  slice_max(n, n = 10)\r\n\r\nplot_df <- peaks %>% \r\n  filter(peak_name %in% top_peaks$peak_name) %>% \r\n  arrange(-height_metres) %>% \r\n  mutate(peak_name = fct_inorder(peak_name))\r\n\r\nplot_df %>% \r\n  ggplot(aes(x = first_ascent_year, y = height_metres)) +\r\n  pmap(list(plot_df$first_ascent_year, plot_df$height_metres, plot_df$peak_name),\r\n       ~ geom_polygon(aes(x, y, fill = ..3), alpha = .6,\r\n                      make_mountain(x_start = 1945, x_end = 1965, base = 5000,\r\n                                    peak_x = ..1, peak_y = ..2, n_peaks = sample(3:5, 1)))\r\n  ) +\r\n  geom_point(color = \"white\") +\r\n  ggrepel::geom_text_repel(aes(label = peak_name),\r\n                           nudge_y = 100, segment.color = 'white',\r\n                           family = \"Montserrat\", fontface = \"bold\", color = \"white\") +\r\n  guides(fill = guide_none()) +\r\n  scale_x_continuous(expand = expansion(0.01, 0)) +\r\n  scale_y_continuous(limits = c(5000, 9000), expand = expansion(0.02, 0)) +\r\n  theme_minimal(base_family = \"Montserrat\", base_size = 12) +\r\n  labs(title = \"TOP 10 Most Attempted Himalayan Peaks\",\r\n       x = \"First Ascent Year\", y = \"Peak Height (m)\") +\r\n  palettetown::scale_fill_poke(pokemon = \"articuno\") +\r\n  theme(\r\n    plot.title.position = \"plot\",\r\n    plot.title = element_text(size = 24, vjust = 3, family = \"Lora\"),\r\n    text = element_text(color = \"white\", face = \"bold\"),\r\n    axis.text = element_text(color = \"white\"),\r\n    axis.title = element_text(size = 14),\r\n    axis.title.x = element_text(vjust = -2),\r\n    axis.title.y = element_text(vjust = 4),\r\n    plot.margin = margin(1, .5, .7, .7, \"cm\"),\r\n    plot.background = element_rect(fill = \"#5C606A\", color = NA),\r\n    panel.grid = element_blank(),\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-13-designing-guiding-aesthetics/preview.png",
    "last_modified": "2021-02-14T03:40:52+09:00",
    "input_file": "designing-guiding-aesthetics.utf8.md",
    "preview_width": 8503,
    "preview_height": 6377
  },
  {
    "path": "posts/2020-09-26-demystifying-stat-layers-ggplot2/",
    "title": "Demystifying stat_ layers in {ggplot2}",
    "description": "The motivation behind stat, the distinction between stat and geom, and a case study of stat_summary()",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-09-27",
    "categories": [
      "data visualization",
      "ggplot2",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nWhen and why should I use STAT?\r\nInterim Summary #1\r\n\r\nUnderstanding STAT with stat_summary()\r\nInterim Summary #2\r\n\r\nPutting STAT to use\r\n1. Error bars showing 95% confidence interval\r\n2. A color-coded bar plot of medians\r\n3. Pointrange plot with changing size\r\n\r\nConclusion\r\nMain Ideas\r\nSTAT vs. GEOM or STAT and GEOM?\r\n\r\n\r\n\r\n\r\n\r\nUPDATE 10/5/20: This blog post was featured in the rweekly highlights podcast! Thanks to the rweekly team for a flattering review of my tutorial!\r\nIntroduction\r\n(Feel free to skip the intro section if you want to get to the point!)\r\nA powerful concept in the Grammar of Graphics is that variables are mapped onto aesthetics. In {ggplot2}, a class of objects called geom implements this idea. For example, geom_point(mapping = aes(x = mass, y = height)) would give you a plot of points (i.e. a scatter plot), where the x-axis represents the mass variable and the y axis represents the height variable.\r\nBecause geom_*()s1 are so powerful and because aesthetic mappings are easily understandable at an abstract level, you rarely have to think about what happens to the data you feed it. Take this simple histogram for example:\r\n\r\n\r\ndata(\"penguins\", package = \"palmerpenguins\")\r\n\r\nggplot(data = penguins, mapping = aes(x = body_mass_g)) +\r\n  geom_histogram()\r\n\r\n\r\n\r\n\r\nWhat’s going on here? You might say that the body_mass_g variable is represented in the x-axis. Sure, that’s not wrong. But a fuller explanation would require you to talk about these extra steps under the hood:\r\nThe variable mapped to x is divided into discrete bins\r\nA count of observations within each bin is calculated\r\nThat new variable is then represented in the y axis\r\nFinally, the provided x variable and the internally calculated y variable is represented by bars that have certain position and height\r\nI don’t mean to say here that you are a total fool if you can’t give a paragraph-long explanation of geom_histogram(). Rather, my intention here is to emphasize that the data-to-aesthetic mapping in GEOM objects is not neutral, although it can often feel very natural, intuitive, and objective (and you should thank the devs for that!). Just think about the many ways in which you can change any of the internal steps above, especially steps 12 and 23, while still having the output look like a histogram.\r\nThis important point rarely crosses our mind, in part because of what we have gotten drilled into our heads when we first started learning ggplot. As beginners we’ve likely experienced the frustration of having all the data we need to plot something, but ggplot just won’t work. You could imagine a beginner today who’s getting frustrated because geom_point(aes(x = mass, y = height)) throws an error with the following data.\r\n\r\n  # A tibble: 2 x 4\r\n    variable subject1 subject2 subject3\r\n    <chr>       <dbl>    <dbl>    <dbl>\r\n  1 mass           75       70       55\r\n  2 height        154      172      144\r\n\r\nAnd what would StackOverflow you tell this beginner? You’d probably tell them to put the data in a tidy format4 first.\r\n\r\n  # A tibble: 3 x 3\r\n    subject  mass height\r\n      <dbl> <dbl>  <dbl>\r\n  1       1    75    154\r\n  2       2    70    172\r\n  3       3    55    144\r\n\r\nNow, that’s something you can tell a beginner for a quick and easy fix. But if you still simply think “the thing that makes ggplot work = tidy data”, it’s important that you unlearn this mantra in order to fully understand the motivation behind stat.\r\nWhen and why should I use STAT?\r\nYou could be using ggplot every day and never even touch any of the two-dozen native stat_*() functions. In fact, because you’ve only used geom_*()s, you may find stat_*()s to be the esoteric and mysterious remnants of the past that only the developers continue to use to maintain law and order in the depths of source code hell.\r\nIf that describes you, you might wonder why you even need to know about all these stat_*() functions.\r\n\r\n\r\n\r\nWell, the main motivation for stat is simply this:\r\n\r\n“Even though the data is tidy it may not represent the values you want to display”5\r\n\r\nThe histogram discussion in the previous section was a good example to this point, but here I’ll introduce another example that I think will hit the point home.\r\nSuppose you have a data simple_data that looks like this:\r\n\r\n\r\nsimple_data <- tibble(group = factor(rep(c(\"A\", \"B\"), each = 15)),\r\n                      subject = 1:30,\r\n                      score = c(rnorm(15, 40, 20), rnorm(15, 60, 10)))\r\n\r\n\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"group\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"subject\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"score\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"A\",\"2\":\"1\",\"3\":\"28.79\"},{\"1\":\"A\",\"2\":\"2\",\"3\":\"35.40\"},{\"1\":\"A\",\"2\":\"3\",\"3\":\"71.17\"},{\"1\":\"A\",\"2\":\"4\",\"3\":\"41.41\"},{\"1\":\"A\",\"2\":\"5\",\"3\":\"42.59\"},{\"1\":\"A\",\"2\":\"6\",\"3\":\"74.30\"},{\"1\":\"A\",\"2\":\"7\",\"3\":\"49.22\"},{\"1\":\"A\",\"2\":\"8\",\"3\":\"14.70\"},{\"1\":\"A\",\"2\":\"9\",\"3\":\"26.26\"},{\"1\":\"A\",\"2\":\"10\",\"3\":\"31.09\"},{\"1\":\"A\",\"2\":\"11\",\"3\":\"64.48\"},{\"1\":\"A\",\"2\":\"12\",\"3\":\"47.20\"},{\"1\":\"A\",\"2\":\"13\",\"3\":\"48.02\"},{\"1\":\"A\",\"2\":\"14\",\"3\":\"42.21\"},{\"1\":\"A\",\"2\":\"15\",\"3\":\"28.88\"},{\"1\":\"B\",\"2\":\"16\",\"3\":\"77.87\"},{\"1\":\"B\",\"2\":\"17\",\"3\":\"64.98\"},{\"1\":\"B\",\"2\":\"18\",\"3\":\"40.33\"},{\"1\":\"B\",\"2\":\"19\",\"3\":\"67.01\"},{\"1\":\"B\",\"2\":\"20\",\"3\":\"55.27\"},{\"1\":\"B\",\"2\":\"21\",\"3\":\"49.32\"},{\"1\":\"B\",\"2\":\"22\",\"3\":\"57.82\"},{\"1\":\"B\",\"2\":\"23\",\"3\":\"49.74\"},{\"1\":\"B\",\"2\":\"24\",\"3\":\"52.71\"},{\"1\":\"B\",\"2\":\"25\",\"3\":\"53.75\"},{\"1\":\"B\",\"2\":\"26\",\"3\":\"43.13\"},{\"1\":\"B\",\"2\":\"27\",\"3\":\"68.38\"},{\"1\":\"B\",\"2\":\"28\",\"3\":\"61.53\"},{\"1\":\"B\",\"2\":\"29\",\"3\":\"48.62\"},{\"1\":\"B\",\"2\":\"30\",\"3\":\"72.54\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nAnd suppose that you want to draw a bar plot where each bar represents group and the height of the bars corresponds to the mean of score for each group.\r\nIf you’re stuck in the mindset of “the data that I feed in to ggplot() is exactly what gets mapped, so I need to tidy it first and make sure it contains all the aesthetics that each geom needs”, you would need to transform the data before piping it in like this:\r\n\r\n\r\nsimple_data %>%\r\n  group_by(group) %>% \r\n  summarize(\r\n    mean_score = mean(score),\r\n    .groups = 'drop' # Remember to ungroup!\r\n  ) %>% \r\n  ggplot(aes(x = group, y = mean_score)) +\r\n  geom_col()\r\n\r\n\r\n\r\n\r\nIt’s a good practice to always ungroup your dataframe before passing into ggplot, as having grouped data can lead to unintended behaviors that are hard to debug.\r\n\r\n\r\n\r\nWhere the data passed in looks like this:\r\n\r\n  # A tibble: 2 x 2\r\n    group mean_score\r\n    <fct>      <dbl>\r\n  1 A           43.0\r\n  2 B           57.5\r\n\r\nOk, not really a problem there. But what if we want to add in error bars too? Error bars also plot a summary statistic (the standard error), so we’d need make another summary of the data to pipe into ggplot().\r\nLet’s first plot the error bar by itself, we’re again passing in a transformed data\r\n\r\n\r\nsimple_data %>% \r\n  group_by(group) %>% \r\n  summarize(\r\n    mean_score = mean(score),\r\n    se = sqrt(var(score)/length(score)),\r\n    .groups = 'drop'\r\n  ) %>% \r\n  mutate(\r\n    lower = mean_score - se,\r\n    upper = mean_score + se\r\n  ) %>% \r\n  ggplot(aes(x = group, y = mean_score, ymin = lower, ymax = upper)) +\r\n  geom_errorbar()\r\n\r\n\r\n\r\n\r\nWhere the transformed data looks like this:\r\n\r\n  # A tibble: 2 x 5\r\n    group mean_score    se lower upper\r\n    <fct>      <dbl> <dbl> <dbl> <dbl>\r\n  1 A           43.0  4.37  38.7  47.4\r\n  2 B           57.5  2.82  54.7  60.4\r\n\r\nOk, now let’s try combining the two. One way to do this is to save the data paseed in for the bar plot and the data passed in for the errorbar plot as two separate variables, and then call each in their respective geoms:\r\n\r\n\r\nsimple_data_bar <- simple_data %>%\r\n  group_by(group) %>% \r\n  summarize(\r\n    mean_score = mean(score),\r\n    .groups = 'drop'\r\n  )\r\n  \r\nsimple_data_errorbar <- simple_data %>% \r\n  group_by(group) %>% \r\n  summarize(\r\n    mean_score = mean(score),\r\n    se = sqrt(var(score)/length(score)),\r\n    .groups = 'drop'\r\n  ) %>% \r\n  mutate(\r\n    lower = mean_score - se,\r\n    upper = mean_score + se\r\n  )\r\n\r\nggplot() +\r\n  geom_col(\r\n    aes(x = group, y = mean_score),\r\n    data = simple_data_bar\r\n  ) +\r\n  geom_errorbar(\r\n    aes(x = group, y = mean_score, ymin = lower, ymax = upper),\r\n    data = simple_data_errorbar\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nYeah… that code is a mouthful. The above approach is not parsimonious because we keep repeating similar processes in different places.6 If you, like myself, don’t like how this looks, then let this be a lesson that this is the consequence of thinking that you must always prepare a tidy data containing values that can be DIRECTLY mapped to geometric objects.\r\nAnd on a more theoretical note, simple_data_bar and simple_data_errorbar aren’t even really “tidy” in the original sense of the term. We need to remind ourselves here that tidy data is about the organization of observations in the data. Under this definition, values like bar height and the top and bottom of whiskers are hardly observations themselves. Rather, they’re abstractions or summaries of the actual observations in our data simple_data which, if you notice, we didn’t even use to make our final plot above!\r\n\r\n\r\n\r\nFigure 1: Tidy data is about the organization of observations.\r\n\r\n\r\n\r\nSo not only is it inefficient to create a transformed dataframe that suits the needs of each geom, this method isn’t even championing the principles of tidy data like we thought.7\r\nWhat we should do instead is to take advantage of the fact that our original data simple_data is the common denominator of simple_data_bar and simple_data_errorbar!\r\nWouldn’t it be nice if you could just pass in the original data containing all observations (simple_data) and have each layer internally transform the data in appropriate ways to suit the needs of the geom for that layer?\r\nOh, so you mean something like this?\r\n\r\n\r\nsimple_data %>% \r\n  ggplot(aes(group, score)) +\r\n  stat_summary(geom = \"bar\") +\r\n  stat_summary(geom = \"errorbar\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nInterim Summary #1\r\nIn this section, I built up a tedious walkthrough of making a barplot with error bars using only geom_*()s just to show that two lines of stat_summary() with a single argument can achieve the same without even touching the data through any form of pre-processing.\r\nSo that was a taste of how powerful stat_*()s can be, but how do they work and how can you use them in practice?\r\nUnderstanding STAT with stat_summary()\r\nLet’s analyze stat_summary() as a case study to understand how stat_*()s work more generally. I think that stat_summary() is a good choice because it’s a more primitive version of many other stat_*()s and is likely to be the one that you’d end up using the most for visualizations in data science.\r\nBefore we start, let’s create a toy data to work with. Let’s call this data height_df because it contains data about a group and the height of individuals in that group.\r\n\r\n\r\nheight_df <- tibble(group = \"A\",\r\n                    height = rnorm(30, 170, 10))\r\n\r\n\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"group\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"height\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"A\",\"2\":\"174.3\"},{\"1\":\"A\",\"2\":\"167.0\"},{\"1\":\"A\",\"2\":\"179.0\"},{\"1\":\"A\",\"2\":\"178.8\"},{\"1\":\"A\",\"2\":\"178.2\"},{\"1\":\"A\",\"2\":\"176.9\"},{\"1\":\"A\",\"2\":\"175.5\"},{\"1\":\"A\",\"2\":\"169.4\"},{\"1\":\"A\",\"2\":\"166.9\"},{\"1\":\"A\",\"2\":\"166.2\"},{\"1\":\"A\",\"2\":\"163.1\"},{\"1\":\"A\",\"2\":\"167.9\"},{\"1\":\"A\",\"2\":\"157.3\"},{\"1\":\"A\",\"2\":\"191.7\"},{\"1\":\"A\",\"2\":\"182.1\"},{\"1\":\"A\",\"2\":\"158.8\"},{\"1\":\"A\",\"2\":\"166.0\"},{\"1\":\"A\",\"2\":\"165.3\"},{\"1\":\"A\",\"2\":\"177.8\"},{\"1\":\"A\",\"2\":\"169.2\"},{\"1\":\"A\",\"2\":\"172.5\"},{\"1\":\"A\",\"2\":\"169.7\"},{\"1\":\"A\",\"2\":\"169.6\"},{\"1\":\"A\",\"2\":\"183.7\"},{\"1\":\"A\",\"2\":\"167.7\"},{\"1\":\"A\",\"2\":\"185.2\"},{\"1\":\"A\",\"2\":\"154.5\"},{\"1\":\"A\",\"2\":\"175.8\"},{\"1\":\"A\",\"2\":\"171.2\"},{\"1\":\"A\",\"2\":\"172.2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nWe can visualize the data with a familiar geom, say geom_point():\r\n\r\n\r\nheight_df %>% \r\n  ggplot(aes(x = group, y = height)) +\r\n  geom_point()\r\n\r\n\r\n\r\n\r\nAs a first step in our investigation, let’s just replace our familiar geom_point() with the scary-looking stat_summary() and see what happens:\r\n\r\n\r\nheight_df %>% \r\n  ggplot(aes(x = group, y = height)) +\r\n  stat_summary()\r\n\r\n\r\n\r\n\r\nInstead of points, we now see a point and a line through that point. And before you get confused, this is actually one geom, called pointrange, not two separate geoms.8 Now that that’s cleared up, we might ask: what data is being represented by the pointrange?\r\nAnswering this question requires us to zoom out a little bit and ask: what variables does pointrange map as a geom? By looking at the documentation with ?geom_pointrange we can see that geom_pointrange() requires the following aesthetics:\r\nx or y\r\nymin or xmin\r\nymax or xmax\r\nSo now let’s look back at our arguments in aes(). We said that group is mapped to x and that height is mapped to y. But we never said anything about ymin/xmin or ymax/xmax anywhere. So how is stat_summary() drawing a pointrange if we didn’t give it the required aesthetic mappings?\r\nWell, a good guess is that stat_summary() is transforming the data to calculate the necessary values to be mapped to pointrange. Here’s one reason for that guess - I’ve been suppressing message throughout this post but if you run the above code with stat_summary() yourself, you’d actually get this message:\r\n\r\n  No summary function supplied, defaulting to `mean_se()`\r\n\r\nHuh, a summary function? That sounds promising. Maybe that’s the key to our mystery!\r\nFirst, we see from the documentation of stat_summary() that this mean_se() thing is the default value for the fun.data argument (we’ll talk more on this later).\r\nNext, let’s call it in the console to see what it is:\r\n\r\n\r\nmean_se\r\n\r\n\r\n  function (x, mult = 1) \r\n  {\r\n      x <- stats::na.omit(x)\r\n      se <- mult * sqrt(stats::var(x)/length(x))\r\n      mean <- mean(x)\r\n      new_data_frame(list(y = mean, ymin = mean - se, ymax = mean + \r\n          se), n = 1)\r\n  }\r\n  <bytecode: 0x0000000019292b88>\r\n  <environment: namespace:ggplot2>\r\n\r\nOk, so it’s a function that takes some argument x and a second argument mult with the default value 1.\r\nLet’s go over what it does by breaking down the function body line by line:\r\nRemove NA values\r\nCalculate variable se which is the standard error of the values in x using the equation \\(SE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_i-\\bar{x})^2}\\)\r\nCalculate the variable mean9 which is the mean of x\r\nCreate a new dataframe with one row, with columns y, ymin, and ymax, where y is the mean of x, ymin is one standard error below the mean, and ymax is one standard error above the mean.10\r\nA cool thing about this is that although mean_se() seems to be exclusively used for internal operations, it’s actually available in the global environment from loading {ggplot2}. So let’s pass height_df to mean_se() and see what we get back!\r\n\r\n\r\nmean_se(height_df)\r\n\r\n\r\n  Error: Elements must equal the number of rows or 1\r\n\r\n\r\n\r\n\r\nUhhh what?\r\nDo you see what happened just now? This is actually really important: stat_summary() summarizes one dimension of the data.11 mean_se() threw an error when we passed it our whole data because it was expecting just a vector of the variable to be summarized.\r\n\r\nWhenever you’re trying out a new stat_*() function, make sure to check what variables/object types the statistical transformation is being applied to!\r\nOk now that we’ve went over that little mishap, let’s give mean_se() the vector it wants.\r\n\r\n\r\nmean_se(height_df$height)\r\n\r\n\r\n        y  ymin  ymax\r\n  1 171.8 170.3 173.3\r\n\r\nAnd look at that, these look like they’re the same values that were being represented by the mid-point and the end-points of the pointrange plot that we drew with stat_summary() above!\r\nYou know how else we can check that this is the case? With this neat function called layer_data().\r\nWe can pull the data that was used to draw the pointrange by passing our plot object to layer_data() and setting the second argument to 112:\r\n\r\n\r\npointrange_plot <- height_df %>% \r\n  ggplot(aes(x = group, y = height)) +\r\n  stat_summary()\r\n\r\nlayer_data(pointrange_plot, 1)\r\n\r\n\r\n    x group     y  ymin  ymax PANEL flipped_aes colour size linetype shape fill\r\n  1 1     1 171.8 170.3 173.3     1       FALSE  black  0.5        1    19   NA\r\n    alpha stroke\r\n  1    NA      1\r\n\r\nWould ya look at that! There’s a lot of stuff in there, but it looks like the values for y, ymin, and ymax used for the actual plot match up with the values we calculated with mean_se() above!\r\nWe’ve solved our mystery of how the pointrange was drawn when we didn’t provide all the required mappings!\r\n\r\n\r\n\r\nInterim Summary #2\r\nTo summarize this section (ha!), stat_summary() works in the following order:\r\nThe data that is passed into ggplot() is inherited if one is not provided\r\nThe function passed into the fun.data argument applies transformations to (a part of) that data (defaults to mean_se())\r\nThe result is passed into the geom provided in the geom argument (defaults to pointrange).\r\nIf the data contains all the required mapppings for the geom, the geom will be plotted.\r\nAnd to make things extra clear & to make stat_summary() less mysterious, we can explicitly spell out the two arguments fun.data and geom that we went over in this section.\r\n\r\n\r\nheight_df %>% \r\n  ggplot(aes(x = group, y = height)) +\r\n  stat_summary(\r\n    geom = \"pointrange\",\r\n    fun.data = mean_se\r\n  )\r\n\r\n\r\n\r\n\r\nYou could also do fun.data = \"mean_se\" but I prefer the unquoted version because it make it extra clear that mean_se is a function, not a special parameter. It also keeps things consistent because if you want to pass in a custom function, they cannot be quoted.\r\n\r\n\r\n\r\nLook, it’s the same plot!\r\nPutting STAT to use\r\nNow we have arrived at the fun part.\r\nHere, I will demonstrate a few ways of modifying stat_summary() to suit particular visualization needs.\r\nFor this section, I will use a modified version of the penguins data that I loaded all the way up in the intro section (I’m just removing NA values here, nothing fancy).\r\n\r\n\r\nmy_penguins <- na.omit(penguins)\r\n\r\n\r\n\r\nAt no point in this section will I be modifying the data being piped into ggplot(). That is the beauty and power of stat.\r\n1. Error bars showing 95% confidence interval\r\nHere, we’re plotting the mean body_mass_g of penguins for each sex, with error bars that show the 95% confidence interval (a range of approx 1.96 standard errors from the mean).\r\n\r\n\r\nmy_penguins %>% \r\n  ggplot(aes(sex, body_mass_g)) +\r\n  stat_summary(\r\n    fun.data = ~mean_se(., mult = 1.96), # Increase `mult` value for bigger interval!\r\n    geom = \"errorbar\",\r\n  )\r\n\r\n\r\n\r\n\r\nAs of {ggplot2} 3.3.0, you can use {rlang}-style anonymous functions. If you aren’t familiar, ~mean_se(., mult = 1.96) is the same as function(x) {mean_se(x, mult = 1.96)}\r\n\r\n\r\n\r\nThe transformed data used for the errorbar geom inside stat_summary():\r\n\r\n\r\nbind_rows(\r\n  mean_se(my_penguins$body_mass_g[my_penguins$sex == \"female\"], mult = 1.96),\r\n  mean_se(my_penguins$body_mass_g[my_penguins$sex == \"male\"], mult = 1.96),\r\n)\r\n\r\n\r\n       y ymin ymax\r\n  1 3862 3761 3964\r\n  2 4546 4427 4665\r\n\r\n2. A color-coded bar plot of medians\r\nHere, we’re plotting the median bill_length_mm for each penguins species and coloring the groups with median bill_length_mm under 40 in pink.\r\n\r\n\r\ncalc_median_and_color <- function(x, threshold = 40) {\r\n  tibble(y = median(x)) %>% \r\n    mutate(fill = ifelse(y < threshold, \"pink\", \"grey35\"))\r\n}\r\n\r\nmy_penguins %>% \r\n  ggplot(aes(species, bill_length_mm)) +\r\n  stat_summary(\r\n    fun.data = calc_median_and_color,\r\n    geom = \"bar\"\r\n  )\r\n\r\n\r\n\r\n\r\nCalculating summaries by group is automatically handled internally when you provide grouping variables (here, the species variable that’s mapped to x), so you don’t have to worry about that in your custom function.\r\n\r\n\r\n\r\nThe transformed data used for the bar geom inside stat_summary():\r\n\r\n\r\ngroup_split(my_penguins, species) %>%\r\n  map(~ pull(., bill_length_mm)) %>% \r\n  map_dfr(calc_median_and_color)\r\n\r\n\r\n\r\n\r\nThis is a more systematic way of mimicking the internal process of stat_summary(). Run each line incrementally see to what they do!\r\n\r\n  # A tibble: 3 x 2\r\n        y fill  \r\n    <dbl> <chr> \r\n  1  38.8 pink  \r\n  2  49.6 grey35\r\n  3  47.4 grey35\r\n\r\nNote how you can calculate non-required aesthetics in your custom functions (e.g., fill) and they also be used to make the geom!\r\n3. Pointrange plot with changing size\r\nHere, we’re plotting bill_depth_mm of penguins inhabiting different islands, with the size of each pointrange changing with the number of observations\r\n\r\n\r\nmy_penguins %>% \r\n  ggplot(aes(species, bill_depth_mm)) +\r\n  stat_summary(\r\n    fun.data = function(x) {\r\n      \r\n      scaled_size <- length(x)/nrow(my_penguins)\r\n      \r\n      mean_se(x) %>% \r\n        mutate(size = scaled_size)\r\n    }\r\n  )\r\n\r\n\r\n\r\n\r\nIf you don’t want to declare a new function in the environment just for one plot, you can just pass in an anonymous function to the fun.data argument. And of course, if it’s long, you should wrap it in function(x){}.\r\n\r\n\r\n\r\n\r\nLooking back, this is actually a cool plot because you can see how lower number of samples (smaller size) contributes to increased uncertainty (longer range) in the pointrange.\r\nThe transformed data used for the pointrange geom inside stat_summary():\r\n\r\n\r\ngroup_split(my_penguins, species) %>%\r\n  map(~ pull(., bill_depth_mm)) %>% \r\n  map_dfr(\r\n    function(x) {\r\n      \r\n      scaled_size <- length(x)/nrow(my_penguins)\r\n      \r\n      mean_se(x) %>% \r\n        mutate(size = scaled_size)\r\n    }\r\n  )\r\n\r\n\r\n        y  ymin  ymax   size\r\n  1 18.35 18.25 18.45 0.4384\r\n  2 18.42 18.28 18.56 0.2042\r\n  3 15.00 14.91 15.09 0.3574\r\n\r\nConclusion\r\nMain Ideas\r\nEven though the data is tidy, it may not represent the values you want to display\r\nThe solution is not to transform your already-tidy data so that it contains those values\r\nInstead, you should pass in your original tidy data into ggplot() as is and allow stat_*() functions to apply transformations internally\r\nThese stat_*() functions can be customized for both their geoms and their transformation functions, and works similarly to geom_*() functions in other regards\r\nIf you want to use your own custom function, make sure to check the documentation of that particular stat_*() function to check the variable/data type it requires.\r\nIf you want to use a different geom, make sure that your transformation function calculates all the required aesthetics for that geom\r\nSTAT vs. GEOM or STAT and GEOM?\r\nAlthough I have talked about the limitations of geom_*()s to demonstrate the usefulness of stat_*()s, both have their place. It’s about knowing when to use which; it’s not a question of either-or. In fact, they require each other - just like how stat_summary() had a geom argument, geom_*()s also have a stat argument. At a higher level, stat_*()s and geom_*()s are simply convenient instantiations of the layer() function that builds up the layers of ggplot.\r\nBecause this is important, I’ll wrap up this post with a quote from Hadley explaining this false dichotomy:\r\n\r\nUnfortunately, due to an early design mistake I called these either stat_() or geom_(). A better decision would have been to call them layer_() functions: that’s a more accurate description because every layer involves a stat and a geom.13\r\n\r\n\r\nJust to clarify on notation, I’m using the star symbol * here to say that I’m referencing all the functions that start with geom_ like geom_bar() and geom_point(). This is called the Kleene star and it’s used a lot in regex, if you aren’t familiar.↩︎\r\nYou could have bins of that are not of equal size. Or, you could have bins that bleed into each other to create a rolling window summary.↩︎\r\nYou could calculate the sum of raw values that are in each bin, or calculate proportions instead of counts↩︎\r\nIf you aren’t familiar already, “tidy” is a specific term of art↩︎\r\nThis quote is adapted from Thomas Lin Pedersen’s ggplot2 workshop video↩︎\r\nYes, you can still cut down on the code somewhat, but will it even get as succinct as what I show below with stat_summary()? (9/30 edit) Okay, I was kinda strawmaning, and Hadley(!) has correctly caught me on that. The bar-errorbar plot was not the best choice to demonstrate the benefits of stat_summary(), but I just wanted to get people excited about stat_*()! Sorry for the confusion/irritation!!↩︎\r\nThere’s actually one more argument against transforming data before piping it into ggplot. When you choose the variables to plot, say cyl and mpg in the mtcars dataset, do you call select(cyl, mpg) before piping mtcars into ggplot? No? Well then why would you transform your data beforehand if you can just have that be handled internally instead? It’s the same logic!↩︎\r\nIf you’re still skeptical, save the plot object to a variable like plot and call plot$layers to confirm that geom_pointrange was used to draw the plot.↩︎\r\nI personally don’t agree with this naming choice since mean is also the name of the base function↩︎\r\nThe function new_data_frame() is from {vctrs}. That last line of code in the function body is doing the same thing as data.frame(y = mean, ymin = mean - se, ymax = mean + se), but there’s less room for error the way it’s done in the source code.↩︎\r\nIf you read the documentation, the very first line starts with “stat_summary() operates on unique x or y …” (emphasis mine)↩︎\r\nThis second argument specifies which layer to return. Here, the pointrange layer is the first and only layer in the plot so I actually could have left this argument out.↩︎\r\nEmphasis mine. Source: https://cran.r-project.org/web/packages/ggplot2/vignettes/extending-ggplot2.html↩︎\r\n",
    "preview": "posts/2020-09-26-demystifying-stat-layers-ggplot2/preview.png",
    "last_modified": "2020-11-03T00:39:46+09:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 278
  },
  {
    "path": "posts/2020-09-23-tidytuesday-2020-week-39/",
    "title": "TidyTuesday 2020 week 39",
    "description": "Stacked area plot of the heights of Himalayan peaks attempted over the last century",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-09-23",
    "categories": [
      "ggplot2",
      "data visualization",
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nVisualization\r\nThings I learned\r\nThings to improve\r\n\r\nCode\r\n\r\n\r\n\r\n\r\nVisualization\r\n\r\n\r\n\r\nThings I learned\r\nHaving a nice background color for the plot (and generally just working with color)\r\nMargin options of various kinds in theme()\r\nUsing {scales}, pretty_breaks() in particular\r\nUsing {ragg} to draw and save high quality plots\r\nThings to improve\r\nThe subtitle is kinda boring (and the entire plot is a bit underwhelming)\r\nFigure out how to increase spacing between y-axis text and the plot (hjust is relative to each label, so doesn’t work)\r\nCode\r\nAlso available on github\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n# DATA\r\n\r\ntuesdata <- tidytuesdayR::tt_load(\"2020-09-22\")\r\n\r\nclimb_data <- tuesdata$expeditions %>% \r\n  left_join(tuesdata$peaks, by = \"peak_name\") %>% \r\n  select(peak = peak_name, year, height = height_metres) %>% \r\n  arrange(-height) %>% \r\n  mutate(height_group = fct_inorder(case_when(peak == \"Everest\" ~ \"Mt. Everest (8850m)\",\r\n                                              between(height, 8000, 8849) ~ \"> 8000m\",\r\n                                              between(height, 7000, 7999) ~ \"7999m ~ 7000m\",\r\n                                              between(height, 6000, 6999) ~ \"6999m ~ 6000m\",\r\n                                              TRUE ~ \"< 6000m\"))\r\n  ) %>% \r\n  count(five_years = round(year/5) * 5, height_group) %>% \r\n  filter(five_years >= 1920) %>% \r\n  complete(five_years, height_group, fill = list(n = 0)) %>% \r\n  group_by(five_years) %>% \r\n  mutate(prop = n / sum(n)) %>% \r\n  ungroup()\r\n\r\n\r\n# PLOT\r\n\r\nmountain_palette <- c(\"#6E86A6\", \"#95A2B3\", \"#5C606A\", \"#44464E\", \"#3D3737\")\r\n\r\nclimb_plot <- climb_data %>% \r\n  ggplot(aes(five_years, prop)) +\r\n  geom_area(aes(fill = height_group, color = height_group))  +\r\n  scale_fill_manual(values = mountain_palette) +\r\n  scale_color_manual(values = mountain_palette) +\r\n  coord_cartesian(xlim = c(1920, 2020), expand = FALSE) +\r\n  scale_x_continuous(breaks = scales::pretty_breaks(11)) +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  labs(\r\n    title = \"Himalayan Peaks Attempted Over Time\",\r\n    subtitle = \"Over 1/4th of all expeditions were to Mount Everest\",\r\n    x = NULL, y = NULL, fill = NULL, color = NULL,\r\n    caption = \"By: @yjunechoe | Source: The Himalayan Database\"\r\n  ) +\r\n  theme_classic(base_family = \"Futura Hv BT\", base_size = 16) +\r\n  theme(\r\n    plot.title.position = \"plot\",\r\n    plot.title = element_text(size = 28, color = \"white\", family = \"Lora\", face = \"bold\"),\r\n    plot.subtitle = element_text(size = 14, color = \"white\", face = \"italic\"),\r\n    plot.margin = margin(2, 2.5, 2, 2, 'cm'),\r\n    plot.caption = element_text(color = \"white\", family = \"Roboto Mono\", hjust = 1.15, vjust = -13),\r\n    legend.position = \"top\",\r\n    legend.direction = \"horizontal\",\r\n    legend.text = element_text(color = \"white\"),\r\n    legend.background = element_rect(fill = NA),\r\n    axis.text = element_text(color = \"white\"),\r\n    axis.text.y = element_text(vjust = -.1),\r\n    axis.text.x = element_text(vjust = -2),\r\n    axis.ticks = element_blank(),\r\n    axis.line = element_blank(),\r\n    panel.background = element_blank(),\r\n    plot.background = element_rect(fill = \"#606F84\", color = NA)\r\n  )\r\n\r\n\r\n# SAVE\r\n\r\npngfile <- fs::path(getwd(), \"plot.png\")\r\nragg::agg_png(\r\n  pngfile,\r\n  width = 60,\r\n  height = 36,\r\n  units = \"cm\",\r\n  res = 300,\r\n  scaling = 2\r\n)\r\nplot(climb_plot); invisible(dev.off())\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-09-23-tidytuesday-2020-week-39/preview.png",
    "last_modified": "2020-11-02T09:03:42+09:00",
    "input_file": {},
    "preview_width": 7086,
    "preview_height": 4251
  },
  {
    "path": "posts/2020-09-20-plot-makeover-1/",
    "title": "Plot Makeover #1",
    "description": "Flattening a faceted grid for strictly horizontal comparisons",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-09-20",
    "categories": [
      "plot makeover",
      "data visualization",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nBefore\r\nMy Plan\r\nAfter\r\nPoint-line plot\r\nBar plot\r\n\r\n\r\n\r\n\r\nknitr::opts_chunk$set(\r\n  comment = \" \",\r\n  echo = TRUE,\r\n  message = TRUE,\r\n  warning = TRUE,\r\n  R.options = list(width = 80)\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis is the first installment of plot makeover where I take a plot in the wild and make very opinionated modifications to it.\r\nBefore\r\nOur plot-in-the-wild comes from the recent AMLAP 2020 conference, where I presented my thesis research and had the opportunity to talk with and listen to expert psycholinguists around the world. The plot that I’ll be looking at here is Figure 3 from the abstract of a work by E. Matthew Husband and Nikole Patson (Husband and Patson 2020).\r\n\r\n\r\n\r\nFigure 1: Plot from Husband and Patson (2020)\r\n\r\n\r\n\r\nWhat we have is 6 pairs of barplots with error bars, laid out in a 2-by-3 grid. The total of 12 bars are grouped at three levels which are mapped in the following way:\r\nFirst level is mapped to the grid column.\r\nSecond level is mapped to the grid row.\r\nThird level is mapped to the x-axis.\r\nTo get a better sense of what they did, and to make data for the plot makeover, I have recreated the original plot below:1\r\n1. Data\r\n\r\n\r\nlibrary(tidyverse)\r\ndf <- crossing(level_1 = fct_inorder(c(\"Within\", \"Between\")),\r\n               level_2 = fct_inorder(c(\"Some\", \"Number\", \"Or\")),\r\n               level_3 = factor(c(\"Strong\", \"Weak\")))\r\ndf$barheight <- c(.63, .35, .72, .55, .61, .15, .60, .55, .52, .63, .17, .16)\r\n\r\ndf\r\n\r\n\r\n\r\n\r\nThe numbers for barheight were eyeballed from looking at the original plot, of course.\r\n\r\n  # A tibble: 12 x 4\r\n     level_1 level_2 level_3 barheight\r\n     <fct>   <fct>   <fct>       <dbl>\r\n   1 Within  Some    Strong       0.63\r\n   2 Within  Some    Weak         0.35\r\n   3 Within  Number  Strong       0.72\r\n   4 Within  Number  Weak         0.55\r\n   5 Within  Or      Strong       0.61\r\n   6 Within  Or      Weak         0.15\r\n   7 Between Some    Strong       0.6 \r\n   8 Between Some    Weak         0.55\r\n   9 Between Number  Strong       0.52\r\n  10 Between Number  Weak         0.63\r\n  11 Between Or      Strong       0.17\r\n  12 Between Or      Weak         0.16\r\n\r\n2. Plot\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(level_3, barheight)) +\r\n  geom_col(\r\n    aes(fill = level_3),\r\n    show.legend = FALSE\r\n  ) +\r\n  geom_errorbar(\r\n    aes(ymin = barheight - .05, ymax = barheight + .05),\r\n    width = .1) +\r\n  facet_grid(level_2 ~ level_1) +\r\n  theme_bw() +\r\n  scale_fill_manual(values = c('grey40', 'grey80')) +\r\n  ylim(0, 1) +\r\n  labs(\r\n    y = \"Proportion of Strong Responses\",\r\n    x = \"Prime Type\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\n\r\nThe original plot for comparison:\r\n\r\n\r\n\r\nMy Plan\r\nMajor Changes:\r\nFlatten the grid in some way so that everything is laid out left-to-right and you can make comparisons horizontally.\r\nCap the y axis to make it clear that the values (proportions) can only lie between 0 and 1.\r\nMinor Changes:\r\nRemove grid lines\r\nIncrease space between axis and axis titles.\r\nRemove boxes around strip labels\r\nMake strip (facet) labels larger and more readable.\r\nIncrease letter spacing (probably by changing font)\r\nAfter\r\nI actually couldn’t settle on one final product2 so here are two plots that incorporate the changes that I wanted to make. I think that both look nice and you may prefer one style over the other depending on what relationships/comparisons you want your graph to emphasize.\r\nPoint-line plot\r\nEdit 11/4/20: Mark has rightly pointed out to me that the groups could additionally be mapped to shape for greater clarity, so I’ve incorporated that change.3\r\n\r\n\r\n\r\n\r\n\r\ndodge <- position_dodge(width = .5)\r\n\r\ndf %>% \r\n  mutate(level_3 = as.numeric(level_3)) %>% \r\n  ggplot(aes(x = level_3, y = barheight, group = level_1)) +\r\n  geom_errorbar(\r\n    aes(ymin = barheight - .05, ymax = barheight + .05),\r\n    width = .2,\r\n    position = dodge\r\n  ) +\r\n  geom_line(\r\n    aes(linetype = level_1),\r\n    position = dodge,\r\n    show.legend = FALSE\r\n  ) +\r\n  geom_point(\r\n    aes(shape = level_1, fill = level_1),\r\n    size = 1.5,\r\n    stroke = .6,\r\n    position = dodge\r\n  ) + \r\n  scale_fill_manual(values = c(\"black\", \"white\")) +\r\n  scale_shape_manual(values = c(21, 24)) +\r\n  facet_wrap(~ level_2) +\r\n  scale_x_continuous(\r\n    breaks = 1:2,\r\n    labels = levels(df$level_3),\r\n    expand = expansion(.2),\r\n  ) +\r\n  scale_y_continuous(\r\n    limits = c(0, 1),\r\n    expand = expansion(c(0, .1))\r\n  ) +\r\n  lemon::coord_capped_cart(left = \"both\") +\r\n  guides(\r\n    fill = guide_none(),\r\n    shape = guide_legend(\r\n      title = NULL,\r\n      direction = \"horizontal\",\r\n      label.theme = element_text(size = 10, family = \"Montserrat\"),\r\n      override.aes = list(fill = c(\"black\", \"white\"))\r\n    )\r\n  ) +\r\n  labs(\r\n    y = \"Strong Responses\",\r\n    x = \"Prime Type\",\r\n    linetype = \"Category\"\r\n  ) +\r\n  ggthemes::theme_clean(base_size = 14) +\r\n  theme(\r\n    text = element_text(family = \"Montserrat\"),\r\n    legend.position = c(.18, .87),\r\n    legend.background = element_rect(color = NA, fill = NA),\r\n    strip.text = element_text(size = 13),\r\n    plot.margin = margin(5, 5, 5, 5, 'mm'),\r\n    axis.title.x = element_text(vjust = -3),\r\n    axis.title.y = element_text(vjust = 5),\r\n    plot.background = element_blank(),\r\n    panel.grid.major.y = element_blank()\r\n  )\r\n\r\n\r\n\r\nBar plot\r\n\r\n\r\n\r\n\r\n\r\ndodge <- position_dodge(width = .5)\r\n\r\ndf %>% \r\n  mutate(level_3 = as.numeric(level_3)) %>% \r\n  ggplot(aes(x = level_3, y = barheight, group = level_1)) +\r\n  geom_col(position = dodge, width = .5, color = 'white', aes(fill = level_1)) +\r\n  scale_fill_manual(values = c(\"grey30\", \"grey60\")) +\r\n  geom_errorbar(\r\n    aes(ymin = barheight - .05, ymax = barheight + .05),\r\n    width = .2,\r\n    position = dodge\r\n  ) +\r\n  facet_wrap(~ level_2) +\r\n  scale_x_continuous(\r\n    breaks = 1:2,\r\n    labels = levels(df$level_3),\r\n    expand = expansion(.2),\r\n  ) +\r\n  ylim(0, 1) +\r\n  lemon::coord_capped_cart(left = \"both\") +\r\n  labs(\r\n    y = \"Strong Responses\",\r\n    x = \"Prime Type\",\r\n    fill = NULL\r\n  ) +\r\n  ggthemes::theme_clean(base_size=14) +\r\n  theme(\r\n    text = element_text(family = \"Montserrat\"),\r\n    legend.text = element_text(size = 10),\r\n    legend.key.size = unit(5, 'mm'),\r\n    legend.direction = \"horizontal\",\r\n    legend.position = c(.17, .85),\r\n    legend.background = element_blank(),\r\n    strip.text = element_text(size = 14),\r\n    axis.ticks.x = element_blank(),\r\n    axis.title.x = element_text(vjust = -3),\r\n    axis.title.y = element_text(vjust = 5),\r\n    panel.grid.major.y = element_blank(),\r\n    plot.background = element_blank(),\r\n    plot.margin = margin(5, 5, 5, 5, 'mm')\r\n  )\r\n\r\n\r\n\r\n\r\nIn this second version, I removed guides() and distributed its arguments across labs() and theme(). I kinda like this layout of having a fat theme(). It’s also not too hard to read if you group and sort the arguments.\r\n\r\n\r\n\r\nHusband, E. Matthew, and Nikole Patson. 2020. Priming of Implicatures Within and Between Categories: The Case of or. AMLaP2020. https://amlap2020.github.io/a/272.pdf.\r\n\r\n\r\nBut note that this is likely not how the original plot was generated: the authors were likely feeding ggplot2 with the raw data (involving 1s and 0s in this case), but here I am just grabbing the summary statistic that was mapped to the bar aesthetic (hence my decision to name the y variable barheight).↩︎\r\nI ran the first plot by a friend who has a degree in design, and she recommended several changes that eventually ended up being the second plot. Some major pointers were removing border lines from the legend, removing x-axis tick marks, and applying color/shade.↩︎\r\nThe plot used to look like this: ↩︎\r\n",
    "preview": "posts/2020-09-20-plot-makeover-1/plot-makeover-1_files/figure-html5/after_bar_plot-1.png",
    "last_modified": "2021-02-14T03:29:43+09:00",
    "input_file": "plot-makeover-1.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-09-14-tidytuesday-2020-week-38/",
    "title": "TidyTuesday 2020 week 38",
    "description": "Visualizing two decades of primary and secondary education spending with {gt}",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-09-14",
    "categories": [
      "tables",
      "data visualization",
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nVisualization\r\nThings I learned\r\nThings to improve\r\n\r\nCode\r\n\r\n\r\n\r\n\r\nVisualization\r\nI had difficulty embedding an HTML table without overriding its styles so the table is also available on its own here.\r\n\r\n\r\n\r\nThings I learned\r\nBasics of working with tables and {gt}1\r\nPutting different font styles together in a nice way\r\nThings to improve\r\nSummarize the data a bit more so the table isn’t huge\r\nAdd conditional formatting (learn how tab_style() and tab_options() work)\r\nFigure out how to save {gt} tables into pdf or png\r\nFigure out how to include an html table without overriding css styles\r\nCode\r\nAlso available on github\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(gt)\r\n\r\nkids <- tidytuesdayR::tt_load(\"2020-09-15\")$kids\r\n\r\n\r\n# TABLE DATA\r\n\r\nstate_regions <- setNames(c(as.character(state.region), \"Northeast\"), c(state.name, \"District of Columbia\"))\r\n\r\nkids_tbl_data <- kids %>% \r\n  filter(variable == \"PK12ed\") %>%\r\n  mutate(region = state_regions[state]) %>% \r\n  select(region, state, year, inf_adj_perchild) %>% \r\n  pivot_wider(names_from = year, values_from = inf_adj_perchild) %>%\r\n  mutate(Trend = NA) \r\n\r\n\r\n# SPARKLINE\r\n\r\nplotter <- function(data){\r\n  data %>% \r\n    tibble(\r\n      year = 1997:2016,\r\n      value = data\r\n    ) %>% \r\n    ggplot(aes(year, value)) +\r\n    geom_line(size = 10, show.legend = FALSE) +\r\n    theme_void() +\r\n    scale_y_continuous(expand = c(0, 0))\r\n}\r\n\r\nspark_plots <- kids_tbl_data %>% \r\n  group_split(state) %>% \r\n  map(~ flatten_dbl(select(.x, where(is.numeric)))) %>% \r\n  map(plotter)\r\n\r\n\r\n# TABLE\r\n\r\nkids_tbl <- kids_tbl_data %>% \r\n  gt(\r\n    groupname_col = 'region',\r\n    rowname_col = 'state'\r\n  ) %>% \r\n  fmt_number(\r\n    columns = 3:22\r\n  ) %>% \r\n  summary_rows(\r\n    groups = TRUE,\r\n    columns = 3:22,\r\n    fns = list(Average = ~mean(.))\r\n  ) %>% \r\n  text_transform(\r\n    locations = cells_body(vars(Trend)),\r\n    fn = function(x){\r\n      map(spark_plots, ggplot_image, height = px(15), aspect_ratio = 4)\r\n    }\r\n  ) %>%\r\n  tab_header(\r\n    title = md(\"**State-by-State Spending on Primary and Secondary Education over 20 years**\"),\r\n    subtitle = md(\"*$1000s per child adjusted for inflation*\")\r\n  ) %>% \r\n  tab_source_note(\r\n    md(\"**By**: @yjunechoe<br>\r\n        **Inspiration**: @thomas_mock<br>\r\n        **Data**: Urban Institute | {tidykids} by Joshua Rosenberg\")\r\n  ) %>% \r\n  tab_style(\r\n    style = list(\r\n      cell_text(font = \"Futura MdCn BT\")\r\n    ),\r\n    locations = list(\r\n      cells_title(groups = \"title\")\r\n    )\r\n  ) %>%\r\n  tab_options(\r\n    table.width = 50,\r\n    heading.align = \"left\",\r\n    heading.title.font.size = 72,\r\n    heading.subtitle.font.size = 32,\r\n    row_group.font.size = 42,\r\n    row_group.font.weight = 'bold',\r\n    row_group.border.top.color = \"black\",\r\n    row_group.border.bottom.color = \"black\",\r\n    table.border.top.color = \"black\",\r\n    heading.border.bottom.color = \"white\",\r\n    heading.border.bottom.width = px(10),\r\n    table.font.names = \"Roboto\",\r\n    column_labels.font.size = 20,\r\n    column_labels.border.bottom.color = \"black\",\r\n    column_labels.border.bottom.width= px(3),\r\n    summary_row.border.color = \"black\", \r\n    summary_row.background.color = \"#c0c5ce\",\r\n    table.border.bottom.color = \"black\"\r\n  )\r\n\r\n\r\n\r\n\r\nMany thanks to Thomas Mock’s blog posts on {gt} (1) (2), a well as to the developers of {gt} for what I think is one of the most comprehensive vignette I’ve ever seen for a package!↩︎\r\n",
    "preview": "posts/2020-09-14-tidytuesday-2020-week-38/preview.png",
    "last_modified": "2020-11-06T01:08:39+09:00",
    "input_file": {},
    "preview_width": 1703,
    "preview_height": 2203
  },
  {
    "path": "posts/2020-09-12-videos-in-reactable/",
    "title": "Embedding videos in {reactable} tables",
    "description": "Pushing the limits of expandable row details",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-09-12",
    "categories": [
      "tables",
      "data visualization"
    ],
    "contents": "\r\n\r\n\r\nknitr::opts_chunk$set(\r\n  comment = \" \",\r\n  echo = TRUE,\r\n  message = TRUE,\r\n  warning = TRUE,\r\n  R.options = list(width = 80)\r\n)\r\n\r\n\r\n\r\nIn a {reactable} table, you can have a row expand to reveal more details by supplying the details argument with a function returning an image, raw html, another reactable table, etc. There are many examples of this in the package vignette, and they give you a good sense of just how flexible and powerful this feature is.\r\nMy first reaction to this was that it seemed like just about anything that can be displayed on a web page can be embedded in the expandable details. So what about something very unusual like… videos? Can {reactable} handle it? Are there potential usecases of this?\r\nAnnotated #tidytuesday screencasts\r\nWhile entertaining this idea, I remembered coming across a tweet by Alex Cookson with a link to a very detailed spreadsheet containing timestamped notes of David Robinson’s live #tidytuesday screencasts.\r\n\r\n\r\nAnyone other #rstats people find @drob's #TidyTuesday screencasts useful?I made a spreadsheet with timestamps for hundreds of specific tasks he does: https://t.co/HvJbLk1chdUseful if, like me, you keep going back and ask, “Where in the video did he do [this thing] again?”\r\n\r\n— Alex Cookson (@alexcookson) January 13, 2020\r\n\r\nSo I turned the spreadsheet into a {reactable} table with rows that can expand to reveal a Youtube video at the timestamp. I actually think this makes a really cool use case - it’s easier here than in Google Spreadsheet to navigate around the table with pagination and search bar, and you don’t need to constantly open and close Youtube videos in new windows (in fact, you can keep multiple videos open across rows here!).\r\nTry it out for yourself!\r\n\r\npreserve1e91bd3c9a653d66\r\n\r\nCode\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(htmltools)\r\nlibrary(reactable)\r\n\r\n# David Robinson's (@drob) #tidytuesday screencast annotations, made by Alex Cookson (@alexcookson)\r\nscreencasts <-\r\n  gsheet::gsheet2tbl(\"docs.google.com/spreadsheets/d/1pjj_G9ncJZPGTYPkR1BYwzA6bhJoeTfY2fJeGKSbOKM\") %>% \r\n  select(Screencast, Date, Timestamp = `Timestamp (sec)`, Link:Functions) %>% \r\n  mutate(Link = str_extract(Link, \"(?<=v=).*(?=&)\"))\r\n\r\n\r\n###############\r\n## The Table ##\r\n###############\r\n\r\nreactable(screencasts,\r\n  \r\n  # Function to embed Youtube Video \r\n  details = function(index){\r\n    \r\n    # Grab video info from hidden columns\r\n    link <- screencasts$Link[index]\r\n    time <- screencasts$Timestamp[index]\r\n    \r\n    # Div container to add grey padding around the video\r\n    tags$div(style = \"text-align:center; padding:10px; background:grey\",\r\n             \r\n             # The actual video\r\n             tags$iframe(\r\n               height = \"640\", width = \"640\", allow = \"fullscreen\",\r\n               src = glue::glue(\"https://www.youtube.com/embed/{link}?start={time}&autoplay=1\")\r\n             )\r\n             \r\n    )\r\n    \r\n  },\r\n  \r\n  # Column options\r\n  columns = list(\r\n    Link = colDef(show = F),\r\n    Timestamp = colDef(show = F),\r\n    Description = colDef(width = 500)\r\n  ),\r\n  \r\n  # Some theme options\r\n  searchable = TRUE,\r\n  bordered = TRUE,\r\n  fullWidth = TRUE,\r\n  theme = reactableTheme(\r\n    style = list(fontSize = '14px'),\r\n    searchInputStyle = list(width = \"100%\")\r\n  ),\r\n  \r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-09-12-videos-in-reactable/preview.png",
    "last_modified": "2021-02-14T03:39:58+09:00",
    "input_file": "2020-09-12-videos-in-reactable.utf8.md",
    "preview_width": 808,
    "preview_height": 617
  },
  {
    "path": "posts/2020-09-06-fonts-for-graphs/",
    "title": "Fonts for graphs",
    "description": "A small collection of my favorite fonts for data visualization",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-09-06",
    "categories": [
      "data visualization",
      "typography"
    ],
    "contents": "\r\n\r\n\r\n\r\n \r\nFor the last few weeks I’ve been reading about and experimenting with fonts for data visualization in my spare time.1 Out of that, I have found a couple fonts that I really like and wanted to do a small showcase of them here.\r\nThese fonts are all free and available for download at Google Fonts.2 Note that not only are they all large font families that come with many different styles, you can also adjust various theme settings like lineheight in {ggplot2}, so what I’m showing here isn’t the full extent of what you can make with these fonts.\r\n \r\n\r\nArial\r\nIt’s the default. It’s dull. It’s just here for comparison.\r\n\r\nMontserrat\r\nSimple design that can handle long lines of text. I like it for minimal plots.\r\n\r\nRoboto Mono\r\nMonospaced member of the Roboto family. Very easy to read.\r\n\r\nFutura Bk BT\r\nA slender and bold member of the Futura family. Looks nice even in larger sizes.\r\n\r\nBarlow\r\nAlso a slender font like Futura, but this has nicer ’j’s\r\n\r\nAdelle\r\nA serif font that doesn’t go overboard. I use it a lot for short paragraphs.\r\n\r\nMerriweather\r\nSimilar to Adelle, but has a bit more pronounced hooks\r\n\r\n\r\n \r\nMisc.\r\nWhy spend 3 minutes copy-pasting code when you can spend an hour automatizing it?\r\nThis was my first time using dynamic Rmarkdown reporting. The plots above and the text descriptions that went with them were generated in a for loop, which I learned about here.\r\nHere is the single chunk of code that made this possible:\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(extrafont)\r\nknitr::opts_chunk$set(fig.width = 7, dpi = 600)\r\n\r\ntheme_set(theme_classic(base_size = 14))\r\n\r\nfavorites <- c(\r\n  \"Arial\" = \"It's the default. It's dull. It's just here for comparison.\",\r\n  \"Montserrat\" = \"Simple design that can handle long lines of text. I like it for minimal plots.\",\r\n  \"Roboto Mono\" = \"Monospaced member of the Roboto family. Very easy to read.\",\r\n  \"Futura Bk BT\" = \"A slender and bold member of the Futura family. Looks nice even in larger sizes.\",\r\n  \"Barlow\" = \"Also a slender font like Futura, but this has nicer 'j's\",\r\n  \"Adelle\" = \"A serif font that doesn't go overboard. I use it a lot for short paragraphs.\",\r\n  \"Merriweather\" = \"Similar to Adelle, but has a bit more pronounced hooks\"\r\n)\r\n\r\n\r\nfor (font in names(favorites)) {\r\n  cat(\"\\n\\n## \", font, \"\\n\\n\")\r\n  cat(\"\", favorites[font], \"\\n\\n\")\r\n  plot <- qplot(data = mtcars, mpg, disp, color = factor(cyl)) +\r\n    annotate(\"text\", 28, 400, label = paste(letters, collapse = ''), family = font) +\r\n    geom_curve(aes(x = 28, y = 380, xend = 22, yend = 260),\r\n               color = 'black', curvature = -.3, arrow = arrow(), show.legend = FALSE) +\r\n    labs(title = \"This is an interesting plot title\",\r\n         subtitle = \"Here's the subtitle 1234567890\",\r\n         caption = \"This is the plot caption\") +\r\n    theme(text = element_text(family = font),\r\n          plot.title.position = 'plot')\r\n  print(plot)\r\n}\r\n\r\n\r\n\r\n\r\nI reference a lot this great collection of fonts used in profesional visualization here.↩︎\r\nFor how to import local fonts into R to use for plotting, check out {extrafont} and/or {showtext}.↩︎\r\n",
    "preview": "posts/2020-09-06-fonts-for-graphs/preview.png",
    "last_modified": "2020-11-02T09:42:42+09:00",
    "input_file": {},
    "preview_width": 1144,
    "preview_height": 675
  },
  {
    "path": "posts/2020-08-17-tidytuesday-2020-week-33/",
    "title": "TidyTuesday 2020 Week 33",
    "description": "An animation of the main characters in Avatar",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-08-17",
    "categories": [
      "tidytuesday",
      "gganimate",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nVisualization\r\nThings I learned\r\nThings to improve\r\n\r\nCode\r\n\r\n\r\n\r\n\r\nVisualization\r\n \r\n\r\n\r\n\r\nThings I learned\r\nReally basic image manipulation with {magick}\r\nThat you can get away with not doing the data part of data visualization for TidyTuesday\r\nThings to improve\r\nShould’ve picked a better color to represent air for Aang.\r\nNot sure why it looks like white grid lines are there. All my attempts at getting rid of them failed, so I’ve just concluded that they’re likely an optical illusion.\r\nCode\r\nAlso available on github\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(magick)\r\nlibrary(gganimate)\r\n\r\naang <- image_read(\"https://vignette.wikia.nocookie.net/avatar/images/a/ae/Aang_at_Jasmine_Dragon.png\")\r\niroh <- image_read(\"https://vignette.wikia.nocookie.net/avatar/images/c/c1/Iroh_smiling.png\")\r\nsokka <- image_read(\"https://vignette.wikia.nocookie.net/avatar/images/c/cc/Sokka.png\")\r\ntoph <- image_read(\"https://vignette.wikia.nocookie.net/avatar/images/4/46/Toph_Beifong.png\")\r\n\r\n# Script by Georgios Karamanis adapted and wrapped into a function\r\n# - from https://github.com/gkaramanis/aRt/blob/master/split-bar/points-portraits.R\r\nimg_to_df <- function(img, index) {\r\n  \r\n  img <- image_convert(img, colorspace = \"gray\")\r\n  \r\n  img_w <- image_info(img)$width\r\n  img_h <- image_info(img)$height\r\n  \r\n  if (img_w >= img_h) {\r\n    img <- image_resize(img, \"120\")\r\n  } else {\r\n    img <- image_resize(img, (\"x120\"))\r\n  }\r\n  \r\n  img_array <- drop(as.integer(img[[1]]))\r\n  rownames(img_array) <- 1:nrow(img_array)\r\n  colnames(img_array) <- 1:ncol(img_array)\r\n  \r\n  as.data.frame.table(img_array) %>% \r\n    `colnames<-`(c(\"y\", \"x\", \"b\")) %>% \r\n    mutate(\r\n      across(everything(), as.numeric),\r\n      bf = 1 - b / 255\r\n    ) %>% \r\n    mutate(character_id = index)\r\n}\r\n\r\nplot_data <- imap_dfr(list(aang, iroh, sokka, toph), ~img_to_df(.x, .y)) %>% \r\n  group_by(character_id) %>% \r\n  mutate(point_id = 1:n()) %>% \r\n  ungroup() %>% \r\n  mutate(across(contains(\"id\"), as.factor))\r\n\r\nanim <- ggplot(plot_data) +\r\n  geom_point(aes(x = x, y = y, size = bf, group = point_id, color = character_id),\r\n             shape = 16, show.legend = FALSE) +\r\n  scale_y_reverse() +\r\n  scale_size_continuous(range = c(0, 4)) +\r\n  scale_color_manual(values = c(\"#C0EDFF\", \"#B33000\", \"#206BA4\", \"#8B4513\")) +\r\n  coord_fixed(expand = FALSE) +\r\n  theme_void() +\r\n  theme(panel.grid = element_blank()) +\r\n  transition_states(id)\r\n\r\nanimate(anim, width = 12, height = 9, units = \"in\", res = 120)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-17-tidytuesday-2020-week-33/preview.png",
    "last_modified": "2020-11-02T09:44:08+09:00",
    "input_file": {},
    "preview_width": 1289,
    "preview_height": 964
  },
  {
    "path": "posts/2020-08-07-saving-a-line-of-piping/",
    "title": "Saving a line of piping",
    "description": "Some notes on lesser known functions/functionalities that combine common chain of {dplyr} verbs.",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-08-07",
    "categories": [
      "data wrangling",
      "dplyr",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\n1. rename() inside select()\r\n2. rename() inside count()\r\n3. mutate() inside count()\r\n4. transmute() + select()\r\n5. ungroup() inside summarize()\r\n6. arrange() + other features inside slice()\r\n7. count and sum by group with add_count()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nUsing a cleaned up version of penguins data from {palmerpenguins}:\r\n\r\n\r\ndata(\"penguins\", package = \"palmerpenguins\")\r\n\r\npenguins <- na.omit(penguins)\r\n\r\n\r\n\r\n\r\n\r\n{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"species\":[\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Gentoo\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\"],\"island\":[\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Biscoe\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\"],\"bill_length_mm\":[39.1,39.5,40.3,36.7,39.3,38.9,39.2,41.1,38.6,34.6,36.6,38.7,42.5,34.4,46,37.8,37.7,35.9,38.2,38.8,35.3,40.6,40.5,37.9,40.5,39.5,37.2,39.5,40.9,36.4,39.2,38.8,42.2,37.6,39.8,36.5,40.8,36,44.1,37,39.6,41.1,36,42.3,39.6,40.1,35,42,34.5,41.4,39,40.6,36.5,37.6,35.7,41.3,37.6,41.1,36.4,41.6,35.5,41.1,35.9,41.8,33.5,39.7,39.6,45.8,35.5,42.8,40.9,37.2,36.2,42.1,34.6,42.9,36.7,35.1,37.3,41.3,36.3,36.9,38.3,38.9,35.7,41.1,34,39.6,36.2,40.8,38.1,40.3,33.1,43.2,35,41,37.7,37.8,37.9,39.7,38.6,38.2,38.1,43.2,38.1,45.6,39.7,42.2,39.6,42.7,38.6,37.3,35.7,41.1,36.2,37.7,40.2,41.4,35.2,40.6,38.8,41.5,39,44.1,38.5,43.1,36.8,37.5,38.1,41.1,35.6,40.2,37,39.7,40.2,40.6,32.1,40.7,37.3,39,39.2,36.6,36,37.8,36,41.5,46.1,50,48.7,50,47.6,46.5,45.4,46.7,43.3,46.8,40.9,49,45.5,48.4,45.8,49.3,42,49.2,46.2,48.7,50.2,45.1,46.5,46.3,42.9,46.1,47.8,48.2,50,47.3,42.8,45.1,59.6,49.1,48.4,42.6,44.4,44,48.7,42.7,49.6,45.3,49.6,50.5,43.6,45.5,50.5,44.9,45.2,46.6,48.5,45.1,50.1,46.5,45,43.8,45.5,43.2,50.4,45.3,46.2,45.7,54.3,45.8,49.8,49.5,43.5,50.7,47.7,46.4,48.2,46.5,46.4,48.6,47.5,51.1,45.2,45.2,49.1,52.5,47.4,50,44.9,50.8,43.4,51.3,47.5,52.1,47.5,52.2,45.5,49.5,44.5,50.8,49.4,46.9,48.4,51.1,48.5,55.9,47.2,49.1,46.8,41.7,53.4,43.3,48.1,50.5,49.8,43.5,51.5,46.2,55.1,48.8,47.2,46.8,50.4,45.2,49.9,46.5,50,51.3,45.4,52.7,45.2,46.1,51.3,46,51.3,46.6,51.7,47,52,45.9,50.5,50.3,58,46.4,49.2,42.4,48.5,43.2,50.6,46.7,52,50.5,49.5,46.4,52.8,40.9,54.2,42.5,51,49.7,47.5,47.6,52,46.9,53.5,49,46.2,50.9,45.5,50.9,50.8,50.1,49,51.5,49.8,48.1,51.4,45.7,50.7,42.5,52.2,45.2,49.3,50.2,45.6,51.9,46.8,45.7,55.8,43.5,49.6,50.8,50.2],\"bill_depth_mm\":[18.7,17.4,18,19.3,20.6,17.8,19.6,17.6,21.2,21.1,17.8,19,20.7,18.4,21.5,18.3,18.7,19.2,18.1,17.2,18.9,18.6,17.9,18.6,18.9,16.7,18.1,17.8,18.9,17,21.1,20,18.5,19.3,19.1,18,18.4,18.5,19.7,16.9,18.8,19,17.9,21.2,17.7,18.9,17.9,19.5,18.1,18.6,17.5,18.8,16.6,19.1,16.9,21.1,17,18.2,17.1,18,16.2,19.1,16.6,19.4,19,18.4,17.2,18.9,17.5,18.5,16.8,19.4,16.1,19.1,17.2,17.6,18.8,19.4,17.8,20.3,19.5,18.6,19.2,18.8,18,18.1,17.1,18.1,17.3,18.9,18.6,18.5,16.1,18.5,17.9,20,16,20,18.6,18.9,17.2,20,17,19,16.5,20.3,17.7,19.5,20.7,18.3,17,20.5,17,18.6,17.2,19.8,17,18.5,15.9,19,17.6,18.3,17.1,18,17.9,19.2,18.5,18.5,17.6,17.5,17.5,20.1,16.5,17.9,17.1,17.2,15.5,17,16.8,18.7,18.6,18.4,17.8,18.1,17.1,18.5,13.2,16.3,14.1,15.2,14.5,13.5,14.6,15.3,13.4,15.4,13.7,16.1,13.7,14.6,14.6,15.7,13.5,15.2,14.5,15.1,14.3,14.5,14.5,15.8,13.1,15.1,15,14.3,15.3,15.3,14.2,14.5,17,14.8,16.3,13.7,17.3,13.6,15.7,13.7,16,13.7,15,15.9,13.9,13.9,15.9,13.3,15.8,14.2,14.1,14.4,15,14.4,15.4,13.9,15,14.5,15.3,13.8,14.9,13.9,15.7,14.2,16.8,16.2,14.2,15,15,15.6,15.6,14.8,15,16,14.2,16.3,13.8,16.4,14.5,15.6,14.6,15.9,13.8,17.3,14.4,14.2,14,17,15,17.1,14.5,16.1,14.7,15.7,15.8,14.6,14.4,16.5,15,17,15.5,15,16.1,14.7,15.8,14,15.1,15.2,15.9,15.2,16.3,14.1,16,16.2,13.7,14.3,15.7,14.8,16.1,17.9,19.5,19.2,18.7,19.8,17.8,18.2,18.2,18.9,19.9,17.8,20.3,17.3,18.1,17.1,19.6,20,17.8,18.6,18.2,17.3,17.5,16.6,19.4,17.9,19,18.4,19,17.8,20,16.6,20.8,16.7,18.8,18.6,16.8,18.3,20.7,16.6,19.9,19.5,17.5,19.1,17,17.9,18.5,17.9,19.6,18.7,17.3,16.4,19,17.3,19.7,17.3,18.8,16.6,19.9,18.8,19.4,19.5,16.5,17,19.8,18.1,18.2,19,18.7],\"flipper_length_mm\":[181,186,195,193,190,181,195,182,191,198,185,195,197,184,194,174,180,189,185,180,187,183,187,172,180,178,178,188,184,195,196,190,180,181,184,182,195,186,196,185,190,182,190,191,186,188,190,200,187,191,186,193,181,194,185,195,185,192,184,192,195,188,190,198,190,190,196,197,190,195,191,184,187,195,189,196,187,193,191,194,190,189,189,190,202,205,185,186,187,208,190,196,178,192,192,203,183,190,193,184,199,190,181,197,198,191,193,197,191,196,188,199,189,189,187,198,176,202,186,199,191,195,191,210,190,197,193,199,187,190,191,200,185,193,193,187,188,190,192,185,190,184,195,193,187,201,211,230,210,218,215,210,211,219,209,215,214,216,214,213,210,217,210,221,209,222,218,215,213,215,215,215,215,210,220,222,209,207,230,220,220,213,219,208,208,208,225,210,216,222,217,210,225,213,215,210,220,210,225,217,220,208,220,208,224,208,221,214,231,219,230,229,220,223,216,221,221,217,216,230,209,220,215,223,212,221,212,224,212,228,218,218,212,230,218,228,212,224,214,226,216,222,203,225,219,228,215,228,215,210,219,208,209,216,229,213,230,217,230,222,214,215,222,212,213,192,196,193,188,197,198,178,197,195,198,193,194,185,201,190,201,197,181,190,195,181,191,187,193,195,197,200,200,191,205,187,201,187,203,195,199,195,210,192,205,210,187,196,196,196,201,190,212,187,198,199,201,193,203,187,197,191,203,202,194,206,189,195,207,202,193,210,198],\"body_mass_g\":[3750,3800,3250,3450,3650,3625,4675,3200,3800,4400,3700,3450,4500,3325,4200,3400,3600,3800,3950,3800,3800,3550,3200,3150,3950,3250,3900,3300,3900,3325,4150,3950,3550,3300,4650,3150,3900,3100,4400,3000,4600,3425,3450,4150,3500,4300,3450,4050,2900,3700,3550,3800,2850,3750,3150,4400,3600,4050,2850,3950,3350,4100,3050,4450,3600,3900,3550,4150,3700,4250,3700,3900,3550,4000,3200,4700,3800,4200,3350,3550,3800,3500,3950,3600,3550,4300,3400,4450,3300,4300,3700,4350,2900,4100,3725,4725,3075,4250,2925,3550,3750,3900,3175,4775,3825,4600,3200,4275,3900,4075,2900,3775,3350,3325,3150,3500,3450,3875,3050,4000,3275,4300,3050,4000,3325,3500,3500,4475,3425,3900,3175,3975,3400,4250,3400,3475,3050,3725,3000,3650,4250,3475,3450,3750,3700,4000,4500,5700,4450,5700,5400,4550,4800,5200,4400,5150,4650,5550,4650,5850,4200,5850,4150,6300,4800,5350,5700,5000,4400,5050,5000,5100,5650,4600,5550,5250,4700,5050,6050,5150,5400,4950,5250,4350,5350,3950,5700,4300,4750,5550,4900,4200,5400,5100,5300,4850,5300,4400,5000,4900,5050,4300,5000,4450,5550,4200,5300,4400,5650,4700,5700,5800,4700,5550,4750,5000,5100,5200,4700,5800,4600,6000,4750,5950,4625,5450,4725,5350,4750,5600,4600,5300,4875,5550,4950,5400,4750,5650,4850,5200,4925,4875,4625,5250,4850,5600,4975,5500,5500,4700,5500,4575,5500,5000,5950,4650,5500,4375,5850,6000,4925,4850,5750,5200,5400,3500,3900,3650,3525,3725,3950,3250,3750,4150,3700,3800,3775,3700,4050,3575,4050,3300,3700,3450,4400,3600,3400,2900,3800,3300,4150,3400,3800,3700,4550,3200,4300,3350,4100,3600,3900,3850,4800,2700,4500,3950,3650,3550,3500,3675,4450,3400,4300,3250,3675,3325,3950,3600,4050,3350,3450,3250,4050,3800,3525,3950,3650,3650,4000,3400,3775,4100,3775],\"sex\":[\"male\",\"female\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\",\"male\",\"female\",\"female\",\"male\",\"female\",\"male\",\"male\",\"female\"],\"year\":[2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2007,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2008,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009,2009]},\"columns\":[{\"accessor\":\"species\",\"name\":\"species\",\"type\":\"factor\"},{\"accessor\":\"island\",\"name\":\"island\",\"type\":\"factor\"},{\"accessor\":\"bill_length_mm\",\"name\":\"bill_length_mm\",\"type\":\"numeric\"},{\"accessor\":\"bill_depth_mm\",\"name\":\"bill_depth_mm\",\"type\":\"numeric\"},{\"accessor\":\"flipper_length_mm\",\"name\":\"flipper_length_mm\",\"type\":\"numeric\"},{\"accessor\":\"body_mass_g\",\"name\":\"body_mass_g\",\"type\":\"numeric\"},{\"accessor\":\"sex\",\"name\":\"sex\",\"type\":\"factor\"},{\"accessor\":\"year\",\"name\":\"year\",\"type\":\"numeric\"}],\"sortable\":false,\"defaultPageSize\":5,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"bordered\":true,\"nowrap\":true,\"dataKey\":\"b650b7a1a5c792f49c2b2303a0449c36\",\"key\":\"b650b7a1a5c792f49c2b2303a0449c36\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}\r\n1. rename() inside select()\r\nYou can rename a column inside select() by assigning a new name on the left hand side:\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   select(species, island) %>% \r\n#   rename(penguin_species = species)\r\n\r\npenguins %>% \r\n  select(penguin_species = species,\r\n         island)\r\n\r\n\r\n  # A tibble: 333 x 2\r\n     penguin_species island   \r\n     <fct>           <fct>    \r\n   1 Adelie          Torgersen\r\n   2 Adelie          Torgersen\r\n   3 Adelie          Torgersen\r\n   4 Adelie          Torgersen\r\n   5 Adelie          Torgersen\r\n   6 Adelie          Torgersen\r\n   7 Adelie          Torgersen\r\n   8 Adelie          Torgersen\r\n   9 Adelie          Torgersen\r\n  10 Adelie          Torgersen\r\n  # ... with 323 more rows\r\n\r\nThis also works with {tidyselect} helpers like starts_with(), ends_with(), contains(), and matches():\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   select(species, island) %>% \r\n#   rename(penguin_species = species,\r\n#          weight = body_weight_g)\r\n\r\npenguins %>% \r\n  select(penguin_species = species,\r\n         island,\r\n         weight = contains(\"mass\"))\r\n\r\n\r\n  # A tibble: 333 x 3\r\n     penguin_species island    weight\r\n     <fct>           <fct>      <int>\r\n   1 Adelie          Torgersen   3750\r\n   2 Adelie          Torgersen   3800\r\n   3 Adelie          Torgersen   3250\r\n   4 Adelie          Torgersen   3450\r\n   5 Adelie          Torgersen   3650\r\n   6 Adelie          Torgersen   3625\r\n   7 Adelie          Torgersen   4675\r\n   8 Adelie          Torgersen   3200\r\n   9 Adelie          Torgersen   3800\r\n  10 Adelie          Torgersen   4400\r\n  # ... with 323 more rows\r\n\r\n2. rename() inside count()\r\nYou can rename the new column of counts (n by default) using the name argument:\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   count(species) %>% \r\n#   rename(total = n)\r\n\r\npenguins %>% \r\n  count(species, name = \"total\")\r\n\r\n\r\n  # A tibble: 3 x 2\r\n    species   total\r\n    <fct>     <int>\r\n  1 Adelie      146\r\n  2 Chinstrap    68\r\n  3 Gentoo      119\r\n\r\nYou can also rename the column(s) that are selected for counting in the same way as shown in the select() examples above:\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   count(species) %>% \r\n#   rename(total = n,\r\n#          penguin_species = species)\r\n\r\npenguins %>% \r\n  count(penguin_species = species, name = \"total\")\r\n\r\n\r\n  # A tibble: 3 x 2\r\n    penguin_species total\r\n    <fct>           <int>\r\n  1 Adelie            146\r\n  2 Chinstrap          68\r\n  3 Gentoo            119\r\n\r\nNote that the new name passed into the name argument must be quoted, but the new name for selected column needs not to be unquoted:\r\n\r\n\r\nidentical(\r\n  # Method 1: new column name UNQUOTED\r\n  penguins %>% \r\n    count(penguin_species = species, name = \"total\"),\r\n  # Method 2: new column name QUOTED\r\n  penguins %>% \r\n    count(\"penguin_species\" = species, name = \"total\") \r\n)\r\n\r\n\r\n  [1] TRUE\r\n\r\nI prefer to unquote the new column names to keep it consistent with the recommended style for rename()\r\nThis feature of select() may seem weird and hackish (and I guess it sort of is in this demonstration) but it’s explicitly documented here if you want to read more on it.\r\n3. mutate() inside count()\r\nYou can also create a new column to count by inside count(). This works very similarly to the above, but I think it’s worth its own mention.\r\nIt’s pretty simple - you just do what you’d do for mutate() inside count():\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   mutate(long_beak = bill_length_mm > 50) %>% \r\n#   count(long_beak)\r\n\r\npenguins %>% \r\n  count(long_beak = bill_length_mm > 50)\r\n\r\n\r\n  # A tibble: 2 x 2\r\n    long_beak     n\r\n    <lgl>     <int>\r\n  1 FALSE       281\r\n  2 TRUE         52\r\n\r\nAnd of course, this also works when specifying multiple variables to count by:\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   mutate(long_beak = bill_length_mm > 50,\r\n#          is_adelie = species == \"Adelie\") %>% \r\n#   count(is_adelie, long_beak)\r\n\r\npenguins %>% \r\n  count(long_beak = bill_length_mm > 50,\r\n        is_adelie = species == \"Adelie\")\r\n\r\n\r\n  # A tibble: 3 x 3\r\n    long_beak is_adelie     n\r\n    <lgl>     <lgl>     <int>\r\n  1 FALSE     FALSE       135\r\n  2 FALSE     TRUE        146\r\n  3 TRUE      FALSE        52\r\n\r\n4. transmute() + select()\r\ntransmute() is a function that mutates columns and returns only those columns:\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   mutate(body_mass_kg = body_mass_g/1000) %>% \r\n#   select(body_mass_kg)\r\n\r\npenguins %>% \r\n  transmute(body_mass_kg = body_mass_g/1000)\r\n\r\n\r\n  # A tibble: 333 x 1\r\n     body_mass_kg\r\n            <dbl>\r\n   1         3.75\r\n   2         3.8 \r\n   3         3.25\r\n   4         3.45\r\n   5         3.65\r\n   6         3.62\r\n   7         4.68\r\n   8         3.2 \r\n   9         3.8 \r\n  10         4.4 \r\n  # ... with 323 more rows\r\n\r\nI’ve rarely used transmute() in the past because I thought it could only return modified columns, which would be very limiting (like in the above example, what good is a single column of penguin body mass in kilograms?)\r\nBut actually you can just name the columns you want to include in transmute() like you would in select() to carry over columns that you aren’t modifying. And of course, you can “rename” them as you do it1:\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   mutate(body_mass_kg = body_mass_g/1000) %>% \r\n#   select(species, island, body_mass_kg) %>% \r\n#   rename(penguin_species = species)\r\n\r\npenguins %>% \r\n  transmute(penguin_species = species,\r\n            island,\r\n            body_mass_kg = body_mass_g/1000)\r\n\r\n\r\n  # A tibble: 333 x 3\r\n     penguin_species island    body_mass_kg\r\n     <fct>           <fct>            <dbl>\r\n   1 Adelie          Torgersen         3.75\r\n   2 Adelie          Torgersen         3.8 \r\n   3 Adelie          Torgersen         3.25\r\n   4 Adelie          Torgersen         3.45\r\n   5 Adelie          Torgersen         3.65\r\n   6 Adelie          Torgersen         3.62\r\n   7 Adelie          Torgersen         4.68\r\n   8 Adelie          Torgersen         3.2 \r\n   9 Adelie          Torgersen         3.8 \r\n  10 Adelie          Torgersen         4.4 \r\n  # ... with 323 more rows\r\n\r\n5. ungroup() inside summarize()\r\nI always found using ungroup() after summarize() to be extremely ugly, but I found myself using it a lot to remove left-over groupings after a summarize call:\r\n\r\n\r\npenguins %>% \r\n  group_by(island, species) %>% \r\n  summarize(mean_mass = mean(body_mass_g, na.rm = TRUE)) %>% \r\n  ungroup()\r\n\r\n\r\n  # A tibble: 5 x 3\r\n    island    species   mean_mass\r\n    <fct>     <fct>         <dbl>\r\n  1 Biscoe    Adelie        3710.\r\n  2 Biscoe    Gentoo        5092.\r\n  3 Dream     Adelie        3701.\r\n  4 Dream     Chinstrap     3733.\r\n  5 Torgersen Adelie        3709.\r\n\r\n… because summarize() only drops the last grouping variable by defaut, meaning that the output is still grouped by the island variable if ungroup() isn’t called:\r\n\r\n\r\n# Without ungroup()\r\npenguins %>% \r\n  group_by(island, species) %>% \r\n  summarize(mean_mass = mean(body_mass_g, na.rm = TRUE)) %>% \r\n  group_vars()\r\n\r\n\r\n  [1] \"island\"\r\n\r\n# With ungroup()\r\npenguins %>% \r\n  group_by(island, species) %>% \r\n  summarize(mean_mass = mean(body_mass_g, na.rm = TRUE)) %>% \r\n  ungroup() %>% \r\n  group_vars()\r\n\r\n\r\n  character(0)\r\n\r\nSince {dplyr} 1.0.0, you can simply set the .groups argument inside summarize() to 'drop' to achieve the same:\r\n\r\n\r\npenguins %>% \r\n  group_by(island, species) %>% \r\n  summarize(mean_mass = mean(body_mass_g, na.rm = TRUE), .groups = 'drop')\r\n\r\n\r\n  # A tibble: 5 x 3\r\n    island    species   mean_mass\r\n    <fct>     <fct>         <dbl>\r\n  1 Biscoe    Adelie        3710.\r\n  2 Biscoe    Gentoo        5092.\r\n  3 Dream     Adelie        3701.\r\n  4 Dream     Chinstrap     3733.\r\n  5 Torgersen Adelie        3709.\r\n\r\nBut ungroup() still remains relevant as you can now selectively remove grouping variables in {dplyr} 1.0.0.\r\n6. arrange() + other features inside slice()\r\nIn past versions of {dplyr}, if you wanted to grab the top n rows sorted by a column, you’d use top_n(), which provides a simpler way of doing slice() + arrange():\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   arrange(desc(body_mass_g)) %>% \r\n#   slice(1:5)\r\n\r\npenguins %>% \r\n  top_n(5, wt = body_mass_g)\r\n\r\n\r\n  # A tibble: 6 x 8\r\n    species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex  \r\n    <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\r\n  1 Gentoo  Biscoe           49.2          15.2              221        6300 male \r\n  2 Gentoo  Biscoe           59.6          17                230        6050 male \r\n  3 Gentoo  Biscoe           51.1          16.3              220        6000 male \r\n  4 Gentoo  Biscoe           45.2          16.4              223        5950 male \r\n  5 Gentoo  Biscoe           49.8          15.9              229        5950 male \r\n  6 Gentoo  Biscoe           48.8          16.2              222        6000 male \r\n  # ... with 1 more variable: year <int>\r\n\r\nBut the recent {dplyr} 1.0.0 augmented slice() with variants like slice_min() and slice_max() that now supresede top_n():\r\n\r\n\r\n##### Pre-1.0.0 #####\r\n# penguins %>% \r\n#   top_n(5, wt = body_mass_g)\r\n\r\npenguins %>% \r\n  slice_max(order_by = body_mass_g, n = 5)\r\n\r\n\r\n  # A tibble: 6 x 8\r\n    species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex  \r\n    <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\r\n  1 Gentoo  Biscoe           49.2          15.2              221        6300 male \r\n  2 Gentoo  Biscoe           59.6          17                230        6050 male \r\n  3 Gentoo  Biscoe           51.1          16.3              220        6000 male \r\n  4 Gentoo  Biscoe           48.8          16.2              222        6000 male \r\n  5 Gentoo  Biscoe           45.2          16.4              223        5950 male \r\n  6 Gentoo  Biscoe           49.8          15.9              229        5950 male \r\n  # ... with 1 more variable: year <int>\r\n\r\nNote that the order of arguments is different for slice_min/max() - the first argument after piping is where you specify the variable for ordering rather than the number of rows to return, like in top_n().\r\nThis is because slice_min/max() gives you an option to either specify a certain number of rows n or a proportion of rows prop:\r\n\r\n\r\npenguins %>% \r\n  slice_max(body_mass_g, prop = .01)\r\n\r\n\r\n  # A tibble: 4 x 8\r\n    species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex  \r\n    <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\r\n  1 Gentoo  Biscoe           49.2          15.2              221        6300 male \r\n  2 Gentoo  Biscoe           59.6          17                230        6050 male \r\n  3 Gentoo  Biscoe           51.1          16.3              220        6000 male \r\n  4 Gentoo  Biscoe           48.8          16.2              222        6000 male \r\n  # ... with 1 more variable: year <int>\r\n\r\nAnd actually, the most significant change with the new slice_*() functions is from adding appropriate behavior for grouped dataframes.\r\nSo for example, this example below returns the top 5% of penguins by weight for each species:\r\n\r\n\r\npenguins %>% \r\n  group_by(species) %>% \r\n  slice_max(body_mass_g, prop = .05)\r\n\r\n\r\n  # A tibble: 16 x 8\r\n  # Groups:   species [3]\r\n     species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g\r\n     <fct>   <fct>           <dbl>         <dbl>            <int>       <int>\r\n   1 Adelie  Biscoe           43.2          19                197        4775\r\n   2 Adelie  Biscoe           41            20                203        4725\r\n   3 Adelie  Torge~           42.9          17.6              196        4700\r\n   4 Adelie  Torge~           39.2          19.6              195        4675\r\n   5 Adelie  Dream            39.8          19.1              184        4650\r\n   6 Adelie  Dream            39.6          18.8              190        4600\r\n   7 Adelie  Biscoe           45.6          20.3              191        4600\r\n   8 Chinst~ Dream            52            20.7              210        4800\r\n   9 Chinst~ Dream            52.8          20                205        4550\r\n  10 Chinst~ Dream            53.5          19.9              205        4500\r\n  11 Gentoo  Biscoe           49.2          15.2              221        6300\r\n  12 Gentoo  Biscoe           59.6          17                230        6050\r\n  13 Gentoo  Biscoe           51.1          16.3              220        6000\r\n  14 Gentoo  Biscoe           48.8          16.2              222        6000\r\n  15 Gentoo  Biscoe           45.2          16.4              223        5950\r\n  16 Gentoo  Biscoe           49.8          15.9              229        5950\r\n  # ... with 2 more variables: sex <fct>, year <int>\r\n\r\nBut note that slice_*() functions do not modify groups in the result if the input is a grouped dataframe, so you need to explicitly add a call to ungroup() if you want to drop groups after slicing.\r\n7. count and sum by group with add_count()\r\nSaving my favorite lesser-known {dplyr} function for last!\r\nadd_count() adds a column with the counts of each group (or combination of groups):\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   group_by(species) %>% \r\n#   mutate(count_by_species = n()) %>% \r\n#   ungroup()\r\n\r\npenguins %>% \r\n  add_count(species, name = \"count_by_species\") %>% \r\n  # cutting down some columns to show the new column\r\n  select(-contains(\"mm\"))\r\n\r\n\r\n  # A tibble: 333 x 6\r\n     species island    body_mass_g sex     year count_by_species\r\n     <fct>   <fct>           <int> <fct>  <int>            <int>\r\n   1 Adelie  Torgersen        3750 male    2007              146\r\n   2 Adelie  Torgersen        3800 female  2007              146\r\n   3 Adelie  Torgersen        3250 female  2007              146\r\n   4 Adelie  Torgersen        3450 female  2007              146\r\n   5 Adelie  Torgersen        3650 male    2007              146\r\n   6 Adelie  Torgersen        3625 female  2007              146\r\n   7 Adelie  Torgersen        4675 male    2007              146\r\n   8 Adelie  Torgersen        3200 female  2007              146\r\n   9 Adelie  Torgersen        3800 male    2007              146\r\n  10 Adelie  Torgersen        4400 male    2007              146\r\n  # ... with 323 more rows\r\n\r\nYou can use the wt to effectively get sums by group (perhaps hackish but very very useful):\r\n\r\n\r\n##### Long Form #####\r\n# penguins %>% \r\n#   group_by(species) %>% \r\n#   mutate(total_weight_by_species = sum(body_mass_g)) %>% \r\n#   ungroup()\r\n  \r\n\r\npenguins %>% \r\n  add_count(species, wt = body_mass_g, name = \"total_weight_by_species\") %>% \r\n    # cutting down some columns to show the new column\r\n  select(-contains(\"mm\"))\r\n\r\n\r\n  # A tibble: 333 x 6\r\n     species island    body_mass_g sex     year total_weight_by_species\r\n     <fct>   <fct>           <int> <fct>  <int>                   <int>\r\n   1 Adelie  Torgersen        3750 male    2007                  541100\r\n   2 Adelie  Torgersen        3800 female  2007                  541100\r\n   3 Adelie  Torgersen        3250 female  2007                  541100\r\n   4 Adelie  Torgersen        3450 female  2007                  541100\r\n   5 Adelie  Torgersen        3650 male    2007                  541100\r\n   6 Adelie  Torgersen        3625 female  2007                  541100\r\n   7 Adelie  Torgersen        4675 male    2007                  541100\r\n   8 Adelie  Torgersen        3200 female  2007                  541100\r\n   9 Adelie  Torgersen        3800 male    2007                  541100\r\n  10 Adelie  Torgersen        4400 male    2007                  541100\r\n  # ... with 323 more rows\r\n\r\nAlso check out its more primitive version add_tally().\r\nBy default, add_tally() adds a count of rows, which you can already do with mutate(n = n()), but it shines when you make use of its wt argument:\r\n\r\n\r\npenguins %>% \r\n  add_count(species, wt = body_mass_g, name = \"total_weight_by_species\") %>% \r\n  add_tally(wt = body_mass_g, name = \"total_weight_of_all_species\") %>% \r\n  select(1:2, last_col(0):last_col(1))\r\n\r\n\r\n  # A tibble: 333 x 4\r\n     species island    total_weight_of_all_species total_weight_by_species\r\n     <fct>   <fct>                           <int>                   <int>\r\n   1 Adelie  Torgersen                     1400950                  541100\r\n   2 Adelie  Torgersen                     1400950                  541100\r\n   3 Adelie  Torgersen                     1400950                  541100\r\n   4 Adelie  Torgersen                     1400950                  541100\r\n   5 Adelie  Torgersen                     1400950                  541100\r\n   6 Adelie  Torgersen                     1400950                  541100\r\n   7 Adelie  Torgersen                     1400950                  541100\r\n   8 Adelie  Torgersen                     1400950                  541100\r\n   9 Adelie  Torgersen                     1400950                  541100\r\n  10 Adelie  Torgersen                     1400950                  541100\r\n  # ... with 323 more rows\r\n\r\n\r\nWhat happens under the hood is actually copying of a sort, so this is probably not the best approach if you care about efficiency. As a case in point, you can’t use {tidyselect} helpers in transmute because you’re creating a new dataframe↩︎\r\n",
    "preview": "posts/2020-08-07-saving-a-line-of-piping/preview.png",
    "last_modified": "2020-11-05T12:34:55+09:00",
    "input_file": {},
    "preview_width": 877,
    "preview_height": 372
  },
  {
    "path": "posts/2020-08-04-tidytuesday-2020-week-32/",
    "title": "TidyTuesday 2020 Week 32",
    "description": "A dumbbell chart visualization of energy production trends among European countries",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-08-04",
    "categories": [
      "tidytuesday",
      "data visualization",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nVisualization\r\nThings I learned\r\nThings to improve\r\n\r\nCode\r\n\r\n\r\n\r\n\r\nVisualization\r\n \r\n\r\n\r\n\r\nThings I learned\r\ngeom_dumbbell() from {ggalt}\r\ncoord_capped_cart() and facet_rep_wrap() from {lemon}\r\nUsing the reorder_within() + facet_wrap(scales = \"free_y\") + scale_y_reordered() combo to sort within facets.\r\nUsing override.aes argument to manipulate legend aesthetics after they’re generated by the geom_*()s\r\nUsing slice_max() instead of top_n() to catch up with the new {dplyr} update\r\nThings to improve\r\nFont sizing and image resolution\r\nPlacement and size of legend is sorta awkward.\r\nPlot feels too… empty. I think I treated this too much like a figure for a journal article. Maybe add some background color next time?\r\nCode\r\nAlso available on github\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytuesdayR)\r\nlibrary(lemon)\r\nlibrary(ggalt)\r\nlibrary(patchwork)\r\nlibrary(extrafont)\r\n\r\ntheme_set(theme_classic())\r\n\r\n### Data\r\n\r\ntuesdata <- tidytuesdayR::tt_load('2020-08-04')\r\n\r\nenergy_types <- tuesdata$energy_types\r\n\r\nenergy_types_tidy <- energy_types %>% \r\n  pivot_longer(where(is.double), names_to = \"Year\", values_to = \"GWh\")\r\n\r\n\r\nplot_data <- energy_types_tidy %>% \r\n  add_count(country, Year, wt = GWh, name = \"Total\") %>% \r\n  mutate(GWh_prop = GWh/Total) %>% \r\n  select(-country_name, -GWh, -Total , -level) %>% \r\n  filter(Year %in% c(2016, 2018))\r\n  \r\n\r\n### Plotting\r\n\r\np1 <- plot_data %>% \r\n  filter(type == \"Conventional thermal\") %>% \r\n  pivot_wider(names_from = Year, values_from = GWh_prop) %>% \r\n  mutate(country = fct_reorder(country, `2018`, max, .desc = TRUE)) %>% \r\n  mutate(increase = (`2018` - `2016`) > 0) %>%  \r\n  ggplot() +\r\n  geom_dumbbell(\r\n    aes(y = country, x = `2016`, xend = `2018`, color = increase),\r\n    dot_guide = TRUE, dot_guide_size = 0.25,\r\n    size = 2, colour_x = \"#babfb6\", colour_xend = \"#5f787b\"\r\n  ) +\r\n  scale_color_manual(values = c(\"#d69896\", \"#a1cf86\"), labels = c(\"2016\", \"2018\")) +\r\n  guides(color = guide_legend(override.aes = list(color = c(\"#babfb6\", \"#5f787b\"), size = 3))) +\r\n  labs(title = \"Conventional Thermal Energy\",\r\n       y = \"Country Codes\",\r\n       color = NULL) +\r\n  theme(legend.position = c(.75, .85),\r\n        axis.title.y = element_text(size = 12, vjust = 5))\r\n\r\np2 <- plot_data %>% \r\n  filter(type != \"Conventional thermal\") %>% \r\n  pivot_wider(names_from = Year, values_from = GWh_prop) %>% \r\n  mutate(type = fct_lump(type, n = 3, w = `2018`)) %>% \r\n  group_by(type, country) %>% \r\n  summarize(`2016` = sum(`2016`), `2018` = sum(`2018`)) %>% \r\n  slice_max(`2018`, n = 10, with_ties = FALSE) %>% \r\n  mutate(country = tidytext::reorder_within(country, `2018`, type)) %>% \r\n  mutate(increase = (`2018` - `2016`) > 0) %>%  \r\n  ggplot() +\r\n  geom_dumbbell(\r\n    aes(y = country, x = `2016`, xend = `2018`, color = increase),\r\n    dot_guide = TRUE, dot_guide_size = .4,\r\n    size = 2.5, colour_x = \"#babfb6\", colour_xend = \"#5f787b\",\r\n    show.legend = FALSE\r\n  ) +\r\n  scale_color_manual(values = c(\"#d69896\", \"#a1cf86\")) +\r\n  tidytext::scale_y_reordered() +\r\n  facet_rep_wrap(~type, scales = \"free_y\") +\r\n  labs(title = \"Clean Energy\",\r\n       subtitle = \"Leaders in Each Category (Top 10)\",\r\n       y = NULL)\r\n\r\npatched <- p1 + p2 &\r\n    coord_capped_cart(bottom = \"both\") &\r\n    scale_x_continuous(labels = scales::percent) &\r\n    labs(x = NULL) &\r\n    theme(\r\n      plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\r\n      text = element_text(family = \"Montserrat\"),\r\n      panel.grid.major.y = element_blank(),\r\n      plot.margin = unit(c(.4,.2,.2,.4), \"cm\"),\r\n      plot.background = element_rect(color = \"transparent\")\r\n    )\r\n\r\npatched + plot_annotation(title = \"Electricity Production in Europe\",\r\n                  subtitle = \"A comparison between 2016 and 2018\",\r\n                  caption = \"Percent Accounting for the Country's Total Electricity Generated\",\r\n                  theme = list(plot.title = element_text(size = 22),\r\n                               plot.subtitle = element_text(face = \"italic\", hjust = .5),\r\n                               plot.caption = element_text(size = 12, hjust = .5)))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-04-tidytuesday-2020-week-32/preview.png",
    "last_modified": "2020-11-02T09:46:26+09:00",
    "input_file": {},
    "preview_width": 1444,
    "preview_height": 805
  },
  {
    "path": "posts/2020-07-29-six-years-of-my-spotify-playlists/",
    "title": "Six years of my Spotify playlists",
    "description": "An analysis of acoustic features with {spotifyr}",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-07-29",
    "categories": [
      "ggplot2",
      "gganimate",
      "spotifyr",
      "data wrangling",
      "data visualization"
    ],
    "contents": "\r\n\r\nContents\r\nBackground\r\nAnalysis #1 - Size of monthly playlists over time\r\nAnalysis #2 - Audio features\r\nAnalysis #3 - Songs during college\r\nAnalysis #4 - “My Top 100” playlists\r\nConclusion\r\n\r\n\r\n\r\n\r\nBackground\r\nOne of my longest running habits is making monthly playlists. At the start of every month, I create a new playlist for that month and add songs that I like. Some songs are carried over from the previous month’s playlist and others are songs that I newly discover, but they’re all representative of the songs that I’m “into” for that month.\r\nI’ve been doing this for many years, and have the best record of my monthly playlists for the past 6 years, which is how long I’ve been using spotify. So when I saw people talking about {spotifyr} - an R wrapper for Spotify’s web API - on twitter, I decided to take a stab at analyzing my monthly playlists (code here).\r\n\r\n\r\n\r\n\r\nAnalysis #1 - Size of monthly playlists over time\r\nWhen I first pulled information about my Spotify account, I noticed that I had some gaps in my monthly playlists. This was a special case of non-random missing data: when I didn’t make a new playlist for the month, it’s because I didn’t think that there was a substantial change in what I’m jamming to from the previous month. The {zoo} package, which I didn’t know about before, came in very handy here for dealing with this missingness with its na.locf() (Last Observation Carried Forward) function.\r\nAfter some more cleaning steps, I first made a simple plot that counted the number of songs that each monthly playlist had.\r\n\r\nThere are some interesting things I notice, and here’s some context for them.\r\n2014, the year I began using Spotify, has high number of songs per playlist. This makes sense because the transition period involved a lot of transferring-over-in-bulk of my favorite music from iTunes (which I used previously).\r\nThe three consecutive months of missing playlists in 2017 was over a summer camp. I had forgotten about that until I started this analysis, but for that summer I just kept one playlist called “Summer 2017.”\r\nBoth the playlist with the most songs (78) and the playlist with the least songs (1) are in 2017. That was the year when I was exposed to a lot with different music genres from going to parties my friends. That was also my Sophomore year, which was a hot mess, but that’s a story for another time. You can get a better sense of my music taste being all over the place in the next section.\r\nMay 2017 was the month of sad azn vibes. My playlist had a single song, which was American Girl by Mitski and I think that says everythihng.\r\nIn November 2017, the month with the largest playlist, a friend who was known for having a great taste in music shared with me their playlist of 200+ EDM songs. I never really got into EDM previously but that was a gamechanger. I kind of went through an EDM phase for a short while after that, and November 2017 playlist is still the playlist I go back to when I want to listen to some good EDM.\r\nThe missing playlist of March 2020 was at perhaps the busiest time of my life - I was juggling my thesis, finals, grad school visits/decisions, finishing up my graduation requirements, moving out of campus back to Korea, and, of course, dealing with the pandemic. Makes sense why that month is missing its playlist.\r\nAnalysis #2 - Audio features\r\nThe real deal of Spotify API is actually the audio features, which Spotify calculates using their special algorithms. Some of the features are listed in the table below (adopted from the documentation). Of these, I decided to narrow down to acousticness, danceability, energy, and valence because others didn’t really show much variation (e.g., I don’t listen to live-recorded music on Spotify, so liveness is always near zero).\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#yjhfkriyxi .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#yjhfkriyxi .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#yjhfkriyxi .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#yjhfkriyxi .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#yjhfkriyxi .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#yjhfkriyxi .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#yjhfkriyxi .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#yjhfkriyxi .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#yjhfkriyxi .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#yjhfkriyxi .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#yjhfkriyxi .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#yjhfkriyxi .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#yjhfkriyxi .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#yjhfkriyxi .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#yjhfkriyxi .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#yjhfkriyxi .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#yjhfkriyxi .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#yjhfkriyxi .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#yjhfkriyxi .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#yjhfkriyxi .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#yjhfkriyxi .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#yjhfkriyxi .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#yjhfkriyxi .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#yjhfkriyxi .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#yjhfkriyxi .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#yjhfkriyxi .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#yjhfkriyxi .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nFeature\r\n      Description\r\n    acousticness\r\n      A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\r\n    danceability\r\n      Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\r\n    energy\r\n      Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\r\n    instrumentalness\r\n      Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\r\n    liveness\r\n      Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\r\n    loudness\r\n      The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\r\n    speechiness\r\n      Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\r\n    tempo\r\n      The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\r\n    valence\r\n      A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\r\n    \r\n\r\nI was interested in looking at how my music taste changed over time, so for each monthly playlist, I calculated the mean values for these four features and made a line plot:\r\n\r\nSome things that pop out:\r\nOverall, I tend to listen to bright music (high dancibility and energy) but there is still a lot of variation (especially in valence).\r\nAcousticness stays pretty low and doesn’t seem to correlate much with time. This makes sense because I primarily listen to pop and electric songs, which are both very far from sounding acoustic.\r\nThe one dip in May 2017 is, you guessed it, that playlist with just that one Mitski song. That was the only playlist where the mean danceability and energy values are below 0.5.\r\nIt also looks like I listen to brighter music with every passing year, though this trend is subtle. This it’s a bit easier to see in the animation below (especially for energy and valence).\r\n\r\nAnalysis #3 - Songs during college\r\nNext, I wanted to focus on my years in college, from Fall 2016 to Spring 2020. For this analysis, I defined time in terms of school years and quarters. While I was a college student, I often felt like the passage of time was defined in terms of quarters, so this scale felt appropriate.\r\nHere is the same line plot, except the feature values are averaged by quarter instead of month, and the plot is now faceted by school year:\r\n\r\nObservations:\r\nThe trend in increasing positivity can be observed in this plot as well (with my Junior year coming out on top).\r\nI was actually wondering whether I’d see a pattern by quarter, but there doesn’t seem to be any strong ones.\r\nAnalysis #4 - “My Top 100” playlists\r\nIn my last analysis, I move from my monthly playlists to the end-of-the-year playlists that Spotify makes for you every year.\r\nFor this, I grabbed audio features of songs in my yearly top 100 playlists from 2016-2019. In this graph, each line represents a song and the top 10 most listened to song of each year are emphasized in black. The thick red line in each panel represents the average of the songs for that year.\r\n\r\nSome observations on the variation in audio features among my top 100 playlists:\r\nAlthough the monthly averages in the previous graphs showed extremely low acousticness values (never going above 0.5), that actually hid a lot of variation. You can see that a good number of songs with high acousticness make it to my top 100 (and sometimes even top 10) songs, but that number seems to gradually decline over time. The song with highest acousticness, Ripchord by Rilo Kiley comes from my 2016 playlist. Rilo Kiley is an indie rock band that had a strong influence on my music taste during high school, so it’s no surprise that this song made it there.\r\nThere’s also a trend of decreasing overall variability in the audio features of my top 100 songs. Perhaps this means that I’m narrowing in on my true music taste? Or maybe that I’m going through a phase in music taste? It’s kind of hard to tell but interesting nonetheless.\r\nConclusion\r\nI didn’t really dig too deeply into the acoustic profile of the songs I listen to in this post, and I doubt that Spotify’s list of audio features are comprehensive enough to describe my music taste, but this was a cool exercise!\r\nAnd although I ignored several of the audio features because they weren’t very informative for the songs I listen to, I thought I should at least leave a summary table showing the mean values for all features that I gave in the table above!\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#xmhaddpekd .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#xmhaddpekd .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#xmhaddpekd .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#xmhaddpekd .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#xmhaddpekd .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#xmhaddpekd .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#xmhaddpekd .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#xmhaddpekd .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#xmhaddpekd .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#xmhaddpekd .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#xmhaddpekd .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#xmhaddpekd .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#xmhaddpekd .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#xmhaddpekd .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#xmhaddpekd .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#xmhaddpekd .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#xmhaddpekd .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#xmhaddpekd .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#xmhaddpekd .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#xmhaddpekd .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#xmhaddpekd .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#xmhaddpekd .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#xmhaddpekd .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#xmhaddpekd .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#xmhaddpekd .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#xmhaddpekd .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#xmhaddpekd .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nFeature\r\n      Description\r\n      Average\r\n    acousticness\r\n      A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\r\n      0.16\r\n    danceability\r\n      Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\r\n      0.64\r\n    energy\r\n      Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\r\n      0.75\r\n    instrumentalness\r\n      Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\r\n      0.02\r\n    liveness\r\n      Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\r\n      0.19\r\n    loudness\r\n      The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\r\n      -4.95\r\n    speechiness\r\n      Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\r\n      0.08\r\n    tempo\r\n      The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\r\n      120.25\r\n    valence\r\n      A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\r\n      0.55\r\n    \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-29-six-years-of-my-spotify-playlists/preview.png",
    "last_modified": "2020-11-02T09:47:08+09:00",
    "input_file": {},
    "preview_width": 3593,
    "preview_height": 2459
  },
  {
    "path": "posts/2020-07-20-shiny-tips-1/",
    "title": "Shiny tips - the first set",
    "description": "%||%, imap() + {shinybusy}, and user inputs in modalDialog()",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-07-20",
    "categories": [
      "shiny"
    ],
    "contents": "\r\n\r\n\r\n\r\nWhen I was an RA in the LEARN lab - a child language development lab at Northwestern - I worked on a shiny app that automates snowball search for meta-analysis research (relevant research poster). Long story short, I worked on it for a couple months, got it working, then stopped working on it for another couple months, and had the chance to revisit it just recently.\r\nWhen I picked the project back up, I realized that my old code was poorly commented, somewhat inefficient, and even hackish at times. So I decided to re-write it from scratch. In this second time around, I learned a lot of useful functions/tricks that really helped streamline my code and I thought I’d document my three favorite ones here for future reference.\r\n1. %||% from {rlang}\r\nBasically, %||% is an infix operator that returns the left-hand side when it is not Null and the right-hand side when it is Null. It’s from the {rlang} package but you can also define the function yourself:\r\n\r\n\r\n\"%||%\" <- function(lhs, rhs) {\r\n  if (is.null(lhs)) rhs else lhs\r\n}\r\n\r\na <- 10\r\nb <- NULL\r\n\r\na %||% b\r\n\r\n\r\n  [1] 10\r\n\r\nb %||% a\r\n\r\n\r\n  [1] 10\r\n\r\n# note how the output is different when b is no longer null\r\nb <- 11\r\nb %||% a\r\n\r\n\r\n  [1] 11\r\n\r\nI found this operator to be extremely useful when displaying empty tables as placeholders when using DT::datatable(). It allows me to communicate to the user where a table is expected to appear rather than just not showing anything at all when no data is loaded (which is what happens by default).\r\nFor example, if you want to show an empty column with an empty row when the data (here, the reactive variable mydf) is null, you might do the following:\r\n\r\n\r\nmydf_display <- renderDataTable({\r\n  datatable(mydf() %||% tibble(` ` = NA))\r\n})\r\n\r\n\r\n\r\nAnother use-case for %||% is when I’m trying a sequence of function calls until one one of them succeeds and returns a non-null value. For example, say I want to scrape some information online and I have API wrappers for different websites that potentially have that information. I can chain them together using %||% like so:\r\n\r\n\r\nmyinfo <- \r\n  scrape_website1() %||%\r\n  scrape_website2() %||%\r\n  scrape_website3()\r\n\r\n\r\n\r\nThis is much neater than nested if…else statements!\r\n2. purrr::imap() and {shinybusy}\r\nUsing my shiny app involves a lot of waiting (querying online databases), so I looked into ways to show a progress bar similar to the family of *Modal() functions from {shiny}. The extension package {shinybusy} (project site) offers a very satisfying solution to this problem.\r\nBasically, you initialize a progress bar with show_modal_progress_*() and increment its value inside whatever operation you’re doing. Here’s a pseudo code demonstrating how it works:\r\n\r\ninitialize a progress bar\r\n\r\ncreate a new_list of same size to store output\r\n\r\nfor index in seq_along(list):\r\n  new_list[index] <- calculations(list[index])\r\n  increment progress bar by index\r\n  \r\nremove progress bar\r\n\r\nreturn new_list\r\n\r\nBut in my case, my “do stuff” part didn’t involve a big wall of code because I packed it into a single function in a separate file that I source at the beginning. This, coupled with my general aversion to for-loops, drove me to imap() and its variants from {purrr}. imap() is like map() except it also keeps track of the index of the element that you’re operating on (to put it another way, it’s like map2() where .y is the index).\r\nNow, you don’t need an explicit for-loop to increment and the above code can be reduced to this:\r\n\r\ninitialize a progress bar\r\n\r\nnew_list <- imap(list,\r\n                 ~{\r\n                   calculations(.x)\r\n                   increment progress bar by .y\r\n                 })\r\n  \r\nremove progress bar\r\n\r\nreturn new_list\r\n\r\nIn my opinion, this is much cleaner! For a more concrete example, here’s a template using actual code:\r\n\r\n\r\nmy_data <- eventReactive(input$my_button, {\r\n  \r\n  # initialize a progress bar\r\n  show_modal_progress_line()\r\n  \r\n  # do operation on elements of vector\r\n  result <- imap(my_reactive_var(),\r\n                 ~{\r\n                   update_modal_progress(value = .y / length(my_reactive_var()))\r\n                   my_operation_on_element(.x)\r\n                 })\r\n  \r\n  # remove progress bar\r\n  remove_modal_progress()\r\n  \r\n  # return output\r\n  return(result)\r\n  \r\n})\r\n\r\n\r\n\r\n3. User inputs inside modalDialog()\r\nIn {shiny}, you can show the user a pop-up message box by first laying out the content of the message in modalDialog() and then rendering it with showModal(). In the first version of my app, I used this to show simple messages like warnings, but did you know that you can include any *Input widgets too?\r\nFor example, this code renders a pop-up box for a file upload in response to a button click:\r\n\r\n\r\nobserveEvent(input$MyButton, {\r\n  showModal(modalDialogue(\r\n    title = \"Upload File Here\",\r\n    fileInput(inputID = \"UploadedFile\", label = \"Upload\")\r\n  ))\r\n})\r\n\r\n\r\n\r\nAnd you can access whatever is uploaded using input$UploadedFile like you would if the file upload widget was in the ui side of the app!\r\nThis took me a bit to get used to because you are defining the modal in the server side where the content of the modal looks like the ui side but can be accessed back at the server side. But this was life-changing and it opened up a lot of potential for my GUI to be less cluttered. Using this neat trick, I was able to move a large feature into a modal that would only be available upon a click of a button (it was a feature designed for a rare case scenario so I thought I’d save the user from having to see the entire interface for that if they don’t ask for it).\r\nEnding note\r\nThe more I learn and use shiny, the less I feel like I know. I’m actually enjoying this stage of my progress because every new thing just absolutely wows me (and I hope to continue sharing what I learn - hence this being the “first set”). And very much looking forward to Hadley Wickham’s new book on shiny!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-20-shiny-tips-1/preview.png",
    "last_modified": "2020-11-02T09:47:55+09:00",
    "input_file": {},
    "preview_width": 746,
    "preview_height": 133
  },
  {
    "path": "posts/2020-07-13-geom-paired-raincloud/",
    "title": "geom_paired_raincloud()",
    "description": "A {ggplot2} geom for visualizing change in distribution between two conditions.",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-07-13",
    "categories": [
      "data visualization",
      "ggplot2"
    ],
    "contents": "\r\n\r\n\r\n\r\nRaincloud Plots\r\n\r\n\r\n\r\nGeoms for rainplots (a.k.a. split violin plots) already exist, but you might have a very special case where you have pairs of rainplots and you want to track the change in individual datapoints between the rainplot distributions.\r\nFor example, say you want to track the height of a plant species across two timepoints and you want to communicate three information:\r\nThe change in the distribution of plant heights between timepoints.\r\nThe individual variation in height (“intercept”).\r\nThe individual variation in change of height between timepoints (“slope”).\r\nAnd the data looks like this:\r\n\r\n\r\nset.seed(1234)\r\nplants <- tibble(Species = \"Dwarf\",\r\n                 Plant = rep(factor(1:100), 2),\r\n                 Timepoint = rep(c(\"First\", \"Second\"), each = 100),\r\n                 Height = c(rnorm(100, 10, 5), rnorm(100, 20, 8)))\r\n\r\nplants %>% \r\n  group_by(Timepoint) %>% \r\n  summarize(across(Height, list(mean = mean, sd = sd), .names = \"{col}_{fn}\"))\r\n\r\n\r\n  # A tibble: 2 x 3\r\n    Timepoint Height_mean Height_sd\r\n    <chr>           <dbl>     <dbl>\r\n  1 First            9.22      5.02\r\n  2 Second          20.3       8.26\r\n\r\nYou can use geom_violhalf() from the {see} package to do this:\r\n\r\n\r\nlibrary(see)\r\nggplot(plants, aes(Timepoint, Height, fill = Timepoint)) +\r\n  geom_violinhalf() +\r\n  geom_point(aes(group = Plant),\r\n             position = position_nudge(-.05),\r\n             alpha = 0.5, shape = 16) +\r\n  geom_line(aes(group = Plant),\r\n            position = position_nudge(-.05))\r\n\r\n\r\n\r\n\r\nBut it’d look better if the lines don’t cross over the raincloud for the first timepoint.\r\ngeom_paired_raincloud() automatically flips the first raincloud for you! You do get a warining that there are overlapping points, but that’s because the x-axis is categorical and {ggplot2} thinks that flipping the raincloud intrudes into a different category. AFAIK you don’t lose any data despite this warning, but you should double check to be sure.\r\n\r\n\r\ndevtools::source_url(\"https://raw.githubusercontent.com/yjunechoe/geom_paired_raincloud/master/geom_paired_raincloud.R\")\r\n\r\nggplot(plants, aes(Timepoint, Height, fill = Timepoint)) +\r\n  geom_paired_raincloud()\r\n\r\n\r\n  Warning: position_dodge requires non-overlapping x intervals\r\n\r\n\r\nWe can add individual points and lines onto this plot in a similar way, except you need to use a 2-length vector for position_dodge().\r\n\r\n\r\nplants %>% \r\n  # arrange by individual plant\r\n  arrange(Plant) %>% \r\n  ggplot(aes(Timepoint, Height, fill = Timepoint)) +\r\n  geom_paired_raincloud() +\r\n  geom_point(aes(group = Plant),\r\n             position = position_nudge(c(.05, -.05)),\r\n             alpha = 0.5, shape = 16,\r\n             show.legend = FALSE) +\r\n  geom_line(aes(group = Plant),\r\n            position = position_nudge(c(.05, -.05)))\r\n\r\n\r\n\r\n\r\nNOTE: you need to make sure that the data is arranged by the variable you’re using for the group aesthetic (in this case, Plant) before being passed into ggplot() for position_nudge() in the other geoms to work properly (sorry it’s a bit hacky):\r\ngeom_paired_raincloud works as long as the grouping is of length two (i.e., as long as you’re comparing distribution between two levels).\r\nLet’s modify the plants dataset to include another species of plant:\r\n\r\n\r\nplants2 <- plants %>% \r\n  bind_rows(\r\n    tibble(Species = \"Giant\",\r\n           Plant = rep(factor(101:200), 2),\r\n           Timepoint = rep(c(\"First\", \"Second\"), each = 100),\r\n           Height = c(rnorm(100, 30, 5), rnorm(100, 50, 8)))\r\n  )\r\n\r\nplants2 %>% \r\n  group_by(Species, Timepoint) %>% \r\n  summarize(across(Height, list(mean = mean, sd = sd), .names = \"{col}_{fn}\"))\r\n\r\n\r\n  # A tibble: 4 x 4\r\n  # Groups:   Species [2]\r\n    Species Timepoint Height_mean Height_sd\r\n    <chr>   <chr>           <dbl>     <dbl>\r\n  1 Dwarf   First            9.22      5.02\r\n  2 Dwarf   Second          20.3       8.26\r\n  3 Giant   First           30.8       4.80\r\n  4 Giant   Second          49.9       8.40\r\n\r\nIn this new plot, I just added facet_wrap(~Species)\r\n\r\n\r\nplants2 %>% \r\n  arrange(Plant) %>% \r\n  ggplot(aes(Timepoint, Height, fill = Timepoint)) +\r\n  geom_paired_raincloud() +\r\n  geom_point(aes(group = Plant),\r\n             position = position_nudge(c(.05, -.05)),\r\n             alpha = 0.5, shape = 16,\r\n             show.legend = FALSE) +\r\n  geom_line(aes(group = Plant),\r\n            position = position_nudge(c(.05, -.05))) +\r\n  facet_wrap(~Species)\r\n\r\n\r\n\r\n\r\ngeom_paired_raincloud() isn’t particularly useful for plotting comparisons between more than two levels, so it throws a warning when that’s the case:\r\n\r\n\r\n# Adding a third timepoint\r\nplants3 <- plants %>% \r\n  bind_rows(tibble(Species = \"Dwarf\",\r\n                   Plant = factor(1:100),\r\n                   Timepoint = \"Third\",\r\n                   Height = rnorm(100, 40, 10)))\r\n\r\nplants3 %>% \r\n  group_by(Timepoint) %>% \r\n  summarize(across(Height, list(mean = mean, sd = sd), .names = \"{col}_{fn}\"))\r\n\r\n\r\n  # A tibble: 3 x 3\r\n    Timepoint Height_mean Height_sd\r\n    <chr>           <dbl>     <dbl>\r\n  1 First            9.22      5.02\r\n  2 Second          20.3       8.26\r\n  3 Third           39.8      11.2\r\n\r\n\r\n\r\nplants3 %>% \r\n  arrange(Plant) %>% \r\n  ggplot(aes(Timepoint, Height, fill = Timepoint)) +\r\n  geom_paired_raincloud() +\r\n  geom_point(aes(group = Plant),\r\n             position = position_nudge(c(.05, -.05)),\r\n             alpha = 0.5, shape = 16,\r\n             show.legend = FALSE) +\r\n  geom_line(aes(group = Plant),\r\n            position = position_nudge(c(.05, -.05)))\r\n\r\n\r\n\r\n\r\nBut I think geom_paired_raincloud() works great if you have the right data. Here’s an example from my recent work, looking at the variation in how subjects respond to stimuli when they’re presented in one condition (Subject Accent) compared to the other (Verb Accent).\r\n\r\n\r\n\r\nThe above plot is a combination of geom_paired_raincloud(), geom_point(), geom_line(), and geom_boxplot(). I like that you can employ all these aesthetics at once without making the plot too overwhelming. I’ve included the important part of the code here and the full code is available at the github repo for this research project.\r\n\r\n\r\nrainplot_data %>% \r\n  ggplot(aes(x = Cond, y = z_RT, fill = Cond)) +\r\n  geom_paired_raincloud(alpha = .5) +\r\n  geom_point(aes(group = Item),\r\n             position = position_nudge(c(.15, -.15)),\r\n             alpha = .5, shape = 16) +\r\n  geom_line(aes(group = Item),\r\n            position = position_nudge(c(.13, -.13)),\r\n            linetype = 3) +\r\n  geom_boxplot(position = position_nudge(c(.07, -.07)),\r\n               alpha = .5, width = .04, outlier.shape = \" \") +\r\n  facet_wrap(~Type, scales = \"free_x\") +\r\n  ...\r\n\r\n\r\n\r\n\r\nNotice how I use position_nudge() to make sure that the points, lines, and boxplots are side-by-side and not overlapping with each other.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-13-geom-paired-raincloud/preview.png",
    "last_modified": "2020-11-02T09:48:46+09:00",
    "input_file": {},
    "preview_width": 7086,
    "preview_height": 4251
  },
  {
    "path": "posts/2020-06-30-treemap-with-ggplot/",
    "title": "Plotting treemaps with {treemap} and {ggplot2}",
    "description": "Using underlying plot data for maximum customization",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-06-30",
    "categories": [
      "data visualization",
      "treemap",
      "ggplot2",
      "tutorial"
    ],
    "contents": "\r\n\r\n\r\n\r\nTo steal the definition from Wikipedia, a treemap is used for “displaying hierarchical data using nested figures, usually rectangles.” There are lots of ways to make one in R, but I didn’t find any one existing solution appealing.\r\nFor illustration, let’s take the pokemon dataset from {highcharter} and plot a treemap with it using different methods.\r\n\r\n\r\n\r\n\r\n\r\ndata(\"pokemon\", package = \"highcharter\")\r\n\r\n# Cleaning up data for a treemap\r\ndata <- pokemon %>% \r\n  select(pokemon, type_1, type_2, color_f) %>%\r\n  mutate(type_2 = ifelse(is.na(type_2), paste(\"only\", type_1), type_2)) %>% \r\n  group_by(type_1, type_2, color_f) %>% \r\n  count(type_1, type_2) %>% \r\n  ungroup()\r\n\r\nhead(data, 5)\r\n\r\n\r\n\r\ntype_1\r\n\r\n\r\ntype_2\r\n\r\n\r\ncolor_f\r\n\r\n\r\nn\r\n\r\n\r\nbug\r\n\r\n\r\nelectric\r\n\r\n\r\n#BBBD23\r\n\r\n\r\n2\r\n\r\n\r\nbug\r\n\r\n\r\nfighting\r\n\r\n\r\n#AD9721\r\n\r\n\r\n1\r\n\r\n\r\nbug\r\n\r\n\r\nfire\r\n\r\n\r\n#B9AA23\r\n\r\n\r\n2\r\n\r\n\r\nbug\r\n\r\n\r\nflying\r\n\r\n\r\n#A8AE52\r\n\r\n\r\n13\r\n\r\n\r\nbug\r\n\r\n\r\nghost\r\n\r\n\r\n#9AA03D\r\n\r\n\r\n1\r\n\r\n\r\n \r\n1. {treemap}\r\nHere’s a plot made from the {treemap} package:\r\n\r\n\r\nlibrary(treemap)\r\n\r\ntreemap(dtf = data,\r\n        index = c(\"type_1\", \"type_2\"),\r\n        vSize = \"n\",\r\n        vColor = \"type_1\")\r\n\r\n\r\n\r\n\r\nIt actually doesn’t look too bad, but this package hasn’t been updated for 3 years and there aren’t a lot of options for customization. For the options that do exist, they’re a big list of additional arguments to the main workhorse function, treemap(), which feels a bit restrictive if you’re used to {ggplot}’s modular and layered grammar. So while it’s very simple to use, I’d probably use it only for exploring the data for myself.\r\n \r\n2. {highcharter}\r\nAll the way on the other side of this ease<—>customizability spectrum is {highcharter} which is arguably the most powerful data visualization package in R.\r\nWith highcharter, you can turn the previous graph into the following:\r\n\r\nThis looks much better, and it’s even interactive (although this particular one isn’t because I just copy pasted the image from this blog post from 2018). I’d use {highcharter} except that there isn’t a great documentation on plotting treemaps, and it definitely doesn’t help that {highcharter} has a pretty steep learning curve, even if you have a lot of experience with {ggplot2}.\r\nThe main problem I ran into is that the function hc_add_series_treemap() that was used to create the above graph is now depreciated. It redirects you to use hctreemap() which itself is also depreciated. That finally redirects you to use hctreemap2() which is pretty sparse in documentation and use-cases, and overall not very transparent IMO.\r\n \r\n3. {treemapify}\r\n{treemapify} is a ggplot solution to plotting treemaps.\r\nHere’s a plot of the pokemon dataset, adopting the example code from the vignette. Since it follows the layered grammar of ggplot, I figured I’d show what each of the four layers outlined in the code does:\r\n\r\n\r\nlibrary(treemapify)\r\n\r\nggplot(data, aes(area = n, fill = color_f, label = type_2,\r\n                subgroup = type_1)) +\r\n  # 1. Draw type_2 borders and fill colors\r\n  geom_treemap() +\r\n  # 2. Draw type_1 borders\r\n  geom_treemap_subgroup_border() +\r\n  # 3. Print type_1 text\r\n  geom_treemap_subgroup_text(place = \"centre\", grow = T, alpha = 0.5, colour = \"black\",\r\n                             fontface = \"italic\", min.size = 0) +\r\n  # 4. Print type_2 text\r\n  geom_treemap_text(colour = \"white\", place = \"topleft\", reflow = T) +\r\n  theme(legend.position = 0)\r\n\r\n\r\n\r\ngeom_treemap() draws type_2 borders and fill colors\r\n\r\n\r\n\r\ngeom_treemap_subgroup_border() draws type_1 borders\r\n\r\n\r\n\r\ngeom_treemap_subgroup_text() prints type_1 text\r\n\r\n\r\n\r\ngeom_treemap_text() prints type_2 text\r\n\r\n\r\n\r\nI find this the most appealing out of the three options and I do recommend this package, but I’m personally a bit hesistant to use it for three reasons:\r\nI don’t want to learn a whole ’nother family of geom_*s just to plot treemaps.\r\nSome of the ggplot “add-ons” that I like don’t really transfer over. For example, I can’t use geom_text_repel() from {ggrepel} because I have to use {treemapify}’s own text geoms like geom_treemap_subgroup_text() and geom_treemap_text().\r\nCustomization options are kind of a mouthful, and I’ve yet to see a nice-looking treemap that was plotted using this package. There are a couple example treemaps in the vignette but none of them look particularly good. An independently produced example here doesn’t look super great either.\r\n \r\nA Mixed (Hack-ish?) Solution\r\nBasically, I’m very lazy and I want to avoid learning any new packages or functions as much as possible.\r\nI’ve come up with a very simple solution to my self-created problem, which is to draw treemaps using geom_rect() with a little help from the {treemap} package introduced earlier.\r\nSo apparently, there’s a cool feature in treemap::treemap() where you can extract the plotting data.\r\nYou can do this by pulling the tm object from the plot function side-effect, and the underlying dataframe used for plotting looks like this.1:\r\n\r\n\r\ntm <- treemap(\r\n  dtf = data,\r\n  index = c(\"type_1\", \"type_2\"),\r\n  vSize = \"n\",\r\n  vColor = \"color_f\",\r\n  type = 'color' # {treemap}'s equivalent of scale_fill_identity()\r\n)\r\n\r\n\r\n\r\nhead(tm$tm)\r\n\r\n\r\n\r\ntype_1\r\n\r\n\r\ntype_2\r\n\r\n\r\nvSize\r\n\r\n\r\nvColor\r\n\r\n\r\nstdErr\r\n\r\n\r\nvColorValue\r\n\r\n\r\nlevel\r\n\r\n\r\nx0\r\n\r\n\r\ny0\r\n\r\n\r\nw\r\n\r\n\r\nh\r\n\r\n\r\ncolor\r\n\r\n\r\nbug\r\n\r\n\r\nelectric\r\n\r\n\r\n2\r\n\r\n\r\n#BBBD23\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\n0.4556639\r\n\r\n\r\n0.3501299\r\n\r\n\r\n0.0319174\r\n\r\n\r\n0.0872727\r\n\r\n\r\n#BBBD23\r\n\r\n\r\nbug\r\n\r\n\r\nfighting\r\n\r\n\r\n1\r\n\r\n\r\n#AD9721\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\n0.4556639\r\n\r\n\r\n0.3064935\r\n\r\n\r\n0.0319174\r\n\r\n\r\n0.0436364\r\n\r\n\r\n#AD9721\r\n\r\n\r\nbug\r\n\r\n\r\nfire\r\n\r\n\r\n2\r\n\r\n\r\n#B9AA23\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\n0.4875812\r\n\r\n\r\n0.3501299\r\n\r\n\r\n0.0319174\r\n\r\n\r\n0.0872727\r\n\r\n\r\n#B9AA23\r\n\r\n\r\nbug\r\n\r\n\r\nflying\r\n\r\n\r\n13\r\n\r\n\r\n#A8AE52\r\n\r\n\r\n13\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\n0.2757660\r\n\r\n\r\n0.2628571\r\n\r\n\r\n0.1160631\r\n\r\n\r\n0.1560000\r\n\r\n\r\n#A8AE52\r\n\r\n\r\nbug\r\n\r\n\r\nghost\r\n\r\n\r\n1\r\n\r\n\r\n#9AA03D\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\n0.4556639\r\n\r\n\r\n0.2628571\r\n\r\n\r\n0.0319174\r\n\r\n\r\n0.0436364\r\n\r\n\r\n#9AA03D\r\n\r\n\r\nbug\r\n\r\n\r\ngrass\r\n\r\n\r\n6\r\n\r\n\r\n#9CBB2B\r\n\r\n\r\n6\r\n\r\n\r\nNA\r\n\r\n\r\n2\r\n\r\n\r\n0.4744388\r\n\r\n\r\n0.4374026\r\n\r\n\r\n0.0450598\r\n\r\n\r\n0.1854545\r\n\r\n\r\n#9CBB2B\r\n\r\n\r\nWe can simply use this data to recreate the treemap that was made with {treemapify} - except this time we have more flexibility!\r\nFirst, we do some data cleaning:\r\n\r\n\r\ntm_plot_data <- tm$tm %>% \r\n  # calculate end coordinates with height and width\r\n  mutate(x1 = x0 + w,\r\n         y1 = y0 + h) %>% \r\n  # get center coordinates for labels\r\n  mutate(x = (x0+x1)/2,\r\n         y = (y0+y1)/2) %>% \r\n  # mark primary groupings and set boundary thickness\r\n  mutate(primary_group = ifelse(is.na(type_2), 1.2, .5)) %>% \r\n  # remove colors from primary groupings (since secondary is already colored)\r\n  mutate(color = ifelse(is.na(type_2), NA, color))\r\n\r\n\r\n\r\nThen we plot. It looks like I can recreate a lot of it with a little help from the {ggfittext} package that was in the source code2:\r\n\r\n\r\nggplot(tm_plot_data, aes(xmin = x0, ymin = y0, xmax = x1, ymax = y1)) + \r\n  # add fill and borders for groups and subgroups\r\n  geom_rect(aes(fill = color, size = primary_group),\r\n            show.legend = FALSE, color = \"black\", alpha = .3) +\r\n  scale_fill_identity() +\r\n  # set thicker lines for group borders\r\n  scale_size(range = range(tm_plot_data$primary_group)) +\r\n  # add labels\r\n  ggfittext::geom_fit_text(aes(label = type_2), min.size = 1) +\r\n  # options\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  scale_y_continuous(expand = c(0, 0)) +\r\n  theme_void()\r\n\r\n\r\n\r\n\r\nNow, I can be a lot more flexible with my customizations.\r\nFor example, let’s say I wanted to isolate and emphasize the secondary types that have unique type-combinations with steel, AND also provide the name of the corresponding pokemon.\r\nI can do this by using geom_text_repel() for a subset of the labels while keeping the same geom_fit_text() setting for the rest of the labels.\r\n\r\n\r\ntm_plot_data %>% \r\n  ggplot(aes(xmin = x0, ymin = y0, xmax = x1, ymax = y1)) + \r\n  geom_rect(aes(fill = color, size = primary_group),\r\n            show.legend = FALSE, color = \"black\", alpha = .3) +\r\n  scale_fill_identity() +\r\n  scale_size(range = range(tm_plot_data$primary_group)) +\r\n  ggfittext::geom_fit_text(data = filter(tm_plot_data, type_1 != \"steel\" | vSize > 1),\r\n                           aes(label = type_2), min.size = 1) +\r\n  # pick out observations of interest and annotate with geom_text_repel\r\n  ggrepel::geom_text_repel(\r\n    data = filter(tm_plot_data, vSize == 1, type_1 == \"steel\") %>% \r\n      inner_join(pokemon, by = c(\"type_1\", \"type_2\")),\r\n    aes(x = x, y = y, label = glue::glue(\"{type_2} ({pokemon})\")),\r\n    color = \"black\", xlim = c(1.02, NA), size = 4,\r\n    direction = \"y\", vjust = .5, force = 3\r\n  ) +\r\n  # expand x-axis limits to make room for test annotations\r\n  scale_x_continuous(limits = c(0, 1.2), expand = c(0, 0)) +\r\n  scale_y_continuous(expand = c(0, 0)) +\r\n  theme_void()\r\n\r\n\r\n\r\n\r\nAnd that’s our final product! This would’ve been pretty difficult to do with any of the three options I reviewed at the top!\r\ntl;dr - Use treemap() from the {treemap} package to get positions for geom_rect()s and you’re 90% of the way there to plotting a treemap! Apply your favorite styles (especially _text() geoms) from the {ggplot2} ecosystem for finishing touches!\r\n \r\nSession Info\r\n\r\n\r\nsessionInfo()\r\n\r\n\r\n  R version 4.0.3 (2020-10-10)\r\n  Platform: x86_64-w64-mingw32/x64 (64-bit)\r\n  Running under: Windows 10 x64 (build 18363)\r\n  \r\n  Matrix products: default\r\n  \r\n  locale:\r\n  [1] LC_COLLATE=English_United States.1252 \r\n  [2] LC_CTYPE=English_United States.1252   \r\n  [3] LC_MONETARY=English_United States.1252\r\n  [4] LC_NUMERIC=C                          \r\n  [5] LC_TIME=English_United States.1252    \r\n  \r\n  attached base packages:\r\n  [1] stats     graphics  grDevices datasets  utils     methods   base     \r\n  \r\n  other attached packages:\r\n   [1] treemapify_2.5.3 treemap_2.4-2    printr_0.1       forcats_0.5.0   \r\n   [5] stringr_1.4.0    dplyr_1.0.2      purrr_0.3.4      readr_1.4.0     \r\n   [9] tidyr_1.1.2      tibble_3.0.4     ggplot2_3.3.2    tidyverse_1.3.0 \r\n  \r\n  loaded via a namespace (and not attached):\r\n   [1] httr_1.4.2          jsonlite_1.7.1      modelr_0.1.8       \r\n   [4] shiny_1.5.0         assertthat_0.2.1    highr_0.8          \r\n   [7] blob_1.2.1          renv_0.12.0         cellranger_1.1.0   \r\n  [10] ggrepel_0.8.2       yaml_2.2.1          pillar_1.4.6       \r\n  [13] backports_1.1.10    glue_1.4.2          digest_0.6.26      \r\n  [16] RColorBrewer_1.1-2  promises_1.1.1      rvest_0.3.6        \r\n  [19] colorspace_1.4-1    htmltools_0.5.0     httpuv_1.5.4       \r\n  [22] pkgconfig_2.0.3     broom_0.7.2         haven_2.3.1        \r\n  [25] xtable_1.8-4        scales_1.1.1        later_1.1.0.1      \r\n  [28] distill_1.0.1       downlit_0.2.0       generics_0.0.2     \r\n  [31] farver_2.0.3        ellipsis_0.3.1      withr_2.2.0        \r\n  [34] cli_2.1.0           magrittr_1.5.0.9000 crayon_1.3.4       \r\n  [37] readxl_1.3.1        mime_0.9            evaluate_0.14      \r\n  [40] fs_1.5.0            fansi_0.4.1         xml2_1.3.2         \r\n  [43] tools_4.0.3         data.table_1.13.2   hms_0.5.3          \r\n  [46] lifecycle_0.2.0     gridBase_0.4-7      munsell_0.5.0      \r\n  [49] reprex_0.3.0        compiler_4.0.3      rlang_0.4.8        \r\n  [52] grid_4.0.3          gt_0.2.2            rstudioapi_0.11    \r\n  [55] igraph_1.2.6        labeling_0.4.2      rmarkdown_2.5      \r\n  [58] gtable_0.3.0        DBI_1.1.0           R6_2.4.1           \r\n  [61] lubridate_1.7.9     knitr_1.30          fastmap_1.0.1      \r\n  [64] prismatic_0.2.0     stringi_1.5.3       Rcpp_1.0.5         \r\n  [67] vctrs_0.3.4         ggfittext_0.9.0     dbplyr_1.4.4       \r\n  [70] tidyselect_1.1.0    xfun_0.18\r\n\r\n\r\nYou might get a warning referencing something about data.table here. No worries if this happens. The outdated {treemap} source code is built on {data.table} and contains a deprecated argument.↩︎\r\nI highly recommend checking {ggfittext} out! Here’s the github repo. Also, this is more of a note to myself but I had some trouble getting this to work at first because the min.size argument defaults to 4, meaning that all fitted text smaller than size 4 are simply not plotted (so I couldn’t get geom_fit_text() to print anything in my treemap at first). You can compare and see the threshold by looking at the geom_text_repel() texts in my second example which also has a size of 4.↩︎\r\n",
    "preview": "posts/2020-06-30-treemap-with-ggplot/2020-06-30-treemap-with-ggplot_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2020-11-05T12:34:24+09:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 768
  },
  {
    "path": "posts/2020-06-25-indexing-tip-for-spacyr/",
    "title": "Indexing tip for {spacyr}",
    "description": "Speeding up the analysis of dependency relations.",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-06-25",
    "categories": [
      "data wrangling",
      "NLP",
      "spacyr"
    ],
    "contents": "\r\n\r\n\r\n\r\nThe {spacyr} package is an R wrapper for Python’s spaCy package, powered by {reticulate}. Although it’s been around for over 3 years, it doesn’t seem to have really been picked up by R users.1 I actually think this makes sense since what makes spaCy so great is its object-oriented approach to NLP (which Python is good at). But perhaps more importantly, a good portion of data wrangling in spaCy is reducible to operating on vectors of such tokens, and I think that comes pretty naturally for R users with a functional programming background.2 So my guess is that since spaCy is accessible to R users, {spacyr} isn’t that widely used.\r\nBut with that said, I like to make my workflow as R-centered as possible and I think there’s still value in {spacyr} at least for very simple, exploratory analysis of text. The results being returned in a tidy format is a huge plus, and it doesn’t seem to sacrifice much speed.\r\nThere’s a good guide to using {spacyr} in the CRAN vignette which covers pretty much everything you need to know if you’re already familiar with spaCy (and if you aren’t, there’s a great cheatsheet from DataCamp).\r\nEverything I just said above was just a whole lot of background information. What I really want to do here to contribute to the discussion around {spacyr} by sharing a tip for analyzing dependency relations from the output of spacy_parse(), which is {spacyr}’s main function that combines both the model-loading and text-processing stages of spaCy.\r\n\r\n\r\n\r\nFor illustration, I’ll be using the 8 State of the Union addresses by President Barack Obama from 2009-2016, which comes from the {sotu} package.\r\n\r\n\r\nlibrary(sotu)\r\ndoc <- tail(sotu::sotu_text, 8)\r\n\r\n# First 100 characters of each speech\r\nstrtrim(doc, 100)\r\n\r\n\r\n  [1] \"Madam Speaker, Mr. Vice President, Members of Congress, the First Lady of the United States--she's a\"\r\n  [2] \"Madam Speaker, Vice President Biden, Members of Congress, distinguished guests, and fellow Americans\"\r\n  [3] \"Mr. Speaker, Mr. Vice President, Members of Congress, distinguished guests, and fellow Americans: To\"\r\n  [4] \"Mr. Speaker, Mr. Vice President, Members of Congress, distinguished guests, and fellow Americans: La\"\r\n  [5] \"Please, everybody, have a seat. Mr. Speaker, Mr. Vice President, Members of Congress, fellow America\"\r\n  [6] \"The President. Mr. Speaker, Mr. Vice President, Members of Congress, my fellow Americans: Today in A\"\r\n  [7] \"The President. Mr. Speaker, Mr. Vice President, Members of Congress, my fellow Americans: We are 15 \"\r\n  [8] \"Thank you. Mr. Speaker, Mr. Vice President, Members of Congress, my fellow Americans: Tonight marks \"\r\n\r\nWe can pass this document to spacy_parse() to get back a dataframe of tokens and their attributes in tidy format, where each row (observation) is a token.3\r\n\r\n\r\nparsed <- spacy_parse(doc, dep = TRUE, entity = FALSE)\r\n\r\nhead(parsed, 10)\r\n\r\n\r\n\r\ndoc_id\r\n\r\n\r\nsentence_id\r\n\r\n\r\ntoken_id\r\n\r\n\r\ntoken\r\n\r\n\r\nlemma\r\n\r\n\r\npos\r\n\r\n\r\nhead_token_id\r\n\r\n\r\ndep_rel\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nMadam\r\n\r\n\r\nMadam\r\n\r\n\r\nPROPN\r\n\r\n\r\n2\r\n\r\n\r\ncompound\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\nSpeaker\r\n\r\n\r\nSpeaker\r\n\r\n\r\nPROPN\r\n\r\n\r\n2\r\n\r\n\r\nROOT\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\n,\r\n\r\n\r\n,\r\n\r\n\r\nPUNCT\r\n\r\n\r\n2\r\n\r\n\r\npunct\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\nMr. \r\n\r\n\r\nMr. \r\n\r\n\r\nPROPN\r\n\r\n\r\n6\r\n\r\n\r\ncompound\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\nVice\r\n\r\n\r\nVice\r\n\r\n\r\nPROPN\r\n\r\n\r\n6\r\n\r\n\r\ncompound\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\nPresident\r\n\r\n\r\nPresident\r\n\r\n\r\nPROPN\r\n\r\n\r\n2\r\n\r\n\r\nappos\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n7\r\n\r\n\r\n,\r\n\r\n\r\n,\r\n\r\n\r\nPUNCT\r\n\r\n\r\n6\r\n\r\n\r\npunct\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n8\r\n\r\n\r\nMembers\r\n\r\n\r\nMembers\r\n\r\n\r\nPROPN\r\n\r\n\r\n6\r\n\r\n\r\nappos\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n9\r\n\r\n\r\nof\r\n\r\n\r\nof\r\n\r\n\r\nADP\r\n\r\n\r\n8\r\n\r\n\r\nprep\r\n\r\n\r\ntext1\r\n\r\n\r\n1\r\n\r\n\r\n10\r\n\r\n\r\nCongress\r\n\r\n\r\nCongress\r\n\r\n\r\nPROPN\r\n\r\n\r\n9\r\n\r\n\r\npobj\r\n\r\n\r\nThis output format is great for plotting in R with the familiar packages. For example, we can make a bar plot of top adjectives used by Obama in his SOTU addresses with minimal changes to the output.\r\n\r\n\r\n# Load tidytext package for stopwords\r\nlibrary(tidytext)\r\n\r\nparsed %>%\r\n  filter(pos == \"ADJ\",\r\n         str_detect(lemma, \"^[:alpha:].*[:alpha:]$\"),\r\n         !lemma %in% tidytext::stop_words$word) %>%\r\n  count(lemma) %>% \r\n  mutate(lemma = fct_reorder(str_to_title(lemma), n)) %>%\r\n  top_n(15) %>% \r\n  ggplot(aes(lemma, n)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n  labs(title = \"Top 15 Adjectives from President Obama's SOTU Addresses\",\r\n       x = \"Adjective\", y = \"Count\") +\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nThe Challenge\r\nBut what if we want to dig a little deeper in our analysis of adjectives? What if, for example, we were interested in the adjectives that were used to describe “America”\r\nBecause we set dep = TRUE when we called spacy_parse() earlier, we have information about dependencies in the dep_rel column and the head_token_id column. To be more precise, dep_rel is the .dep_ attribute from spaCy and head_token_id is the row index of the head token (.head attribute from spaCy) that is unique to the spacy_parse() output.\r\nFor example, let’s look at the the 298th sentence from Obama’s third SOTU address:\r\n\r\n\r\nexample_sentence <- parsed %>% \r\n  filter(doc_id == \"text3\", sentence_id == 298) %>% \r\n  pull(token) %>% \r\n  str_c(collapse = \" \") %>% \r\n  str_remove_all(\" (?=[:punct:])\")\r\n\r\nexample_sentence\r\n\r\n\r\n  [1] \"Now, we've made great strides over the last 2 years in using technology and getting rid of waste.\"\r\n\r\nAnd here’s a visualization of the dependency parse made with displaCy. Sadly, displaCy is not a part of {spacyr}, so I’m just calling Python here using {reticulate}.\r\n\r\n######## Python Code ########\r\nimport spacy\r\nfrom spacy import displacy\r\nnlp = spacy.load('en_core_web_sm')\r\nexample_parsed = nlp(r.example_sentence)\r\n\r\n\r\ndisplacy.render(example_parsed, style = \"dep\")\r\n\r\n\r\n  .superbigimage{\r\n      overflow-x:scroll;\r\n      white-space: nowrap;\r\n  }\r\n  .superbigimage img{\r\n     max-width: none;\r\n  }\r\n\r\n\r\n‘Now,ADVwePRON'veAUXmadeVERBgreatADJstridesNOUNoverADPtheDETlastADJ2NUMyearsNOUNinADPusingVERBtechnologyNOUNandCCONJgettingVERBridVERBofADPwaste.NOUNadvmodnsubjauxamoddobjprepdetamodnummodpobjpreppcompdobjccauxpassconjpreppobj’\r\n\r\n\r\n \r\nBasically, the task here is to find words like “competitive” in the example sentence above where the token is an adjective and its head is the word “America”, but it turns out harder than it seems.\r\nThe output of spacy_parse is set up such that every sentence stands on their own. More specifically speaking, the indices stored in token_id and head_token_id are local indices relative to each sentence.4 So while there are a total of 62791 tokens in parsed, the max token_id is 99, which is the index of the last token in the longest sentence.\r\nA strictly tidyverse approach (which has become a sort of a tunnel-vision for me) would be to split parsed by sentence and map a filter function to each sentence. There are two ways of going about this and both are pretty slow.\r\nThe first way is to explicitly split the dataframe into a list of dataframes at the sentence level then map the filter function, using group_split() then map_df():\r\n\r\n\r\ntic <- Sys.time()\r\n\r\nparsed %>%\r\n    group_split(doc_id, sentence_id, .drop = FALSE) %>%\r\n    map_df(~filter(., pos == \"ADJ\", slice(.x, head_token_id)$lemma == \"America\"))\r\n\r\n\r\n\r\ndoc_id\r\n\r\n\r\nsentence_id\r\n\r\n\r\ntoken_id\r\n\r\n\r\ntoken\r\n\r\n\r\nlemma\r\n\r\n\r\npos\r\n\r\n\r\nhead_token_id\r\n\r\n\r\ndep_rel\r\n\r\n\r\ntext3\r\n\r\n\r\n302\r\n\r\n\r\n33\r\n\r\n\r\ncompetitive\r\n\r\n\r\ncompetitive\r\n\r\n\r\nADJ\r\n\r\n\r\n34\r\n\r\n\r\namod\r\n\r\n\r\ntext4\r\n\r\n\r\n231\r\n\r\n\r\n33\r\n\r\n\r\nrural\r\n\r\n\r\nrural\r\n\r\n\r\nADJ\r\n\r\n\r\n34\r\n\r\n\r\namod\r\n\r\n\r\ntext5\r\n\r\n\r\n245\r\n\r\n\r\n2\r\n\r\n\r\nstronger\r\n\r\n\r\nstrong\r\n\r\n\r\nADJ\r\n\r\n\r\n3\r\n\r\n\r\namod\r\n\r\n\r\ntext6\r\n\r\n\r\n340\r\n\r\n\r\n18\r\n\r\n\r\nstrong\r\n\r\n\r\nstrong\r\n\r\n\r\nADJ\r\n\r\n\r\n21\r\n\r\n\r\namod\r\n\r\n\r\ntext7\r\n\r\n\r\n317\r\n\r\n\r\n23\r\n\r\n\r\nliberal\r\n\r\n\r\nliberal\r\n\r\n\r\nADJ\r\n\r\n\r\n24\r\n\r\n\r\namod\r\n\r\n\r\ntext7\r\n\r\n\r\n317\r\n\r\n\r\n27\r\n\r\n\r\nconservative\r\n\r\n\r\nconservative\r\n\r\n\r\nADJ\r\n\r\n\r\n28\r\n\r\n\r\namod\r\n\r\n\r\nSys.time() - tic\r\n\r\n\r\n  Time difference of 12.0861 secs\r\n\r\nThe second way is to implicitly declare a grouping by sentence and then map the filter function, using group_by() then group_map():\r\n\r\n\r\ntic <- Sys.time()\r\n\r\nparsed %>%\r\n  group_by(doc_id, sentence_id) %>%\r\n  group_map(~filter(., pos == \"ADJ\", slice(.x, head_token_id)$lemma == \"America\"), .keep = TRUE) %>%\r\n  bind_rows()\r\n\r\n\r\n\r\ndoc_id\r\n\r\n\r\nsentence_id\r\n\r\n\r\ntoken_id\r\n\r\n\r\ntoken\r\n\r\n\r\nlemma\r\n\r\n\r\npos\r\n\r\n\r\nhead_token_id\r\n\r\n\r\ndep_rel\r\n\r\n\r\ntext3\r\n\r\n\r\n302\r\n\r\n\r\n33\r\n\r\n\r\ncompetitive\r\n\r\n\r\ncompetitive\r\n\r\n\r\nADJ\r\n\r\n\r\n34\r\n\r\n\r\namod\r\n\r\n\r\ntext4\r\n\r\n\r\n231\r\n\r\n\r\n33\r\n\r\n\r\nrural\r\n\r\n\r\nrural\r\n\r\n\r\nADJ\r\n\r\n\r\n34\r\n\r\n\r\namod\r\n\r\n\r\ntext5\r\n\r\n\r\n245\r\n\r\n\r\n2\r\n\r\n\r\nstronger\r\n\r\n\r\nstrong\r\n\r\n\r\nADJ\r\n\r\n\r\n3\r\n\r\n\r\namod\r\n\r\n\r\ntext6\r\n\r\n\r\n340\r\n\r\n\r\n18\r\n\r\n\r\nstrong\r\n\r\n\r\nstrong\r\n\r\n\r\nADJ\r\n\r\n\r\n21\r\n\r\n\r\namod\r\n\r\n\r\ntext7\r\n\r\n\r\n317\r\n\r\n\r\n23\r\n\r\n\r\nliberal\r\n\r\n\r\nliberal\r\n\r\n\r\nADJ\r\n\r\n\r\n24\r\n\r\n\r\namod\r\n\r\n\r\ntext7\r\n\r\n\r\n317\r\n\r\n\r\n27\r\n\r\n\r\nconservative\r\n\r\n\r\nconservative\r\n\r\n\r\nADJ\r\n\r\n\r\n28\r\n\r\n\r\namod\r\n\r\n\r\nSys.time() - tic\r\n\r\n\r\n  Time difference of 11.63191 secs\r\n\r\nBoth ways give us the result we want, but it’s significantly slower than what we could quickly and easily do in Python.\r\n\r\n######## Python Code ########\r\ndoc = nlp(' '.join(r.doc))\r\n\r\nimport time\r\ntic = time.time()\r\n\r\n[token.text for token in doc if token.pos_ == \"ADJ\" and token.head.lemma_ == \"America\"]\r\n  ['competitive', 'rural', 'stronger', 'strong', 'liberal', 'conservative']\r\ntime.time() - tic\r\n  0.04711294174194336\r\n\r\nA Work-around\r\nWhat would really help here is if we had global indices for tokens and head tokens, so that we can directly index a head from a token without going through the trouble of figuring out how sentences are organized in the dataframe.\r\nSo here’s my take on doing this:\r\n\r\n\r\n# Calculate global indices from local indices\r\nglobal_index <- parsed %>% \r\n  group_by(doc_id, sentence_id) %>% \r\n  # add token counts for each sentence\r\n  add_count() %>% \r\n  ungroup() %>% \r\n  select(doc_id, sentence_id, n) %>% \r\n  distinct() %>%\r\n  # take the cumulative sum and shift 1 to the right (fill first index with 0)\r\n  mutate(n = c(0, cumsum(n)[1:n()-1]))\r\n\r\n# Clean the output\r\nparsed2 <- parsed %>% \r\n  inner_join(global_index, by = c(\"doc_id\", \"sentence_id\")) %>% \r\n  mutate(token_id_global = token_id + n,\r\n         head_token_id_global = head_token_id + n) %>% \r\n  relocate(token_id_global, .after = token_id) %>% \r\n  relocate(head_token_id_global, .after = head_token_id) %>% \r\n  select(-n)\r\n\r\n\r\n\r\nThis adds two colums - token_id_global and head_token_id_global - that stores indices that range over the entire dataframe. Here’s a sample of the new dataframe to demonstrate:\r\n\r\n\r\nsample_n(parsed2, 10)\r\n\r\n\r\n\r\ndoc_id\r\n\r\n\r\nsentence_id\r\n\r\n\r\ntoken_id\r\n\r\n\r\ntoken_id_global\r\n\r\n\r\ntoken\r\n\r\n\r\nlemma\r\n\r\n\r\npos\r\n\r\n\r\nhead_token_id\r\n\r\n\r\nhead_token_id_global\r\n\r\n\r\ndep_rel\r\n\r\n\r\ntext7\r\n\r\n\r\n37\r\n\r\n\r\n11\r\n\r\n\r\n48388\r\n\r\n\r\nto\r\n\r\n\r\nto\r\n\r\n\r\nADP\r\n\r\n\r\n10\r\n\r\n\r\n48387\r\n\r\n\r\ndative\r\n\r\n\r\ntext3\r\n\r\n\r\n154\r\n\r\n\r\n3\r\n\r\n\r\n18253\r\n\r\n\r\nthe\r\n\r\n\r\nthe\r\n\r\n\r\nDET\r\n\r\n\r\n5\r\n\r\n\r\n18255\r\n\r\n\r\ndet\r\n\r\n\r\ntext1\r\n\r\n\r\n44\r\n\r\n\r\n2\r\n\r\n\r\n1036\r\n\r\n\r\nof\r\n\r\n\r\nof\r\n\r\n\r\nADP\r\n\r\n\r\n1\r\n\r\n\r\n1035\r\n\r\n\r\npcomp\r\n\r\n\r\ntext8\r\n\r\n\r\n17\r\n\r\n\r\n6\r\n\r\n\r\n56007\r\n\r\n\r\nto\r\n\r\n\r\nto\r\n\r\n\r\nADP\r\n\r\n\r\n5\r\n\r\n\r\n56006\r\n\r\n\r\nprep\r\n\r\n\r\ntext5\r\n\r\n\r\n316\r\n\r\n\r\n3\r\n\r\n\r\n38631\r\n\r\n\r\nthis\r\n\r\n\r\nthis\r\n\r\n\r\nDET\r\n\r\n\r\n4\r\n\r\n\r\n38632\r\n\r\n\r\nnsubj\r\n\r\n\r\ntext6\r\n\r\n\r\n340\r\n\r\n\r\n15\r\n\r\n\r\n46579\r\n\r\n\r\nthen\r\n\r\n\r\nthen\r\n\r\n\r\nADV\r\n\r\n\r\n23\r\n\r\n\r\n46587\r\n\r\n\r\nadvmod\r\n\r\n\r\ntext4\r\n\r\n\r\n162\r\n\r\n\r\n7\r\n\r\n\r\n26488\r\n\r\n\r\n\r\n\r\n\r\n\r\nSPACE\r\n\r\n\r\n6\r\n\r\n\r\n26487\r\n\r\n\r\n\r\n\r\ntext6\r\n\r\n\r\n87\r\n\r\n\r\n13\r\n\r\n\r\n41466\r\n\r\n\r\nto\r\n\r\n\r\nto\r\n\r\n\r\nPART\r\n\r\n\r\n14\r\n\r\n\r\n41467\r\n\r\n\r\naux\r\n\r\n\r\ntext8\r\n\r\n\r\n260\r\n\r\n\r\n17\r\n\r\n\r\n60517\r\n\r\n\r\npositioned\r\n\r\n\r\nposition\r\n\r\n\r\nVERB\r\n\r\n\r\n9\r\n\r\n\r\n60509\r\n\r\n\r\nconj\r\n\r\n\r\ntext8\r\n\r\n\r\n281\r\n\r\n\r\n2\r\n\r\n\r\n60856\r\n\r\n\r\nis\r\n\r\n\r\nbe\r\n\r\n\r\nAUX\r\n\r\n\r\n11\r\n\r\n\r\n60865\r\n\r\n\r\nccomp\r\n\r\n\r\nAnd since this process isn’t destructive, we actually don’t need to assign the output to a new object. This is great because we can flexibly incorporate it into the pipeline workflow.\r\nHere is my solution wrapped in a function:5\r\n\r\n\r\nadd_global_index <- function(spacy_parsed) {\r\n  \r\n  global_index <- spacy_parsed %>% \r\n    group_by(doc_id, sentence_id) %>% \r\n    add_count() %>% \r\n    ungroup() %>% \r\n    select(doc_id, sentence_id, n) %>% \r\n    distinct() %>%\r\n    mutate(n = c(0, cumsum(n)[1:n()-1]))\r\n  \r\n  spacy_parsed %>% \r\n    inner_join(global_index, by = c(\"doc_id\", \"sentence_id\")) %>% \r\n    mutate(token_id_global = token_id + n,\r\n           head_token_id_global = head_token_id + n) %>% \r\n    relocate(token_id_global, .after = token_id) %>% \r\n    relocate(head_token_id_global, .after = head_token_id) %>% \r\n    select(-n)\r\n  \r\n}\r\n\r\n\r\n\r\nIn action:\r\n\r\n\r\n# Find adjectives describing \"America\"\r\nparsed %>% \r\n  add_global_index() %>% \r\n  filter(pos == \"ADJ\", slice(., head_token_id_global)$lemma == \"America\")\r\n\r\n\r\n\r\n\r\n\r\ndoc_id\r\n\r\n\r\nsentence_id\r\n\r\n\r\ntoken_id\r\n\r\n\r\ntoken_id_global\r\n\r\n\r\ntoken\r\n\r\n\r\nlemma\r\n\r\n\r\npos\r\n\r\n\r\nhead_token_id\r\n\r\n\r\nhead_token_id_global\r\n\r\n\r\ndep_rel\r\n\r\n\r\ntext3\r\n\r\n\r\n302\r\n\r\n\r\n33\r\n\r\n\r\n21359\r\n\r\n\r\ncompetitive\r\n\r\n\r\ncompetitive\r\n\r\n\r\nADJ\r\n\r\n\r\n34\r\n\r\n\r\n21360\r\n\r\n\r\namod\r\n\r\n\r\ntext4\r\n\r\n\r\n231\r\n\r\n\r\n33\r\n\r\n\r\n27772\r\n\r\n\r\nrural\r\n\r\n\r\nrural\r\n\r\n\r\nADJ\r\n\r\n\r\n34\r\n\r\n\r\n27773\r\n\r\n\r\namod\r\n\r\n\r\ntext5\r\n\r\n\r\n245\r\n\r\n\r\n2\r\n\r\n\r\n36825\r\n\r\n\r\nstronger\r\n\r\n\r\nstrong\r\n\r\n\r\nADJ\r\n\r\n\r\n3\r\n\r\n\r\n36826\r\n\r\n\r\namod\r\n\r\n\r\ntext6\r\n\r\n\r\n340\r\n\r\n\r\n18\r\n\r\n\r\n46582\r\n\r\n\r\nstrong\r\n\r\n\r\nstrong\r\n\r\n\r\nADJ\r\n\r\n\r\n21\r\n\r\n\r\n46585\r\n\r\n\r\namod\r\n\r\n\r\ntext7\r\n\r\n\r\n317\r\n\r\n\r\n23\r\n\r\n\r\n54054\r\n\r\n\r\nliberal\r\n\r\n\r\nliberal\r\n\r\n\r\nADJ\r\n\r\n\r\n24\r\n\r\n\r\n54055\r\n\r\n\r\namod\r\n\r\n\r\ntext7\r\n\r\n\r\n317\r\n\r\n\r\n27\r\n\r\n\r\n54058\r\n\r\n\r\nconservative\r\n\r\n\r\nconservative\r\n\r\n\r\nADJ\r\n\r\n\r\n28\r\n\r\n\r\n54059\r\n\r\n\r\namod\r\n\r\n\r\n\r\n\r\n# Find adjectives describing \"America\" inside a prepositional phrase\r\nparsed %>% \r\n  add_global_index() %>% \r\n  filter(pos == \"ADJ\", slice(., head_token_id_global)$lemma == \"America\",\r\n         slice(., slice(., head_token_id_global)$head_token_id_global)$dep_rel == \"prep\")\r\n\r\n\r\n\r\n\r\n\r\ndoc_id\r\n\r\n\r\nsentence_id\r\n\r\n\r\ntoken_id\r\n\r\n\r\ntoken_id_global\r\n\r\n\r\ntoken\r\n\r\n\r\nlemma\r\n\r\n\r\npos\r\n\r\n\r\nhead_token_id\r\n\r\n\r\nhead_token_id_global\r\n\r\n\r\ndep_rel\r\n\r\n\r\ntext3\r\n\r\n\r\n302\r\n\r\n\r\n33\r\n\r\n\r\n21359\r\n\r\n\r\ncompetitive\r\n\r\n\r\ncompetitive\r\n\r\n\r\nADJ\r\n\r\n\r\n34\r\n\r\n\r\n21360\r\n\r\n\r\namod\r\n\r\n\r\ntext4\r\n\r\n\r\n231\r\n\r\n\r\n33\r\n\r\n\r\n27772\r\n\r\n\r\nrural\r\n\r\n\r\nrural\r\n\r\n\r\nADJ\r\n\r\n\r\n34\r\n\r\n\r\n27773\r\n\r\n\r\namod\r\n\r\n\r\nPerformance:\r\n\r\n\r\ntest <- function(){\r\n  parsed %>% \r\n    add_global_index() %>% \r\n    filter(pos == \"ADJ\", slice(., head_token_id_global)$lemma == \"America\")\r\n}\r\n\r\nprint(microbenchmark::microbenchmark(test(), unit = \"s\"))\r\n\r\n\r\n  Unit: seconds\r\n     expr       min         lq      mean  median        uq       max neval\r\n   test() 0.0909179 0.09831835 0.1109029 0.10356 0.1122529 0.3129214   100\r\n\r\nMuch better!\r\n \r\nSession Info\r\n\r\n  R version 4.0.3 (2020-10-10)\r\n  Platform: x86_64-w64-mingw32/x64 (64-bit)\r\n  Running under: Windows 10 x64 (build 18363)\r\n  \r\n  Matrix products: default\r\n  \r\n  locale:\r\n  [1] LC_COLLATE=English_United States.1252 \r\n  [2] LC_CTYPE=English_United States.1252   \r\n  [3] LC_MONETARY=English_United States.1252\r\n  [4] LC_NUMERIC=C                          \r\n  [5] LC_TIME=English_United States.1252    \r\n  \r\n  attached base packages:\r\n  [1] stats     graphics  grDevices datasets  utils     methods   base     \r\n  \r\n  other attached packages:\r\n   [1] tidytext_0.2.6  sotu_1.0.2      reticulate_1.18 stringr_1.4.0  \r\n   [5] spacyr_1.2.1    forcats_0.5.0   ggplot2_3.3.2   purrr_0.3.4    \r\n   [9] dplyr_1.0.2     printr_0.1     \r\n  \r\n  loaded via a namespace (and not attached):\r\n   [1] Rcpp_1.0.5           highr_0.8            pillar_1.4.6        \r\n   [4] compiler_4.0.3       tokenizers_0.2.1     tools_4.0.3         \r\n   [7] digest_0.6.26        downlit_0.2.0        jsonlite_1.7.1      \r\n  [10] lattice_0.20-41      evaluate_0.14        lifecycle_0.2.0     \r\n  [13] tibble_3.0.4         gtable_0.3.0         pkgconfig_2.0.3     \r\n  [16] rlang_0.4.8          Matrix_1.2-18        rstudioapi_0.11     \r\n  [19] microbenchmark_1.4-7 distill_1.0          yaml_2.2.1          \r\n  [22] xfun_0.18            janeaustenr_0.1.5    withr_2.2.0         \r\n  [25] knitr_1.30           rappdirs_0.3.1       generics_0.0.2      \r\n  [28] vctrs_0.3.4          grid_4.0.3           tidyselect_1.1.0    \r\n  [31] data.table_1.13.2    glue_1.4.2           R6_2.4.1            \r\n  [34] fansi_0.4.1          rmarkdown_2.5        farver_2.0.3        \r\n  [37] magrittr_1.5.0.9000  SnowballC_0.7.0      prismatic_0.2.0     \r\n  [40] scales_1.1.1         ellipsis_0.3.1       htmltools_0.5.0     \r\n  [43] gt_0.2.2             colorspace_1.4-1     renv_0.12.0         \r\n  [46] labeling_0.4.2       stringi_1.5.3        munsell_0.5.0       \r\n  [49] crayon_1.3.4\r\n\r\n\r\nThere are less than 30 posts about it on StackOverflow, for example.↩︎\r\nI personally found it very easy to pick up vector comprehension in Python after working with purrr::map, for example.↩︎\r\nThe argument entity = FALSE is the same as disable = ['ner'] in spacy.load() in Python. I did this to save computation time.↩︎\r\nThis format is shared across other NLP packages in R based on spacCy, like {cleanNLP}↩︎\r\nThis would need to be tweaked a bit if you want to use it for the output of {cleanNLP} because the column for the local index of token heads, tid_source, is 0 when the token is the ROOT, as opposed to its own token index, which is the case in {spacyr}. You could add something like mutate(tid_source = ifelse(tid_source == 0, tid, tid_source) to the beginning of the pipeline to address this.↩︎\r\n",
    "preview": "posts/2020-06-25-indexing-tip-for-spacyr/preview.png",
    "last_modified": "2020-11-02T09:52:47+09:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 686
  },
  {
    "path": "posts/2020-06-07-correlation-parameter-mem/",
    "title": "The Correlation Parameter in Mixed Effects Models",
    "description": "Notes on the Corr term in {lme4} output",
    "author": [
      {
        "name": "June Choe",
        "url": {}
      }
    ],
    "date": "2020-06-07",
    "categories": [
      "statistics",
      "mixed-effects models",
      "tutorial"
    ],
    "contents": "\r\n\r\n\r\n\r\nWhat is Corr in the output of mixed-effects models?\r\nWhen fitting mixed effects regression models, especially those that try to keep it “maximal” (as per Barr, Levy, Scheepers, & Tily 2013), the random effects in the output of the model sometimes displays a column named Corr where some rows have numbers that range from -1 to 1.\r\n\r\nIt’s easy to guess that Corr stands for correlation and that the numbers in the column are correlation coefficients. If there are multiple predictors and the random effects structure includes more than one of those terms (e.g., (1 + Effect_1 * Effect_2 | Subject)), we even get another clue for this from the way that the Corr values spread out in the shape of a right triangle, much like in a correlation matrix.\r\n\r\nDespite the fact that we’re bound to have encountered this at some point when working with or reading about mixed effects models, I’ve found that there aren’t many beginner-friendly material explaining what they are - there are one-paragraph StackExchange answers and dense statistics papers, but not much in between in terms of comprehensiveness.\r\nSo here are my compiled notes on correlation parameters in linear mixed effects models that I’ve made for myself (with a basic knowledge of LMEMs).\r\nBefore we get started\r\nOur toy data and model\r\n\r\n\r\n\r\nFor the purposes of this discussion, I have created a toy experiment data (the code used to generate it is attached at the bottom).\r\nThe dataset toydata has 1,920 rows with the following columns:\r\nSubject: The subject ID, which ranges from 1 to 80\r\nItem: The item ID, which ranges from 1 to 24\r\nCondition: The experimental condition, which is either Control or Treatment\r\nResponse: A continuous observed variable\r\n\r\n\r\n\r\n\r\n\r\ntoydata\r\n\r\n\r\n  # A tibble: 1,920 x 4\r\n     Subject Item  Condition Response\r\n     <fct>   <fct> <fct>        <dbl>\r\n   1 1       1     Control       226.\r\n   2 1       2     Treatment     300.\r\n   3 1       3     Control       239.\r\n   4 1       4     Treatment     262.\r\n   5 1       5     Control       241.\r\n   6 1       6     Treatment     264.\r\n   7 1       7     Control       237.\r\n   8 1       8     Treatment     230.\r\n   9 1       9     Control       229.\r\n  10 1       10    Treatment     283.\r\n  # ... with 1,910 more rows\r\n\r\nImagine toydata to be the results from a very simple experiment. In this imaginary experiment, there are 80 subjects and each subject is tested on 24 items, resulting in a total of 1,920 trials/observations. This is a within-partipant design, so each participant sees 12 of the items in the Control condition and the other 12 in the Treatment condition.\r\nLet’s say that with our toy data, we want to know whether Condition has a positive effect on Response. Our goal by using mixed-effects modeling is to isolate the effect of Condition on Response (fixed effect), while controlling for by-item and by-subject variations (random effects). So let’s fit a simple linear mixed-effects model with the maximal random effects structure, with random intercepts and slopes for Condition by Subject and Item:\r\n\r\n\r\nmodel <- lmer(Response ~ Condition + (1+Condition|Subject) + (1+Condition|Item),\r\n              REML = FALSE, control = lmerControl('bobyqa'), data = toydata)\r\n\r\n\r\n\r\nAnd let’s really quickly check model assumptions:\r\n\r\n\r\nperformance::check_model(model)\r\n\r\n\r\n\r\n\r\nEverything looks okay, so let’s look at the model output:\r\n\r\n\r\nsummary(model)\r\n\r\n\r\n  Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\r\n    method [lmerModLmerTest]\r\n  Formula: Response ~ Condition + (1 + Condition | Subject) + (1 + Condition |  \r\n      Item)\r\n     Data: toydata\r\n  Control: lmerControl(\"bobyqa\")\r\n  \r\n       AIC      BIC   logLik deviance df.resid \r\n     13267    13317    -6624    13249     1911 \r\n  \r\n  Scaled residuals: \r\n     Min     1Q Median     3Q    Max \r\n  -3.176 -0.627 -0.065  0.576  4.864 \r\n  \r\n  Random effects:\r\n   Groups   Name               Variance Std.Dev. Corr\r\n   Subject  (Intercept)        637.1    25.24        \r\n            ConditionTreatment 108.1    10.40    0.85\r\n   Item     (Intercept)         44.4     6.66        \r\n            ConditionTreatment 308.2    17.56    0.14\r\n   Residual                     37.4     6.11        \r\n  Number of obs: 1920, groups:  Subject, 80; Item, 24\r\n  \r\n  Fixed effects:\r\n                     Estimate Std. Error     df t value Pr(>|t|)    \r\n  (Intercept)          209.64       3.14 100.84    66.8  < 2e-16 ***\r\n  ConditionTreatment    35.88       3.78  28.52     9.5  2.5e-10 ***\r\n  ---\r\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n  \r\n  Correlation of Fixed Effects:\r\n              (Intr)\r\n  CndtnTrtmnt 0.291\r\n\r\nThe high t-statistic and low p-value of ConditionTreatment in the fixed effects output suggests that our data is extremely unlikely given the null hypothesis that Condition has no effect on Response (and the sign of the estimate further suggests a positive effect of the Treatment condition on Response compared to the Control condition). Therefore, this is strong evidence in support of our hypothesis.\r\nHere’s a nicer-looking summary table made with tab_model() from the {sjPlot} package. I will keep using this format from this point on. Not only is this nicer to look at, the notations used here (like \\(\\tau_{00}\\) and \\(\\rho_{01}\\)) are from Barr et al. (2013), so it’s easier to connect the pieces IMO.\r\n\r\n\r\nsjPlot::tab_model(model)\r\n\r\n\r\n\r\n \r\n\r\n\r\nResponse\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI\r\n\r\n\r\np\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n209.64\r\n\r\n\r\n203.49 – 215.80\r\n\r\n\r\n<0.001\r\n\r\n\r\nCondition [Treatment]\r\n\r\n\r\n35.88\r\n\r\n\r\n28.48 – 43.28\r\n\r\n\r\n<0.001\r\n\r\n\r\nRandom Effects\r\n\r\n\r\nσ2\r\n\r\n37.37\r\n\r\n\r\nτ00Subject\r\n\r\n637.14\r\n\r\n\r\nτ00Item\r\n\r\n44.39\r\n\r\n\r\nτ11Subject.ConditionTreatment\r\n\r\n108.10\r\n\r\n\r\nτ11Item.ConditionTreatment\r\n\r\n308.21\r\n\r\n\r\nρ01Subject\r\n\r\n0.85\r\n\r\n\r\nρ01Item\r\n\r\n0.14\r\n\r\n\r\nICC\r\n\r\n\r\n0.97\r\n\r\n\r\nN Subject\r\n\r\n80\r\n\r\n\r\nN Item\r\n\r\n24\r\n\r\n\r\nObservations\r\n\r\n\r\n1920\r\n\r\n\r\nMarginal R2 / Conditional R2\r\n\r\n0.216 / 0.975\r\n\r\n\r\nOkay so our finding is great and all but we’re more interested in the random effects here so let’s look at that.\r\nThe random effects (a review)\r\nWe can isolate the random effects from the model using VarCorr():\r\n\r\n\r\nVarCorr(model)\r\n\r\n\r\n   Groups   Name               Std.Dev. Corr\r\n   Subject  (Intercept)        25.24        \r\n            ConditionTreatment 10.40    0.85\r\n   Item     (Intercept)         6.66        \r\n            ConditionTreatment 17.56    0.14\r\n   Residual                     6.11\r\n\r\nLet’s ignore the Corr column for a moment and talk about Std.Dev first.\r\nThe Std.Dev. values for the Subject random effects group suggest that the variation in the subject intercepts are fitted with a larger standard deviation of 25.24 and that the variation in subject slopes for Condition are fitted with a smaller standard deviation of 10.4.\r\nLet’s plot the by-subject variation:\r\n\r\n\r\n\r\nWe see a clear variation in the intercepts, and a subtler variation in the slopes. This is overall pretty consistent with the stand deviation values for subject random effects that we found earlier.\r\nLet’s plot the by-item variation as well:\r\n\r\n\r\n\r\nHere, we see the opposite: a clear variation in the slopes, and a subtler variation in the intercepts. Again, this is overall pretty consistent given the item random effects output: a larger standard deviation for the item slopes and a smaller standard deviation for item intercepts, as we found earlier.\r\nNow that we’ve reviewed Std.Dev., let’s talk about Corr, which is our main focus.\r\nThe Correlation Parameter\r\nLooking at the Corr column now, we see two numbers: 0.85 within the Subject random effects group and 0.14 within the Item random effects group.\r\n\r\n   Groups   Name               Std.Dev. Corr\r\n   Subject  (Intercept)        25.24        \r\n            ConditionTreatment 10.40    0.85\r\n   Item     (Intercept)         6.66        \r\n            ConditionTreatment 17.56    0.14\r\n   Residual                     6.11\r\n\r\nAs you might have guessed, they have something to do with the correlation between random effects (intercept and slope) within each group (subject and item).\r\nBut if we extract the subject random effects, for example, and measure the correlation between subject intercepts and subject slopes, we get a slightly different number:\r\n\r\n\r\n# Extract by-subject intercept and slope\r\nranef_subj <- ranef(model)$Subject\r\n\r\nranef_subj_intercepts <- ranef_subj$`(Intercept)`\r\nranef_subj_slopes <- ranef_subj$ConditionTreatment\r\n\r\n# Calculate correlation\r\ncor(ranef_subj_intercepts, ranef_subj_slopes)\r\n\r\n\r\n  [1] 0.88\r\n\r\nIn fact, we get slightly different values for the standard deviation of the subject random intercepts and slopes as well:\r\n\r\n\r\n# Calculate the standard deviation of by-subject intercepts and slopes\r\nsummarize_all(ranef_subj, sd)\r\n\r\n\r\n    (Intercept) ConditionTreatment\r\n  1        25.3              10.18\r\n\r\nSo it looks like what the model isn’t just taking the random effects in our toydata dataset and calculating their variations and correlations. So then what is it doing?\r\nHow the model estimates random effects\r\nWhat we have to keep in mind when doing mixed effects modeling is that the model is fitting the random effects in the data, rather than just describing them. More specifically, the model is estimating population parameters that generated the sample of random effects that are seen in the data.1\r\nAnd in fact that’s exactly what we want to do. We don’t care about how individual subjects or items behave in our experiment, in the sense that we don’t care how fast John Doe presses a button, for example. We don’t want to predict John Doe’s behavior, but we do want to estimate, using data from John Doe, Jane Doe, Average Joe, and other participants from our experiment, the overall distribution of people’s idiosyncratic tendencies so that we can statistically control for them to get a better estimate for the fixed effects that we care about.\r\nTake the random intercepts by subject for example. The model estimated the distribution of subject intercepts to follow a normal distribution with a standard deviation of 25.24, which is a an estimate of the Population. The variation in subject intercepts in the data itself that we manually calculated above (25.3) is the Sample standard deviation. Of course, if the sample follows a normal distribution and if we also assume the population to be normally distributed, the sample variance should be the best estimate for the population variance. And in fact they do end up being very close!\r\nSo the numbers in the Std.Dev. colum are the model’s fit for the variation within each random effect.\r\nWith this, we now have a better understanding of the numbers in the Corr column: they are the model’s fit for the correlation between random effects.\r\nTo go more in depth with our discussion, let’s plot the intercepts and slopes for our 80 subjects:\r\n\r\n\r\n\r\nRecall that when we manually calculated the correlation between subject intercepts and subject slopes within our sample of 80 subjects in the data, we got 0.88. That is in fact what is shown by the plot above.\r\nAnd as we discussed earlier, the numbers in the Std.Dev. column are the model’s fit for the variation within each random effect. So the model is saying that the variation for subject intercepts follows a normal distribution with mean = 0 and standard deviation = 25.24. Likewise, the model is saying that the variation for subject slopes follows a normal distribution with mean = 0 and standard deviation = 10.4.2\r\nSo the model estimates these two distributions - one for subject slopes and one for subject intercepts - to capture the overall distribution of subject random effects.\r\nThis is illustrated below, where the normal curve at the top is the distribution of subject intercepts estimated by the model, and the normal curve to the right is the distribution of subject slopes estimated by the model. For every subject, their intercepts and slopes are understood to be generated from these two underlying parameters:\r\n\r\n\r\n\r\nBut is specifying each distribution for intercept and item enough to capture the overall distribution of subject random effects?\r\nOne way to test this is to work backwards and generate some observations from the model’s parameters. The idea here is this: if the two normal distributions (one for subject intercept and one for subject slope) can sufficiently capture the distribution of the subject random effects, then sampling from them should yield a distribution that is in the shape of the actual distribution in our data.\r\nLet’s draw 80 samples of subject intercepts and subject slopes from their respective distributions and then plot them together. Here’s one result:\r\n\r\n\r\n\r\nThese points are consistent with what the two distributions predict: there are more points towards the center (the means of the distributions) and less points towards the corners (the tails of the distributions).\r\nBut this doesn’t look like the actual distribution of our subject random effects.\r\nWe can repeat this sampling procedure many times, but none of them look close to the distribution of the subject random effects in our data:\r\nFor clearer comparison, here is the plot of the subject random effects in our data again.\r\n \r\n\r\n\r\n\r\n\r\n\r\n\r\nSo what are we missing here?\r\nWell, what’s missing here is the correlation between the intercepts and slopes that I conveniently left out to demonstrate that just specifying the individual distributions for subject intercepts and subject slopes poorly captures the actual distribution of subject random effects in our data.\r\nIn more technical terms, treating the two random effects as independently sampled from their respective distributions fails to fit the data well because the two random effects are highly correlated. They should instead be treated as being jointly sampled from a bivariate distribution\r\nAnd that’s exactly what adding the correlation parameter does. Let’s break this down.\r\nWhen we say that two variables are independently sampled from two distributions (as we just did above), then their joint distribution looks something like this, where most of the data is expected to fall within the grey shaded ellipse:\r\n\r\n\r\n\r\nThis is clearly a bad fit for the distribution of our subject random effects…\r\n\r\n\r\n\r\n… because the distribution of the subject random effects actually takes the shape of a tilted ellipse instead (dotted outline):\r\n\r\n\r\n\r\nIn fact, we cannot generate any distribution of a tilted shape with just two independent distributions for each variable. We need to factor in covariation to capture the correlation between variables. Barr et al. (2013) illustrates this clearly in the supplementary materials to their paper. You can see from the plot below (originally Figure 1 on the linked page) that without the correlation parameter, you can only capture distributions that are symmetrical with respect to the axes (the darker ellipses). However, once you add in a correlation parameter (\\(\\rho\\)), you can capture distributions that are in the “tilted” shape (the lighter ellipses) like the distribution of our highly correlated subject intercepts and subject slopes.\r\n\r\n\r\n\r\nFigure 1: Figure from Barr et al. (2013)\r\n\r\n\r\n\r\nPutting it all together\r\nHere’s the output of model again:\r\n\r\n\r\n \r\n\r\n\r\nResponse\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI\r\n\r\n\r\np\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n209.64\r\n\r\n\r\n203.49 – 215.80\r\n\r\n\r\n<0.001\r\n\r\n\r\nCondition [Treatment]\r\n\r\n\r\n35.88\r\n\r\n\r\n28.48 – 43.28\r\n\r\n\r\n<0.001\r\n\r\n\r\nRandom Effects\r\n\r\n\r\nσ2\r\n\r\n37.37\r\n\r\n\r\nτ00Subject\r\n\r\n637.14\r\n\r\n\r\nτ00Item\r\n\r\n44.39\r\n\r\n\r\nτ11Subject.ConditionTreatment\r\n\r\n108.10\r\n\r\n\r\nτ11Item.ConditionTreatment\r\n\r\n308.21\r\n\r\n\r\nρ01Subject\r\n\r\n0.85\r\n\r\n\r\nρ01Item\r\n\r\n0.14\r\n\r\n\r\nICC\r\n\r\n\r\n0.97\r\n\r\n\r\nN Subject\r\n\r\n80\r\n\r\n\r\nN Item\r\n\r\n24\r\n\r\n\r\nObservations\r\n\r\n\r\n1920\r\n\r\n\r\nMarginal R2 / Conditional R2\r\n\r\n0.216 / 0.975\r\n\r\n\r\nAnd let’s keep focusing on the subject random effects for now.\r\nThere are three parameters that the model estimated to capture the by-subject variation:\r\nThe variation (Std.Dev.) for subject intercept\r\nThe variation (Std.Dev.) for subject slope\r\nThe correlation (Corr) between subject intercept and subject slope.\r\nWith these three parameters, the model is defining a bivariate normal distribution, from which subject intercepts (\\(S_{0s}\\)) and subject slopes (\\(S_{1s}\\)) are sampled from (Equation 3 from Barr et al., 2013):\r\n\\[(S_{0s}, S_{1s})\\ \\sim\\ N(0,\\begin{bmatrix}\\tau_{00}^2 & \\rho\\ \\tau_{00}^2 \\tau_{11}^2 \\\\ \\rho\\ \\tau_{00}^2 \\tau_{11}^2 & \\tau_{11}^2 \\end{bmatrix})\\]\r\nFor the variance-covariance matrix, we can substitute the standard deviation for the subject intercept \\(\\tau_{00}^2\\) with 25.24, the standard deviation for the subject slope \\(\\tau_{11}\\) with 10.4, and the correlation \\(\\rho\\) with 0.85 to get the following:\r\n\\[(S_{0s}, S_{1s})\\ \\sim\\ N(0,\\begin{bmatrix}25.24^2 & 0.85\\ \\times\\ 25.24\\ \\times\\ 10.4 \\\\ 0.85\\  \\times\\ 25.24\\ \\times\\ 10.4 & 10.4^2 \\end{bmatrix})\\]\r\nIf subject intercepts and subject slopes are jointly sampled from the above distribution, most observations should fall within this grey area:\r\n\r\n\r\n\r\n\r\nNotice how I added \\(\\rho = 0.85\\) in this top right corner of this plot.\r\nLet’s again repeatedly sample from this new bivariate distribution (which you can do with mvrnorm() from the {MASS} package) to check:\r\n\r\n\r\n\r\nLike we expected, this new distribution generates observations of subject slopes and subject intercepts that are highly correlated. But more importantly, the distribution of subject random effects in our data looks like it could be one of these samples, meaning that this bivariate normal distribution fits our data well.\r\nGood thing that we had the model estimate this parameter by specifying the random effects structure for subjects as (1 + Condition | Subject) in our model formula!\r\nWhat if we leave out this correlation parameter? Would it significantly worsen model fit?\r\nWe can check by building another model without the correlation term between the subject random effects and comparing it with our original model.3\r\nThe no_subj_cor_model below is a depleted model without the correlation parameter between the random intercepts and random slopes by subject. You can see that the subject group is missing a value in the Corr column.4\r\n\r\n\r\nno_subj_cor_model <- lmer(Response ~ Condition + (1+model.matrix(model)[,2]||Subject) + (1+Condition|Item),\r\n                          REML = FALSE, control = lmerControl('bobyqa'), data = toydata)\r\n\r\ntab_model(no_subj_cor_model)\r\n\r\n\r\n\r\n \r\n\r\n\r\nResponse\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI\r\n\r\n\r\np\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n209.64\r\n\r\n\r\n203.44 – 215.84\r\n\r\n\r\n<0.001\r\n\r\n\r\nCondition [Treatment]\r\n\r\n\r\n35.88\r\n\r\n\r\n28.43 – 43.33\r\n\r\n\r\n<0.001\r\n\r\n\r\nRandom Effects\r\n\r\n\r\nσ2\r\n\r\n37.34\r\n\r\n\r\nτ00Subject\r\n\r\n649.20\r\n\r\n\r\nτ00Subject.1\r\n\r\n110.96\r\n\r\n\r\nτ00Item\r\n\r\n44.56\r\n\r\n\r\nτ11Item.ConditionTreatment\r\n\r\n311.76\r\n\r\n\r\nρ01Item\r\n\r\n0.14\r\n\r\n\r\nICC\r\n\r\n\r\n0.96\r\n\r\n\r\nN Subject\r\n\r\n80\r\n\r\n\r\nN Item\r\n\r\n24\r\n\r\n\r\nObservations\r\n\r\n\r\n1920\r\n\r\n\r\nMarginal R2 / Conditional R2\r\n\r\n0.263 / 0.970\r\n\r\n\r\nNow let’s perform a likelihood ratio test using anova():\r\n\r\n\r\nanova(no_subj_cor_model, model, test = 'Chisq')\r\n\r\n\r\n  Data: toydata\r\n  Models:\r\n  no_subj_cor_model: Response ~ Condition + (1 + model.matrix(model)[, 2] || Subject) + \r\n  no_subj_cor_model:     (1 + Condition | Item)\r\n  model: Response ~ Condition + (1 + Condition | Subject) + (1 + Condition | \r\n  model:     Item)\r\n                    npar   AIC   BIC logLik deviance Chisq Df Pr(>Chisq)    \r\n  no_subj_cor_model    8 13352 13396  -6668    13336                        \r\n  model                9 13267 13317  -6624    13249  87.2  1     <2e-16 ***\r\n  ---\r\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nThe first thing to notice is that no_subj_cor_model has one less Df, or degrees of freedom, than model. This is because every correlation between random effects is an additional parameter that the model is estimating. So removing the correlation between random effects for no_subj_cor_model leaves it with 8 parameters, one less than the original, full model. There’s a good discussion of what parameters are specified by different lmer() formulas in this StackExchange thread.\r\nAfter performing this sanity check, the next thing to note is the very low number in Pr(>Chisq), telling us that the models are significantly different from one another. This might come off as weird if you’re only used to performing ANOVA comparisons to check whether a predictor is significant or not. In fact, there are no obvious differences between the output of no_subj_cor_model and the output of our original model other than the presence/absence of the correlation between subject random effects.\r\nBut clearly something major is going on behind the curtains, so we turn to the last term(s) of interest - AIC and BIC, which are scores for model fit. The numbers are hard to interpret on their own, but useful when comparing models. Here, both the AIC and the BIC of no_subj_cor_model are higher than model, suggesting that no_subj_cor_model has a worse fit, and a statistically significant one at that.\r\nMore specifically, we know that the only meaningful difference between no_subj_cor_model and model is the correlation parameter for the subject random effects, so no_subj_cor_model must be capturing the subject random effects relativelty poorly under its assumption that subject intercepts and subject slopes do not correlate with one another (i.e., that they are independent).\r\nSo let’s look at the random effects calculated by no_subj_cor_model and its poor attempt at fitting their distribution.\r\nFirst, let’s plot the subject intercepts by subject slopes like we did for our original model:\r\n\r\n\r\n\r\nYou might notice that the no_subj_cor_model calculates subject random effects that are very similar to those calculated by our original mode. Here’s a side-by-side comparison of the subject random effects from model and no_subj_cor_model:\r\n\r\n\r\n\r\nThis illustrates a very important point. Removing the correlation parameter does not change the calculation of the random effects (barring any serious convergence failures, of course). This shouldn’t be surprising because random effects, like fixed effects, speak to facts (in the frequentist sense) about how the data that we observe is generated. It is literally the case here since I included these random effects explicitly in making toydata. But more importantly, the idea that there are random variations generated from underlying population-level parameters is an assumption that we are making when we use mixed-effects models.\r\nThe only meaningful difference between the two models here, then, is in their fit - e.g., how well the model captures the distribution of the subject random effects. We actually went over this above - we saw that our original model fits subject random effects using a bivariate normal distribution assuming a correlation, while no_subj_cor_model should be fitting subject random effects using two univariate normal distributions, assuming no (i.e., zero) correlation.\r\nHere’s a visual comparison of model fit, with the plot for model at the top and the plot for no_subj_cor_model at the bottom:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWhere the term \\(\\rho = 0\\) in the plot for no_subj_cor_model indicates that the subject intercepts and subject slopes are generated from this bivariate distribution:\r\n\\[(S_{0s}, S_{1s})\\ \\sim\\ N(0,\\begin{bmatrix} \\tau_{00}^2 & 0 \\\\ 0 & \\tau_{11}^2 \\end{bmatrix})\\]\r\nWhich is the same as independetly sampling from these two univariate normal distributions:\r\n\\[S_{0s} \\sim N(0, \\tau_{00})\\]\r\n\\[S_{1s} \\sim N(0, \\tau_{11})\\]\r\nNow, looking at the previous pair of plots, no_subj_cor_model (bottom) clearly fits the distribution of the subject random effects poorly compared to our original model model (top), and that appears to be the driving the significant decrease in fit that we found from the likelihood ratio test earlier. It seems to be the case that the inclusion of the correlation parameter between subject random intercepts is necessary to fit the data well.\r\nAre correlation parameters always necessary?\r\nThe question of how “maximal” our models should be is very tricky, and especially so when it concerns the inclusion/exclusion of correlation parameters (see discussions here and here). For example, Bates, Kliegl, Vasishth, & Baayen (2015) and Matuschek, Kliegl, Vasishth, & Baayen (2017) have called for parsimonious models with stricter criteria for including terms in the model, beyond whether they cause the model to fail to converge.5\r\nI’ll demonstrate one case here where it doesn’t seem like including a correlation parameter particularly improves model fit.\r\nLet’s repeat the model comparison process above, except this time taking out the correlation parameter for item.\r\nFor context, here is the random effects output of model again:\r\n\r\n   Groups   Name               Std.Dev. Corr\r\n   Subject  (Intercept)        25.24        \r\n            ConditionTreatment 10.40    0.85\r\n   Item     (Intercept)         6.66        \r\n            ConditionTreatment 17.56    0.14\r\n   Residual                     6.11\r\n\r\nAgain, there are three parameters that the model estimated to capture the by-item variation:\r\nThe variation (Std.Dev.) for item intercept\r\nThe variation (Std.Dev.) for item slope\r\nThe correlation (Corr) between item intercept and item slope.\r\nAnd here is what the distribution of item random effects from model look like:\r\n\r\n\r\n\r\nOur model fitted a bivariate normal distribution with the standard deviation of item intercepts = 6.66, the standard deviation of item slopes = 10.4, and correlation = 0.14.\r\nWe can again visualize the fit of model to the distribution of the item random effects:\r\n\r\n\r\n\r\nThe model estimates a low correlation of 0.14, which is reflected in the small tilt of the ellipse. It looks like the model is capturing the distribution of the item random effects pretty well. But is the correlation parameter really that necessary here?\r\nLet’s make another depleted model, no_item_cor_model, with the correlation between item random effects removed:\r\n\r\n\r\nno_item_cor_model <- lmer(Response ~ Condition + (1+Condition|Subject) + (1+model.matrix(model)[,2]||Item),\r\n                          REML = FALSE, control = lmerControl('bobyqa'), data = toydata)\r\n\r\ntab_model(no_item_cor_model)\r\n\r\n\r\n\r\n \r\n\r\n\r\nResponse\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI\r\n\r\n\r\np\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n209.64\r\n\r\n\r\n203.49 – 215.80\r\n\r\n\r\n<0.001\r\n\r\n\r\nCondition [Treatment]\r\n\r\n\r\n35.88\r\n\r\n\r\n28.45 – 43.31\r\n\r\n\r\n<0.001\r\n\r\n\r\nRandom Effects\r\n\r\n\r\nσ2\r\n\r\n37.37\r\n\r\n\r\nτ00Subject\r\n\r\n636.98\r\n\r\n\r\nτ00Item\r\n\r\n44.55\r\n\r\n\r\nτ00Item.1\r\n\r\n310.22\r\n\r\n\r\nτ11Subject.ConditionTreatment\r\n\r\n108.08\r\n\r\n\r\nρ01Subject\r\n\r\n0.85\r\n\r\n\r\nICC\r\n\r\n\r\n0.96\r\n\r\n\r\nN Subject\r\n\r\n80\r\n\r\n\r\nN Item\r\n\r\n24\r\n\r\n\r\nObservations\r\n\r\n\r\n1920\r\n\r\n\r\nMarginal R2 / Conditional R2\r\n\r\n0.244 / 0.972\r\n\r\n\r\nAgain, the output of the depleted model printed here does not differ that much from the output of model. We can see also get a sense of this by visualizing the fit of no_item_cor_model to the distribution of item random effects:\r\n\r\n\r\n\r\nWe can see this more clearly with a side-by-side comparison of model fit by model (blue) and no_item_cor_model (red):\r\n\r\n\r\n\r\nDoesn’t seem like there are big differences here, but we have to run some statistics to be sure. So let’s perform another log likelihood ratio test:\r\n\r\n\r\nanova(no_item_cor_model, model)\r\n\r\n\r\n  Data: toydata\r\n  Models:\r\n  no_item_cor_model: Response ~ Condition + (1 + Condition | Subject) + (1 + model.matrix(model)[, \r\n  no_item_cor_model:     2] || Item)\r\n  model: Response ~ Condition + (1 + Condition | Subject) + (1 + Condition | \r\n  model:     Item)\r\n                    npar   AIC   BIC logLik deviance Chisq Df Pr(>Chisq)\r\n  no_item_cor_model    8 13265 13310  -6625    13249                    \r\n  model                9 13267 13317  -6624    13249  0.47  1       0.49\r\n\r\n\r\n\r\n\r\nFirst, for sanity check, we see that no_item_cor_model has one less Df than model, which is what we’d expect if no_item_cor_model indeed lacks the correlation parameter for item random effects. Next, we see that the value of Pr(>Chisq) is very high, at 0.49, suggesting that the models are not significantly different. This is corroborated by the very small differences between the models’ AIC and BIC values. These small differences here are likely reducible to the fact that AIC and BIC penalize more number of parameters (as a way of balancing model fit with model complexity). In fact, the differences in AIC between the models is approximately 2, which is exactly what you’d expect if you added a redundant parameter with no additional explanatory power to the model.\r\nIn sum, we see that when modeling our dataset toydata, estimating a correlation parameter for subject random effects improves model fit, while doing so for item random effects doesn’t as much.\r\nConclusion and implications for model building\r\nMy main goal here was to simply go over what the correlation parameter in mixed-effects models is, so the question of whether we should be including certain correlation parameter(s) in our models are beyond the scope of this discussion (and should be handled on a case-by-case basis). It’s also something that I’m in the process of learning, so I don’t have a good answer to it yet. But my personal view (which may change later with more knowledge and experience) is to keep correlation parameters in the model unless they make the model fail to converge. So in the case of the correlation parameter for item random effects in model that we discussed above, I’d personally keep that in since we didn’t run into any convergence issues fitting the maximal model. In general, if there’s no meaningful difference either way, I err towards leaving the correlation parameter in there. In other words, I try to keep the model as maximal as possible without overparameterizing it (Barr et al., 2013).\r\nI don’t think that the spirit of this message is really controversial, and the more challenging part of this is putting it into practice. We not only need to balance model fit with model complexity, but we also often need to navigate conflicts between important considerations from statistical theory and from whatever domain our research is in (linguistics, psychology, etc.).\r\nTo resolve this, a lot of people have streamlined different methods of reducing the complexity of the random effects structure in a statistically motivated way. One such example is using Principal Components Analysis (PCA) (suggested in Bates et al., 2015). In Section 3 of their paper, Bates and colleagues outline a procedure for iterative model reduction which involves PCA (now available as rePCA() in the lme4 package) to determine how many random effect terms are sufficient to capture the variance in the random effects. This is still not a perfect solution, of course, but it’s a good next step for putting this knowledge into practice. Or you can just do fancy Bayesian analyses and avoid all these problems, so I hear\r\nAnyways, that’s it for my notes. Here’s the code that generated toydata:\r\n\r\n\r\n###########\r\n## Setup ##\r\n###########\r\n\r\n# Load Packages (make sure dplyr::filter() isn't makes by MASS:filter())\r\nlibrary(MASS)\r\nlibrary(tidyverse)\r\nlibrary(lme4)\r\n\r\n# Set seed\r\nset.seed(1234)\r\n\r\n# Set number of participants and items\r\nn_subjects <- 80\r\nn_items <- 24\r\n\r\n#################\r\n## Make trials ##\r\n#################\r\n\r\n# Generate levels\r\nSubject <- gl(n_subjects, n_items)\r\nItem <- rep(gl(n_items, 1), n_subjects)\r\nCondition <- factor(rep(c(rep(c(\"Control\", \"Treatment\"), n_items/2),\r\n                   rep(c(\"Treatment\", \"Control\"), n_items/2)),\r\n                 n_subjects/2))\r\n\r\n# Treatment coding\r\nCondition_coded <- ifelse(Condition == \"Control\", 0, 1)\r\n\r\n# Combine into trials\r\nData <- tibble(Subject, Item, Condition, Condition_coded)\r\n\r\n#############################\r\n## Add Intercept and Slope ##\r\n#############################\r\n\r\n# Add intercept\r\nData$Intercept <- 200\r\n\r\n# Add slope\r\nData$Slope <- ifelse(Data$Condition == \"Treatment\", 30, 0)\r\n\r\n########################\r\n## Add Random Effects ##\r\n########################\r\n\r\n# By-subject variation in intercept and slope (sampled from bivariate normal)\r\nsd_subj_intercept <- 25\r\nsd_subj_slope <- 10\r\nsubj_ranef_cor <- 0.8\r\n\r\nsubj_ranef <- mvrnorm(n_subjects,\r\n                      # means of two normals are both 0\r\n                      c(\"Intercept\" = 0, \"Slope\" = 0),\r\n                      # 2x2 variance-covariance matrix\r\n                      matrix(\r\n                        c(sd_subj_intercept^2,\r\n                          subj_ranef_cor*sd_subj_intercept*sd_subj_slope,\r\n                          subj_ranef_cor*sd_subj_intercept*sd_subj_slope,\r\n                          sd_subj_slope^2),\r\n                        ncol = 2)\r\n                      )\r\n\r\nData$Subj_intercept <- rep(subj_ranef[,\"Intercept\"], each = n_items)\r\nData$Subj_slope <- rep(subj_ranef[,\"Slope\"], each = n_items)\r\n\r\n# By-item variation in intercept and slope (sampled independently)\r\nData$Item_intercept <- rep(rnorm(n_items, sd = 5), times = n_subjects)\r\nData$Item_slope <- rep(rnorm(n_items, sd = 15), times = n_subjects)\r\n\r\n# Random noise\r\nData$Noise <- rnorm(nrow(Data), 0, 5) + rlnorm(nrow(Data), 0.5)\r\n\r\n###########################\r\n## Generate Observations ##\r\n###########################\r\n\r\nData <- Data %>%\r\n  mutate(Response =\r\n           Intercept +\r\n           Slope * Condition_coded +\r\n           Subj_intercept +\r\n           Subj_slope * Condition_coded +\r\n           Item_intercept +\r\n           Item_slope * Condition_coded +\r\n           Noise)\r\n\r\n#################\r\n## Toy Dataset ##\r\n#################\r\n\r\ntoydata <- Data %>% \r\n  select(Subject, Item, Condition, Response)\r\n\r\n\r\n\r\n\r\nThis distinction is also reflected in the fact that the notation for random effect standard deviation is tau (\\(\\tau\\)), which is a Greek symbol. In statistics, Greek symbols (like \\(\\beta\\), which we may be more familiar with) refers to population-level paramters.↩︎\r\nBut what if the distribution of random effects has a mean that is not equal to zero? Well that just shifts the fixed effects estimate, so the distribution of random effects can be fully characterized by just its variance/standard deviation. This is also why you should never remove a term from fixed effects without removing it from random effects like in Response ~ 1 + (1 + Condiiton | Subject) without a good reason, because the model will assume the fixed effect of Condition to be zero.↩︎\r\nRemoving a correlation term in lmer() turns out to be actually sort of tricky if you don’t explicitly numerically code your factors - sometimes just using the double bar syntax (||) doesn’t always work. I won’t go into the details of how to do that here, but there are good discussions of doing this using model.matrix() in this Rpubs post and Section 5.4 (also Appendix E) of Frossard and Renaud (2019). I could have done numeric coding with something like mutate(data, Condition = as.integer(Condition == \"Treatment\")), but I wanted to try this way out for myself↩︎\r\nHere, \\(\\tau_{00\\ Subject.1}\\) is actually the same as the \\(\\tau_{00\\ Subject.ConditionTreatment}\\) term from the maximal model, model. I don’t know how to suppress this name change after dropping a correlation term - if you do, please let me know!↩︎\r\nThe (simplified) argument here is that having the model estimate superfluous variance components can make it more difficult for the model to detect an effect if it actually exists - i.e., can lead to a loss of power↩︎\r\n",
    "preview": "posts/2020-06-07-correlation-parameter-mem/preview.png",
    "last_modified": "2020-11-05T12:36:36+09:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
